# Custom Fundamentals System - Architecture & Implementation Guide

## Overview

This document provides comprehensive documentation of the custom fundamentals data integration system in Zipline Reloaded. This system enables users to load custom fundamental data (e.g., REFE fundamentals) into SQLite databases and use them seamlessly in Zipline Pipeline for algorithmic trading strategies.

**Target Audience**: LLM coding assistants, new developers, and maintainers who need to understand the complete architecture and implementation details.

---

## Table of Contents

1. [System Architecture](#system-architecture)
2. [File Structure](#file-structure)
3. [Key Components](#key-components)
4. [Data Flow](#data-flow)
5. [Implementation Details](#implementation-details)
6. [Usage Workflow](#usage-workflow)
7. [Known Issues & Solutions](#known-issues--solutions)
8. [Recent Fixes & Commits](#recent-fixes--commits)
9. [Testing & Examples](#testing--examples)

---

## System Architecture

### High-Level Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CSV Data Files    â”‚
â”‚  (Fundamentals)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Load & Transform   â”‚  â† notebooks/load_csv_fundamentals.ipynb
â”‚  (Pandas)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SQLite Database    â”‚  â† ~/.zipline/data/custom/refe-fundamentals.db
â”‚  (Custom Schema)    â”‚     Schema: Price table with Sid, Date, columns
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CustomSQLiteLoader  â”‚  â† src/zipline/data/custom/pipeline_integration.py
â”‚ (PipelineLoader)    â”‚     Implements load_adjusted_array()
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SimplePipelineEngineâ”‚  â† Uses get_loader factory pattern
â”‚                     â”‚     Routes columns to appropriate loaders
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Database Class     â”‚  â† User-defined (e.g., REFEFundamentals)
â”‚  (Dataset)          â”‚     Defines columns with dtype, missing_value
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pipeline Queries   â”‚  â† Returns filtered DataFrames of stocks
â”‚  (Factors/Filters)  â”‚     e.g., .top(100), .latest, .rank()
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Trading Strategy   â”‚  â† Uses run_algorithm() or TradingAlgorithm
â”‚  (Backtest)         â”‚     Rebalances portfolio based on pipeline
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Principles

1. **Separation of Concerns**: Data loading, pipeline integration, and strategy logic are separate
2. **Extensibility**: Custom Database classes can define any columns with appropriate dtypes
3. **Performance**: SQLite indexing on (Date, Sid) for fast queries
4. **Type Safety**: Explicit dtype handling prevents mixed-type errors in Pipeline
5. **Compatibility**: Works seamlessly with existing Zipline bundles (e.g., sharadar)

---

## File Structure

```
zipline-reloaded/
â”‚
â”œâ”€â”€ src/zipline/data/custom/
â”‚   â”œâ”€â”€ __init__.py                      # Exports CustomSQLiteLoader, helpers
â”‚   â”œâ”€â”€ config.py                        # DEFAULT_DB_DIR config
â”‚   â”œâ”€â”€ db_manager.py                    # Database creation, connection utilities
â”‚   â””â”€â”€ pipeline_integration.py          # â­ CustomSQLiteLoader implementation
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ load_csv_fundamentals.ipynb      # â­ Main workflow: CSV â†’ SQLite â†’ Pipeline
â”‚   â””â”€â”€ strategy_top5_roe.py             # â­ Example strategy using custom fundamentals
â”‚
â”œâ”€â”€ tests/zipline/data/
â”‚   â””â”€â”€ test_custom_pipeline.py          # Unit tests for custom loaders
â”‚
â”œâ”€â”€ CUSTOM_FUNDAMENTALS_ARCHITECTURE.md  # This document
â””â”€â”€ CLAUDE.md                            # Project context for Claude Code
```

### Key Files Deep Dive

#### ðŸ“„ `src/zipline/data/custom/pipeline_integration.py`

**Purpose**: Core implementation of the custom data loader for Zipline Pipeline.

**Key Classes**:
- `CustomSQLiteLoader(PipelineLoader)`: Loads custom data from SQLite into Pipeline
  - `load_adjusted_array()`: Main entry point called by Pipeline engine
  - `_query_database()`: Executes SQL queries and returns 2D numpy arrays

**Critical Implementation Details**:
- **Dtype Handling**: Numeric columns (float64, int64) vs object columns (str) require different handling
- **Missing Values**:
  - Numeric: `np.nan`
  - Text/Object: `''` (empty string, NOT None or NaN)
- **AdjustedArray Creation**: Must use matching `missing_value` based on dtype
- **Type Conversion**: SQLite may return numeric data as strings â†’ explicit `pd.to_numeric()` conversion

**Lines of Interest**:
- Line 144-252: `load_adjusted_array()` - Main pipeline integration
- Line 254-452: `_query_database()` - SQL query and data transformation
- Line 245: `missing_val = '' if column.dtype == object else column.missing_value` (CRITICAL for text columns)
- Line 387: `reindexed.fillna('')` for object columns (prevents mixed str/float types)

---

#### ðŸ“„ `notebooks/load_csv_fundamentals.ipynb`

**Purpose**: Complete workflow for loading custom fundamentals data into Zipline.

**Notebook Structure**:

1. **Setup & Imports** (Cells 1-3)
   - Imports zipline custom data modules
   - Configures logging

2. **Load CSV Data** (Cells 4-6)
   - Reads REFE fundamentals CSV
   - Shows data structure and available columns

3. **Transform & Clean** (Cells 7-12)
   - Maps tickers to Zipline sids using asset_finder
   - Handles date formatting (YYYY-MM-DD)
   - Filters valid date range

4. **Database Schema** (Cells 13-15)
   - Defines column types (float, int, text)
   - Creates SQLite database with proper schema

5. **â­ Critical Data Loading** (Cell 16)
   ```python
   # CRITICAL: Handle text columns to prevent dtype errors
   for col in text_columns:
       custom_data[col].fillna('', inplace=True)  # Empty string, not NaN

   # Insert with explicit dtype preservation
   custom_data.to_sql('Price', conn, if_exists='replace', index=False, dtype=column_types_sql)
   ```

   **Why this matters**: If text columns contain NaN, they become mixed str/float types in numpy arrays,
   which breaks Zipline's LabelArray/Categorical system with error: "Categorical categories cannot be null"

6. **Verify Database** (Cells 17-20)
   - Check data was loaded correctly
   - Verify dtypes match expectations
   - Sample queries

7. **Pipeline Setup** (Cells 21-25)
   - Define `REFEFundamentals` Database class
   - Implement `get_pipeline_loader()` factory
   - Create SimplePipelineEngine with custom loader

8. **Pipeline Examples** (Cells 26-34)
   - Example 1: Top 10 by ROE
   - Example 2: Filter by sector
   - Example 3: Combine multiple factors

9. **Time Series Analysis** (Cells 35-38)
   - Plot individual stock fundamentals over time
   - **Important**: Uses `reset_index(names=['date', 'asset'])` to handle unnamed MultiIndex
   - **Important**: Uses `get_level_values(1)` to access asset level of MultiIndex

10. **Strategy Example** (Removed in commit d488870)
    - Previous cells 39-45 had broken backtest implementation
    - Now replaced with pointer to `strategy_top5_roe.py`

**Key Cell Fixes**:
- **Cell 16** (commit 71949ca): Added `fillna('')` for text columns before SQL insert
- **Cell 35** (commit 5c69b4a): Changed to `get_level_values(1)` instead of `get_level_values('asset')`
- **Cell 37** (commit 62eac90): Changed to `reset_index(names=['date', 'asset'])` for clean column names

---

#### ðŸ“„ `notebooks/strategy_top5_roe.py`

**Purpose**: Standalone executable strategy demonstrating custom fundamentals in backtests.

**Architecture**:

1. **Database Definition** (Lines 44-58)
   ```python
   class REFEFundamentals(Database):
       CODE = "refe-fundamentals"
       LOOKBACK_WINDOW = 252  # One year of trading days

       # Column definitions with proper dtypes
       ReturnOnEquity_SmartEstimat = Column(float)
       CompanyMarketCap = Column(float)
       GICSSectorName = Column(str)  # Note: object dtype
       # ... more columns
   ```

2. **Loader Factory Pattern** (Lines 67-95)
   ```python
   _loader_cache = {}

   def get_pipeline_loader(column):
       """Route columns to appropriate loaders."""
       dataset = column.dataset

       # Route custom fundamentals
       if 'REFEFundamentals' in str(dataset):
           if cache_key not in _loader_cache:
               _loader_cache[cache_key] = CustomSQLiteLoader(
                   db_code=REFEFundamentals.CODE,
                   db_dir=Path.home() / '.zipline' / 'data' / 'custom'
               )
           return _loader_cache[cache_key]

       # Route pricing data to bundle loader
       if column in USEquityPricing.columns:
           if 'pricing' not in _loader_cache:
               bundle_data = load_bundle('sharadar')
               _loader_cache['pricing'] = USEquityPricingLoader(...)
           return _loader_cache['pricing']
   ```

   **Why this pattern**: Allows mixing custom fundamentals with bundle price data in same pipeline.

3. **Pipeline Definition** (Lines 98-120)
   ```python
   def make_pipeline():
       roe = REFEFundamentals.ReturnOnEquity_SmartEstimat.latest
       market_cap = REFEFundamentals.CompanyMarketCap.latest

       # Screen: top 100 by market cap
       top_100_by_mcap = market_cap.top(100)

       # Select top 5 by ROE from top 100
       top_5_roe = roe.top(5, mask=top_100_by_mcap)

       return Pipeline(
           columns={'ROE': roe, 'Market_Cap': market_cap, 'Sector': sector},
           screen=top_5_roe,
       )
   ```

4. **Strategy Logic** (Lines 127-205)
   - `initialize()`: Attach pipeline, schedule weekly rebalancing
   - `before_trading_start()`: Get pipeline output each day
   - `rebalance()`: Equal-weight top 5 stocks
   - `handle_data()`: Record daily metrics

5. **â­ Monkey-Patch Pattern** (Lines 213-258)
   ```python
   # Create custom engine with our loader
   engine = SimplePipelineEngine(
       get_loader=get_pipeline_loader,
       asset_finder=bundle_data.asset_finder,
       default_domain=US_EQUITIES,
   )

   # Inject into run_algorithm()
   import zipline.algorithm
   original_make_engine = zipline.algorithm.SimplePipelineEngine
   zipline.algorithm.SimplePipelineEngine = lambda *args, **kwargs: engine

   # Run backtest
   results = run_algorithm(
       start=pd.Timestamp('2025-10-01'),  # NO tz parameter!
       end=pd.Timestamp('2025-11-05'),
       initialize=initialize,
       before_trading_start=before_trading_start,
       handle_data=handle_data,
       capital_base=100000,
       data_frequency='daily',
       bundle='sharadar',
   )

   # Restore original
   zipline.algorithm.SimplePipelineEngine = original_make_engine
   ```

   **Why monkey-patching**: `run_algorithm()` creates its own SimplePipelineEngine internally,
   which doesn't know about our custom loaders. Monkey-patching is the cleanest way to inject
   our pre-configured engine without modifying Zipline core code.

**Key Fixes**:
- **Commit b120aec**: Removed `tz='UTC'` from timestamps (breaks backtests)
- **Commit b120aec**: Updated date range to 2025-10-01 â†’ 2025-11-05 (matches available data)

---

## Key Components

### Database Schema

**Table Structure**: Single table named `Price` (historical convention)

```sql
CREATE TABLE Price (
    Sid INTEGER NOT NULL,           -- Zipline asset ID
    Date TEXT NOT NULL,             -- Format: 'YYYY-MM-DD'
    ReturnOnEquity_SmartEstimat REAL,
    CompanyMarketCap REAL,
    GICSSectorName TEXT,            -- Text columns for categorical data
    -- ... other columns
    PRIMARY KEY (Date, Sid)
);

CREATE INDEX idx_date_sid ON Price(Date, Sid);  -- Critical for performance
```

**Design Decisions**:
- **Sid (not Ticker)**: Zipline uses integer sids internally; more robust than tickers
- **Date as TEXT**: SQLite date handling; converted to datetime in Python
- **Denormalized**: All columns in one table for query performance
- **Indexed**: (Date, Sid) index enables fast range queries

### Custom Database Class Pattern

**Template**:
```python
from zipline.pipeline.data.db import Database, Column

class MyFundamentals(Database):
    """Custom fundamentals database."""

    CODE = "my-fundamentals"           # Database identifier
    LOOKBACK_WINDOW = 252              # Days of historical data to load

    # Numeric columns
    Revenue = Column(float)            # Uses np.nan for missing
    Employees = Column(int)            # Uses -1 for missing

    # Text/categorical columns
    Sector = Column(str)               # Uses '' for missing
    Country = Column(str)
```

**Column Type Mapping**:
| Python Type | NumPy Dtype | Missing Value | SQL Type |
|-------------|-------------|---------------|----------|
| `float`     | `float64`   | `np.nan`      | `REAL`   |
| `int`       | `int64`     | `-1`          | `INTEGER`|
| `str`       | `object`    | `''`          | `TEXT`   |

### CustomSQLiteLoader Implementation

**Core Methods**:

1. **`load_adjusted_array(domain, columns, dates, sids, mask)`**
   - Called by Pipeline engine for each query
   - Parameters:
     - `columns`: List of BoundColumn objects to load
     - `dates`: pd.DatetimeIndex of dates in query range
     - `sids`: pd.Int64Index of asset IDs
     - `mask`: Boolean array for active assets
   - Returns: Dict mapping BoundColumn â†’ AdjustedArray

2. **`_query_database(dates, sids, columns)`**
   - Executes SQL query for date/sid range
   - Converts DataFrame to 2D numpy arrays
   - Handles dtype conversions and missing values
   - Returns: Dict mapping column_name â†’ np.ndarray

**Critical Type Handling Logic**:

```python
# For object/text columns, fill NaN BEFORE array conversion
if col_dtype == object:
    reindexed = reindexed.fillna('')  # Empty string, not NaN or None

# Create array
arr = reindexed.values

# For AdjustedArray, use matching missing_value
missing_val = '' if column.dtype == object else column.missing_value
arrays[column] = AdjustedArray(
    data=arr,
    adjustments={},
    missing_value=missing_val,
)
```

**Why this is critical**: Zipline's LabelArray (used for categorical data) converts to pandas Categorical,
which requires all categories to be non-null. Mixed str/float arrays cause:
```
ValueError: Categorical categories cannot be null
```

---

## Data Flow

### End-to-End Example: Top 5 ROE Strategy

**Step 1: Data Loading (notebook)**
```python
# CSV â†’ Pandas DataFrame
df = pd.read_csv('fundamentals.csv')

# Map tickers to sids
df['Sid'] = df['Ticker'].map(ticker_to_sid_map)

# Clean text columns (CRITICAL!)
df['GICSSectorName'].fillna('', inplace=True)

# Save to SQLite
df.to_sql('Price', conn, if_exists='replace', index=False)
```

**Step 2: Database Storage**
```
~/.zipline/data/custom/refe-fundamentals.db
â”‚
â””â”€â”€ Price table
    â”œâ”€â”€ (2025-10-01, sid=1234) â†’ {ROE: 15.5, MarketCap: 1.2e9, Sector: 'Technology'}
    â”œâ”€â”€ (2025-10-01, sid=5678) â†’ {ROE: 12.3, MarketCap: 5.5e8, Sector: 'Healthcare'}
    â””â”€â”€ ...
```

**Step 3: Pipeline Query (strategy)**
```python
# Define pipeline
roe = REFEFundamentals.ReturnOnEquity_SmartEstimat.latest
market_cap = REFEFundamentals.CompanyMarketCap.latest

top_100 = market_cap.top(100)
top_5_roe = roe.top(5, mask=top_100)

pipeline = Pipeline(
    columns={'ROE': roe, 'Market_Cap': market_cap},
    screen=top_5_roe
)
```

**Step 4: Engine Execution**
```python
# engine.run_pipeline(pipeline, start_date, end_date)
#
# For each date:
#   1. Get list of all assets in domain
#   2. Call get_pipeline_loader(column) for each column
#   3. CustomSQLiteLoader.load_adjusted_array() is called
#   4. SQL query: SELECT Sid, Date, ROE, Market_Cap FROM Price WHERE Date = ? AND Sid IN (...)
#   5. Pivot to 2D array: rows=dates, cols=sids
#   6. Apply pipeline filters/factors
#   7. Return filtered DataFrame
```

**Step 5: Strategy Execution**
```python
def before_trading_start(context, data):
    # Get top 5 stocks for today
    context.pipeline_data = pipeline_output('roe_strategy')
    # Example result:
    #              ROE  Market_Cap        Sector
    # 2025-10-01
    #   Equity(1234)  15.5  1.2e9     Technology
    #   Equity(5678)  14.2  8.5e8     Healthcare
    #   ...

def rebalance(context, data):
    stocks = context.pipeline_data.index
    weight = 1.0 / len(stocks)

    for stock in stocks:
        order_target_percent(stock, weight)
```

**Step 6: Backtest Results**
```
Final Portfolio Value: $112,450.00
Total Return: 12.45%
Sharpe Ratio: 1.85
Max Drawdown: -8.32%
```

---

## Implementation Details

### Dtype Handling: The Critical Issue

**Problem**: Mixed str/float types in object arrays break Zipline's categorical handling.

**Scenario**:
```python
# What happens without proper fillna:
df['Sector'] = ['Technology', 'Healthcare', np.nan, 'Energy']
arr = df.values  # Creates object array
# arr contains: ['Technology', 'Healthcare', nan, 'Energy']
# Types: [str, str, float, str]  â† MIXED TYPES!

# Zipline tries to create LabelArray
LabelArray(arr, missing_value='')  # â† Creates pandas Categorical
# pandas.Categorical(['Technology', 'Healthcare', nan, 'Energy'])
# âŒ ValueError: Categorical categories cannot be null
```

**Solution**:
```python
# Fill NaN with empty string BEFORE creating array
df['Sector'].fillna('', inplace=True)
arr = df.values
# arr contains: ['Technology', 'Healthcare', '', 'Energy']
# Types: [str, str, str, str]  â† ALL STRINGS!

# Now LabelArray works
LabelArray(arr, missing_value='')  # âœ“ Success
```

**Implementation Locations**:
1. **Data Loading** (`load_csv_fundamentals.ipynb` Cell 16):
   ```python
   for col in text_columns:
       custom_data[col].fillna('', inplace=True)
   ```

2. **Pipeline Loading** (`pipeline_integration.py` Line 387):
   ```python
   if col_dtype == object:
       reindexed = reindexed.fillna('')
   ```

3. **AdjustedArray Creation** (`pipeline_integration.py` Line 245):
   ```python
   missing_val = '' if column.dtype == object else column.missing_value
   ```

### Index Naming in Pipeline Results

**Problem**: Pipeline MultiIndex results have unnamed levels `[None, None]`.

**Manifestation**:
```python
result = engine.run_pipeline(pipeline, start, end)
print(result.index.names)  # [None, None]

# This fails:
result.index.get_level_values('asset')  # âŒ KeyError: 'Level asset not found'
```

**Solutions**:

**Option 1**: Use numeric level indices
```python
assets = result.index.get_level_values(1)  # Second level is assets
dates = result.index.get_level_values(0)   # First level is dates
```

**Option 2**: Name the levels explicitly (cleaner)
```python
result_named = result.reset_index(names=['date', 'asset'])
# Now you can use:
result_named['date']
result_named['asset']
```

**Implementation**: `load_csv_fundamentals.ipynb` Cell 37 (commit 62eac90)

### Timestamp Timezone Handling

**Problem**: Zipline is sensitive to timezone-aware vs timezone-naive timestamps.

**Don't Do**:
```python
start = pd.Timestamp('2025-10-01', tz='UTC')  # âŒ Breaks in some contexts
```

**Do**:
```python
start = pd.Timestamp('2025-10-01')  # âœ“ Zipline handles TZ internally
```

**Why**: `run_algorithm()` internally handles timezone conversion based on trading calendar.
Passing `tz='UTC'` can cause mismatches between calendar timezone and data timezone.

**Implementation**: `strategy_top5_roe.py` Lines 236-237 (commit b120aec)

### Caching Pattern for Loaders

**Problem**: Creating new loaders on every column lookup is inefficient.

**Solution**: Global cache dictionary
```python
_loader_cache = {}

def get_pipeline_loader(column):
    dataset = column.dataset

    if 'REFEFundamentals' in str(dataset):
        cache_key = REFEFundamentals.CODE
        if cache_key not in _loader_cache:
            _loader_cache[cache_key] = CustomSQLiteLoader(
                db_code=REFEFundamentals.CODE,
                db_dir=db_dir
            )
        return _loader_cache[cache_key]

    # Similar for pricing loader
    if column in USEquityPricing.columns:
        if 'pricing' not in _loader_cache:
            bundle_data = load_bundle('sharadar')
            _loader_cache['pricing'] = USEquityPricingLoader(...)
        return _loader_cache['pricing']
```

**Benefits**:
- Single database connection reused across pipeline execution
- Avoids repeated bundle loading
- Significant performance improvement for multi-date pipelines

---

## Usage Workflow

### Initial Setup (One-Time)

1. **Prepare CSV Data**
   - Format: Columns include Ticker/Symbol, Date, fundamental metrics
   - Example: `REFE_fundamentals.csv`

2. **Load Notebook**
   ```bash
   jupyter notebook notebooks/load_csv_fundamentals.ipynb
   ```

3. **Execute Loading Cells** (Cells 1-20)
   - Adjust file path to your CSV
   - Review column mappings (text vs numeric)
   - Verify database creation

4. **Test Pipeline** (Cells 21-34)
   - Run example queries
   - Verify data is accessible
   - Check for dtype errors

### Strategy Development

**Option A: Notebook Development** (Recommended for exploration)
```python
# Use cells 21-34 pattern
from zipline.pipeline import Pipeline
from zipline.pipeline.engine import SimplePipelineEngine

# Define pipeline
pipeline = Pipeline(...)

# Run query
results = engine.run_pipeline(pipeline, start_date, end_date)

# Analyze results
print(results.head())
results['ROE'].plot()
```

**Option B: Standalone Script** (Recommended for production)
```bash
# Copy and modify strategy_top5_roe.py
cp notebooks/strategy_top5_roe.py my_strategy.py

# Edit strategy logic
# Run backtest
python my_strategy.py
```

### Updating Data

**Full Refresh**:
```python
# In notebook Cell 16, change:
custom_data.to_sql('Price', conn, if_exists='replace', ...)  # Replaces all data
```

**Append New Data**:
```python
# Load new data only
new_data = pd.read_csv('fundamentals_2025_Q4.csv')
# ... apply same transformations ...
new_data.to_sql('Price', conn, if_exists='append', ...)
```

**Note**: Ensure no duplicate (Date, Sid) pairs when appending.

---

## Known Issues & Solutions

### Issue 1: "Categorical categories cannot be null"

**Error Message**:
```
ValueError: Categorical categories cannot be null
```

**Root Cause**: Text columns contain NaN values, creating mixed str/float object arrays.

**Solution**: Apply in **two places**:

1. **Data Loading** (Cell 16):
   ```python
   for col in text_columns:
       custom_data[col].fillna('', inplace=True)
   ```

2. **Pipeline Integration** (already fixed in `pipeline_integration.py`):
   ```python
   if col_dtype == object:
       reindexed = reindexed.fillna('')
   ```

**Commits**: 71949ca, earlier fixes

---

### Issue 2: "KeyError: 'Level asset not found'"

**Error Message**:
```
KeyError: "Level asset not found"
```

**Root Cause**: Pipeline results have unnamed MultiIndex levels.

**Solution**:
```python
# Instead of:
assets = result.index.get_level_values('asset')  # âŒ

# Use:
assets = result.index.get_level_values(1)  # âœ“

# Or name the levels:
result = result.reset_index(names=['date', 'asset'])  # âœ“
```

**Commits**: 5c69b4a, 2e98946, 62eac90

---

### Issue 3: Timezone-related backtest failures

**Symptoms**: Backtest fails with date/timezone mismatches.

**Root Cause**: Passing `tz='UTC'` to `pd.Timestamp()` conflicts with Zipline's internal TZ handling.

**Solution**:
```python
# Don't:
start = pd.Timestamp('2025-10-01', tz='UTC')  # âŒ

# Do:
start = pd.Timestamp('2025-10-01')  # âœ“
```

**Commit**: b120aec

---

### Issue 4: Empty Pipeline Results

**Symptoms**: `pipeline_output()` returns empty DataFrame.

**Debugging Steps**:

1. **Check data exists in database**:
   ```python
   import sqlite3
   conn = sqlite3.connect('~/.zipline/data/custom/refe-fundamentals.db')
   df = pd.read_sql('SELECT COUNT(*) FROM Price', conn)
   print(df)  # Should show row count
   ```

2. **Check date range**:
   ```python
   df = pd.read_sql('SELECT MIN(Date), MAX(Date) FROM Price', conn)
   print(df)  # Verify your backtest dates are in this range
   ```

3. **Check sids**:
   ```python
   df = pd.read_sql('SELECT DISTINCT Sid FROM Price LIMIT 10', conn)
   print(df)  # Verify sids are valid integers
   ```

4. **Verify loader routing**:
   ```python
   # Add debug logging in get_pipeline_loader
   def get_pipeline_loader(column):
       print(f"Loading column: {column}, dataset: {column.dataset}")
       # ...
   ```

---

### Issue 5: "TradingAlgorithm missing sim_params"

**Symptoms**: Direct use of `TradingAlgorithm()` fails.

**Root Cause**: `TradingAlgorithm` requires manual setup of SimulationParameters, calendar, data portal, etc.

**Solution**: Use `run_algorithm()` instead, which handles setup automatically:
```python
# Don't:
algo = TradingAlgorithm(...)  # âŒ Complex setup required

# Do:
results = run_algorithm(...)  # âœ“ Handles setup automatically
```

**Commit**: Earlier work, final solution in 9b5de06

---

### Issue 6: "KeyError: 'roe_strategy'" (Pipeline Cache)

**Symptoms**: `pipeline_output('roe_strategy')` fails with KeyError.

**Root Cause**: `run_algorithm()` creates its own SimplePipelineEngine, doesn't know about custom loaders.

**Solution**: Monkey-patch pattern (see `strategy_top5_roe.py`):
```python
# Create custom engine
engine = SimplePipelineEngine(get_loader=get_pipeline_loader, ...)

# Inject into run_algorithm
import zipline.algorithm
zipline.algorithm.SimplePipelineEngine = lambda *args, **kwargs: engine

results = run_algorithm(...)

# Restore
zipline.algorithm.SimplePipelineEngine = original_make_engine
```

**Commit**: 9b5de06

---

## Recent Fixes & Commits

### Commit History (Chronological)

**1. ace81c7** - `fix: Replace run_algorithm with manual backtest simulation`
- Attempted to use TradingAlgorithm directly
- Later reverted in favor of monkey-patch approach

**2. fb7bb29** - `fix: Use TradingAlgorithm with custom pipeline engine`
- Added sim_params setup
- Still had issues with run_algorithm integration

**3. 564de15** - `fix: Add sim_params to TradingAlgorithm initialization`
- Continued attempts to make TradingAlgorithm work
- Recognized complexity of manual setup

**4. d488870** - `refactor: Remove broken backtest cells, add usage note`
- Removed broken Cells 39-45 from notebook
- Added pointer to standalone strategy file

**5. 9b5de06** - `feat: Add working strategy file for custom fundamentals backtesting`
- Created `strategy_top5_roe.py`
- Implemented monkey-patch pattern for run_algorithm
- **This is the working solution**

**6. 5c69b4a** - `fix: Use positional index for MultiIndex levels in pipeline results`
- Fixed Cell 35 to use `get_level_values(1)` instead of `get_level_values('asset')`

**7. 2e98946** - `fix: Use correct column name from reset_index in time series plot`
- Fixed Cell 37 to use `sym_data['level_0']` for dates

**8. 62eac90** - `fix: Use named reset_index for cleaner time series handling`
- Improved Cell 37 to use `reset_index(names=['date', 'asset'])`
- User suggestion, much cleaner solution

**9. 71949ca** - `fix: Set correct missing_value for object dtype columns in AdjustedArray`
- Critical fix for text column dtype errors
- Set `missing_value=''` for object columns in AdjustedArray creation
- Fixed GICSSectorName categorical error

**10. b120aec** - `chore: Remove verbose logging and fix backtest dates`
- Removed all DEBUG and verbose logging from pipeline_integration.py
- Removed `tz='UTC'` from timestamps in strategy file
- Updated date range to 2025-10-01 â†’ 2025-11-05

### Key Learnings from Development

1. **Text columns require special handling**: Empty string ('') not NaN or None
2. **Simplicity wins**: User suggestions (e.g., `reset_index(names=[...])`) often cleaner than complex solutions
3. **Monkey-patching is acceptable**: For injecting custom engines into run_algorithm()
4. **Timezone handling is subtle**: Let Zipline handle TZ internally
5. **Logging cleanup matters**: Verbose debug output is helpful during development, noise in production

---

## Testing & Examples

### Unit Tests

**Location**: `tests/zipline/data/test_custom_pipeline.py`

**Key Test Cases**:
1. Database creation and schema validation
2. CustomSQLiteLoader basic loading
3. Dtype handling (float, int, object)
4. Missing value handling
5. Pipeline integration end-to-end

**Running Tests**:
```bash
# All custom data tests
pytest tests/zipline/data/test_custom_pipeline.py -v

# Specific test
pytest tests/zipline/data/test_custom_pipeline.py::test_object_dtype_handling -v
```

### Example Pipelines

**Example 1: Top N by Single Factor**
```python
roe = REFEFundamentals.ReturnOnEquity_SmartEstimat.latest
top_10 = roe.top(10)

pipeline = Pipeline(
    columns={'ROE': roe},
    screen=top_10
)
```

**Example 2: Sector Filter**
```python
sector = REFEFundamentals.GICSSectorName.latest
tech_stocks = (sector == 'Technology')

pipeline = Pipeline(
    columns={'Sector': sector},
    screen=tech_stocks
)
```

**Example 3: Combined Filters**
```python
roe = REFEFundamentals.ReturnOnEquity_SmartEstimat.latest
roa = REFEFundamentals.ReturnOnAssets_SmartEstimate.latest
market_cap = REFEFundamentals.CompanyMarketCap.latest

# Universe: Large caps
large_cap = market_cap.top(200)

# High profitability
profitable = (roe > 10) & (roa > 5)

# Combined screen
screen = large_cap & profitable

pipeline = Pipeline(
    columns={'ROE': roe, 'ROA': roa, 'MarketCap': market_cap},
    screen=screen
)
```

**Example 4: Ranking**
```python
roe = REFEFundamentals.ReturnOnEquity_SmartEstimat.latest
market_cap = REFEFundamentals.CompanyMarketCap.latest

# Rank stocks by ROE within top 500 market cap
universe = market_cap.top(500)
roe_rank = roe.rank(mask=universe)

pipeline = Pipeline(
    columns={
        'ROE': roe,
        'ROE_Rank': roe_rank,
    },
    screen=universe
)
```

### Validation Queries

**Check Data Loaded Correctly**:
```python
import sqlite3
conn = sqlite3.connect('~/.zipline/data/custom/refe-fundamentals.db')

# Count rows
pd.read_sql('SELECT COUNT(*) FROM Price', conn)

# Date range
pd.read_sql('SELECT MIN(Date), MAX(Date) FROM Price', conn)

# Sample data
pd.read_sql('SELECT * FROM Price LIMIT 10', conn)

# Check for NULLs in text columns (should be none!)
pd.read_sql("SELECT COUNT(*) FROM Price WHERE GICSSectorName IS NULL OR GICSSectorName = ''", conn)
```

**Verify Pipeline Results**:
```python
# Run simple pipeline
results = engine.run_pipeline(pipeline, start_date, end_date)

# Check shape
print(f"Results shape: {results.shape}")

# Check for NaNs
print(f"NaN counts:\n{results.isna().sum()}")

# Check dtypes
print(f"Dtypes:\n{results.dtypes}")

# Sample results
print(results.head())
```

---

## Appendix: Quick Reference

### File Paths
```
~/.zipline/data/custom/                    # Custom database directory
~/.zipline/data/custom/refe-fundamentals.db  # Database file
```

### Common Commands
```bash
# List databases
ls ~/.zipline/data/custom/*.db

# Check database size
du -h ~/.zipline/data/custom/refe-fundamentals.db

# SQLite CLI
sqlite3 ~/.zipline/data/custom/refe-fundamentals.db
```

### Import Snippets

**Basic Setup**:
```python
from zipline.pipeline import Pipeline
from zipline.pipeline.engine import SimplePipelineEngine
from zipline.pipeline.data.db import Database, Column
from zipline.data.custom import CustomSQLiteLoader
from zipline.data.bundles import load as load_bundle
from zipline.pipeline.domain import US_EQUITIES
```

**Database Definition**:
```python
class MyFundamentals(Database):
    CODE = "my-fundamentals"
    LOOKBACK_WINDOW = 252
    Revenue = Column(float)
    Sector = Column(str)
```

**Loader Factory**:
```python
def get_pipeline_loader(column):
    if 'MyFundamentals' in str(column.dataset):
        return CustomSQLiteLoader(
            db_code=MyFundamentals.CODE,
            db_dir=Path.home() / '.zipline' / 'data' / 'custom'
        )
    if column in USEquityPricing.columns:
        bundle_data = load_bundle('sharadar')
        return USEquityPricingLoader(
            bundle_data.equity_daily_bar_reader,
            bundle_data.adjustment_reader
        )
    raise ValueError(f"No loader for {column}")
```

**Engine Creation**:
```python
bundle_data = load_bundle('sharadar')
engine = SimplePipelineEngine(
    get_loader=get_pipeline_loader,
    asset_finder=bundle_data.asset_finder,
    default_domain=US_EQUITIES,
)
```

---

## Summary

This custom fundamentals system enables seamless integration of third-party fundamental data into Zipline's powerful Pipeline framework. The architecture is:

- **Modular**: Separate data loading, pipeline integration, and strategy logic
- **Extensible**: Easy to add new data sources via Database classes
- **Type-safe**: Explicit dtype handling prevents runtime errors
- **Performant**: SQLite indexing and loader caching
- **Production-ready**: Validated through unit tests and example strategies

**Key Success Factors**:
1. Proper text column handling (empty string for missing)
2. Loader factory pattern for mixing custom + bundle data
3. Monkey-patch pattern for run_algorithm integration
4. Timezone-naive timestamps
5. Clean logging for production use

**Next Steps for New Sessions**:
- Review this document for full context
- Check recent commits for latest fixes
- Run notebook cells 1-34 to verify setup
- Test strategy file with `python notebooks/strategy_top5_roe.py`
- Extend with your own Database classes and strategies

---

**Document Version**: 1.0
**Last Updated**: 2025-11-14
**Maintainer**: Auto-generated from session context
**Related**: CLAUDE.md, notebooks/load_csv_fundamentals.ipynb, notebooks/strategy_top5_roe.py
