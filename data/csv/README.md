# ML-Based Stock Return Forecasting

Fast, production-ready machine learning system for predicting stock returns using fundamental data.

## üéØ Overview

This tool uses **Histogram-based Gradient Boosting** with extensive feature engineering to predict stock returns at customizable horizons (10-day, 90-day, etc.). It's optimized for institutional trading strategies with:

- ‚úÖ **No look-ahead bias** - Forward-fill per symbol, proper lagging
- ‚úÖ **Flexible data support** - Works with any fundamental columns (LSEG, FMP, Sharadar, custom)
- ‚úÖ **Scalable features** - Handles 70-290+ features automatically based on your data
- ‚úÖ **Market cap weighting** - Focused training on large-cap stocks
- ‚úÖ **80%+ correlation** - Excellent predictive power
- ‚úÖ **Fast execution** - Processes millions of rows in seconds
- ‚úÖ **Lean output** - Only adds 2 columns to original data
- ‚úÖ **Complete logging** - Auto-generated log files for reproducibility
- ‚úÖ **Pre-lagged data support** - Use your own lagging pipeline

## üÜï What's New in v3.2.2 (2026-01-07)

### üéØ Fully Deterministic Design - ZERO Randomness:
- **100% reproducible results** - Eliminated ALL random number generation
- **2-5% faster** - No RNG overhead from deterministic sampling
- **Simpler code** - No seed management needed
- **Perfect reproducibility** - Same input ‚Üí same output, every time
- **Fixed critical bug** - Resolved 5.62% prediction difference between identical runs

### ‚ö° Major Performance Optimizations (v3.2.0):
- **10-30x faster alignment** - Vectorized pandas merge instead of iterrows() loops (5-10 min ‚Üí 10-30 sec)
- **85% less merge memory** - Merge before feature engineering (2 GB ‚Üí 300 MB temp df)
- **3-5x faster CSV reading** - PyArrow engine for high-performance parsing
- **5-10x faster I/O** - Parquet format support (auto-detection by file extension)
- **10x smaller files** - Parquet compression reduces 50 MB CSV to 5 MB

### üîß Simplified Resume Logic (v3.2.0):
- **Removed checkpoint JSON** - No more confusing `.json` files and automatic renaming
- **Simple `--resume-file`** - Just point to previous predictions CSV/Parquet
- **Explicit flags** - Changed from positional `input` to `--input-file` and `--output`
- **Cleaner workflow** - 300 lines of complexity ‚Üí 83 lines of clean code

### New Features:
- **Auto-export forecast CSV** - Automatically creates `YYYYMMDD_YYYYMMDD_forecast_only.csv` after each run
- **Automatic date cleanup** - Removes erroneous future-dated records based on filename
- **Parquet support** - Works with `.parquet` and `.pq` files for faster I/O

### Impact:
- **Weekly updates**: 30 seconds to 2 minutes (vs 5-12 minutes before)
- **100% look-ahead bias free** - All optimizations mathematically verified safe
- **Simpler workflow** - No more checkpoint confusion or file renaming
- **Better storage** - 10x smaller prediction files with Parquet format

**See CHANGELOG.md for complete v3.2.0 details**

---

## üÜï What's New in v3.1 (2025-12-28)

### Critical Bug Fixes:
- **Fixed look-ahead bias** in missing value handling - Now uses forward-fill per symbol instead of median across all dates
- **Fixed `--no-lag` bug** - Raw LSEG fundamentals now correctly included as features when using pre-lagged data

### New Features:
- **PCA dimensionality reduction** with `--pca N` - Reduce features to N components while preserving variance
- **Feature descriptions** - Detailed list of all training features logged at start with human-readable descriptions
- **Automatic logging** with `--log-file` - All output saved to timestamped log files
- **Performance optimization** with `--sample-fraction` - Train on subset for 2-5x speedup
- **Pre-lagged data support** with `--no-lag` - Use your own data preparation pipeline
- **Auto-normalization** - Automatically handles lowercase column names (converts to PascalCase)

### Impact:
- **Better accuracy** - Forward-fill is point-in-time accurate (no future data contamination)
- **Faster training** - Sample 50% of data for 2x speedup with minimal accuracy loss
- **Full audit trail** - Complete logs for every run

---

## üöÄ Production Deployment Guide

### ‚úÖ RECOMMENDED: Walk-Forward Mode (Zero Look-Ahead Bias)

**For live trading and production use, ALWAYS use walk-forward mode:**

```bash
# First run - full training
python forecast_returns_ml_walk_forward.py \
    --input-file your_data.csv \
    --output predictions.parquet \
    --forecast-days 10 \
    --target-return-days 90

# Weekly updates - fast resume (recommended)
python forecast_returns_ml_walk_forward.py \
    --input-file updated_data.csv \
    --output updated_predictions.parquet \
    --resume-file predictions.parquet \
    --overwrite-months 1 \
    --forecast-days 10 \
    --target-return-days 90
```

**Why walk-forward is critical:**
- ‚úÖ **Realistic backtesting** - Mimics real-world deployment
- ‚úÖ **Zero look-ahead bias** - Each month uses only past data
- ‚úÖ **Repeatable** - Same input produces identical results
- ‚úÖ **Production-ready** - Exactly how model will perform live

### ‚ùå DO NOT Use in Production

**Avoid these flags for live trading:**

1. **`--pca`** - Has look-ahead bias in walk-forward mode
   - PCA fits on ALL training data (including future months)
   - Model sees future feature distributions
   - Use raw features instead (290 features work well)

2. **`--no-walk-forward`** - Single model with look-ahead bias
   - Trains one model on all historical data
   - Uses future information for validation
   - Unrealistic performance estimates

### Production Safety Checklist

Before deploying to live trading:

- [x] ‚úÖ Using `forecast_returns_ml_walk_forward.py` (walk-forward script)
- [x] ‚úÖ NO `--pca` flag
- [x] ‚úÖ NO `--no-walk-forward` flag
- [x] ‚úÖ Features properly lagged (automatic with script)
- [x] ‚úÖ Resume mode tested (`--resume-file`)
- [x] ‚úÖ Prediction files saved for quick updates (Parquet recommended)

### Look-Ahead Bias Protection

**The walk-forward script automatically protects against:**

| Component | Protection | Status |
|-----------|-----------|--------|
| Feature engineering | T-1 lagging on all fundamentals | ‚úÖ Safe |
| Inf/NaN handling | Row-level replacement (no statistics) | ‚úÖ Safe |
| Outlier clipping | Disabled in walk-forward mode | ‚úÖ Safe |
| Walk-forward loop | Strict `Date < first_day_of_month` cutoff | ‚úÖ Safe |
| Model training | Each month trains on past data only | ‚úÖ Safe |
| PCA/StandardScaler | Disabled (would use future data) | ‚úÖ Safe |

### ‚ö†Ô∏è PCA Warning

If you see this warning, **DO NOT** use for production:

```
‚ö†Ô∏è  WARNING: PCA in walk-forward mode has look-ahead bias!
   PCA is fit on ALL training data (including future months)
   For production, use --no-walk-forward or remove --pca flag
```

**Solution:** Remove `--pca` flag from command.

### Performance Without PCA

**Good news:** HistGradientBoosting handles 290 features efficiently:
- Training time: Fast (no PCA overhead)
- Memory: Modest (fits in RAM easily)
- Accuracy: Better (preserves all signal)
- Production-safe: Zero look-ahead bias

---

## üìä Key Features

### 1. No Look-Ahead Bias
All fundamental features are **lagged by 1 day** to ensure we only use information available before making predictions. This prevents overfitting and ensures the model works in real trading.

```
Today (T): Use fundamentals from T-1
Predict:   Returns from T+10 to T+100 (90-day return)
```

### 2. Predictions for ALL Dates (Including Recent)
The model intelligently handles recent dates:
- **Training**: Uses only dates with valid `forward_return` (where future prices exist for validation)
- **Prediction**: Generates forecasts for ALL dates with fundamentals, including:
  - Historical dates (with `forward_return` for accuracy validation)
  - Recent dates (without `forward_return`, but perfect for live trading)

**This is critical for production use** - you get predictions for the most recent dates (where you actually trade) even though you can't validate them yet!

**Example:**
```
Date range: 2010-01-01 to 2024-12-09
Training samples: 39,570 (dates with future price data)
Predictions: 40,138 (100% coverage, including 568 recent dates)
```

### 3. Market Cap Weighted Training
The model focuses on **large-cap stocks** which are:
- More liquid and tradeable
- Less noisy in fundamental data
- More relevant for institutional strategies

**Weighting Scheme:**
- Top 2000 stocks: weight = 1.0 (full importance)
- Rank 2001-4000: weight = 0.5 (medium)
- Rank 4000+: weight = 0.1 (low but not ignored)

### 4. Flexible Return Periods
Predict any return horizon:
- **Short-term**: 10-20 day returns
- **Medium-term**: 30-60 day returns
- **Long-term**: 90-180 day returns

**Key Insight:** Longer periods = better predictions (fundamentals predict long-term better than short-term noise)

### 5. Extensive Feature Engineering (70-290+ features)

The number of features depends on your input data:
- **LSEG-only dataset**: ~70-100 features
- **Production dataset (LSEG + FMP + Sharadar)**: 290 features

**Price-based features:**
- Momentum: 5, 10, 20-day returns
- Volatility: Rolling volatility metrics
- Volume: Relative volume indicators

**Fundamental features:**
- All lagged fundamentals from your CSV (_lag1 versions)
- Derived ratios: ROE, ROA, EV/EBITDA, EV/EBIT, PEG, etc.
- Growth metrics: Long-term growth estimates, revenue growth, earnings growth
- Quality metrics: Alpha model rankings, earnings quality

**Cross-sectional:**
- Percentile ranks within each date
- Sector-relative metrics

**All features use lagged data (T-1) to prevent look-ahead bias!**

### 6. PCA Dimensionality Reduction (Optional)

> **‚ö†Ô∏è PRODUCTION WARNING:** PCA has look-ahead bias in walk-forward mode and is **NOT RECOMMENDED for live trading**. Use PCA only for exploration or with `forecast_returns_ml.py` (single model, non-walk-forward). For production deployment, skip PCA and use all 290 features - HistGradientBoosting handles them efficiently with zero look-ahead bias.

**What is PCA?**

Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms correlated features into a smaller set of uncorrelated components while preserving the most important variance in the data.

**Why Use PCA? (Exploration Only)**

- ‚úÖ **Reduce overfitting** - Fewer features mean less risk of fitting to noise
- ‚úÖ **Faster training** - Dramatically speeds up model training (5-10x faster)
- ‚úÖ **Handle multicollinearity** - Eliminates correlations between fundamental ratios
- ‚úÖ **Preserve signal** - Keeps 90-99% of variance with just 20-50 components
- ‚úÖ **Cleaner models** - Removes redundant information from highly correlated features

**When to Use PCA?**

| Scenario | Recommendation |
|----------|---------------|
| Initial exploration | Try `--pca 20` for fast iterations (non-walk-forward only) |
| **Production/Live Trading** | **‚ùå DO NOT USE PCA** - Use all 290 features |
| Research/Backtesting | `forecast_returns_ml.py` (single model) with `--pca 30-50` |
| Maximum accuracy | No PCA (use all features) |

**How to Use (Exploration Only):**

```bash
# FOR EXPLORATION ONLY - Use forecast_returns_ml.py (single model)
# NOT for production walk-forward backtesting

# Reduce 110+ features to 20 components (fast, ~90% variance)
python forecast_returns_ml.py data.csv --pca 20

# Balanced: 30 components (~95% variance)
python forecast_returns_ml.py data.csv --pca 30

# Maximum retention: 50 components (~98% variance)
python forecast_returns_ml.py data.csv --pca 50

# ‚ùå DO NOT USE: Walk-forward with PCA (has look-ahead bias)
# python forecast_returns_ml_walk_forward.py data.csv --pca 25
```

**Output Example:**

```
üî¨ Applying PCA dimensionality reduction...
  ‚Ä¢ Original features: 110
  ‚Ä¢ Target components: 20
  ‚Ä¢ Variance explained: 92.34%
  ‚Ä¢ Components shape: (39570, 20)
  ‚úì PCA transformation complete
```

**How It Works:**

1. **Fit PCA on training data** (only valid rows) to avoid look-ahead bias
2. **Transform all data** (training + prediction) using the fitted PCA
3. **Train model** on reduced feature space (PC1, PC2, ..., PCN)
4. **Make predictions** using the same PCA-transformed features

**Important Notes:**

- PCA is fit ONLY on training data to prevent look-ahead bias
- The same PCA transformation is applied to all prediction data
- For walk-forward: PCA is fit once on all training data, then reused consistently
- Feature importance will show PC1, PC2, etc. (not original feature names)
- Variance explained indicates how much information is preserved

**Performance Impact:**

| Components | Training Speed | Variance | Accuracy |
|------------|----------------|----------|----------|
| 10 | 10x faster | ~85% | Good for exploration |
| 20 | 6x faster | ~92% | Balanced speed/accuracy |
| 30 | 4x faster | ~95% | High accuracy |
| 50 | 2x faster | ~98% | Near-optimal accuracy |
| None (110+) | Baseline | 100% | Maximum accuracy |

## üî¨ Feature Engineering Pipeline (Detailed)

### Overview

The model is **flexible and data-agnostic** - it automatically processes ANY fundamental columns in your CSV. The number of features depends on your input data:

- **LSEG-only (32 columns)**: Creates ~70-100 engineered features
- **LSEG + FMP + Sharadar (200+ columns)**: Creates ~290 engineered features
- **Your custom data**: Creates features proportional to your input columns

**Key principle**: ALL fundamental columns are automatically lagged by 1 day to prevent look-ahead bias, then engineered into price momentum, ratios, rankings, and lag-2 features.

### Example Production Dataset (290 Features)

The example production runs use a combined dataset with **~240 raw columns** from multiple sources:

**1. FMP (Financial Modeling Prep) - ~170 columns**
- Income statement growth metrics (revenue, EBITDA, net income, EPS, etc.)
- Balance sheet growth metrics (assets, debt, equity, etc.)
- Cash flow growth metrics (operating CF, free CF, etc.)
- Example columns: `growthrevenue_fmp`, `growthnetincome_fmp`, `growebitda_fmp`

**2. LSEG (London Stock Exchange Group) - 32 columns**

**Price & Volume (2):**
- `RefPriceClose` - Daily closing price
- `RefVolume` - Daily volume

**Company Info (2):**
- `CompanyMarketCap` - Market capitalization
- `GICSSectorName` - Sector classification (excluded from training)

**Valuation Metrics (9):**
- `EnterpriseValue_DailyTimeSeries_`
- `EnterpriseValueToEBIT_DailyTimeSeriesRatio_`
- `EnterpriseValueToEBITDA_DailyTimeSeriesRatio_`
- `EnterpriseValueToSales_DailyTimeSeriesRatio_`
- `ForwardEnterpriseValueToOperatingCashFlow_DailyTimeSeriesRatio_`
- `ForwardPEG_DailyTimeSeriesRatio_`
- `ForwardPriceToCashFlowPerShare_DailyTimeSeriesRatio_`
- `ForwardPriceToSalesPerShare_DailyTimeSeriesRatio_`
- `PriceEarningsToGrowthRatio_SmartEstimate_`

**Cash Flow (2):**
- `FOCFExDividends_Discrete` - Free cash flow
- `CashCashEquivalents_Total` - Cash and equivalents

**Debt & Interest (2):**
- `Debt_Total` - Total debt
- `InterestExpense_NetofCapitalizedInterest` - Interest expense

**Earnings Metrics (4):**
- `EarningsPerShare_Actual`
- `EarningsPerShare_SmartEstimate_current_Q`
- `EarningsPerShare_SmartEstimate_prev_Q`
- `EarningsPerShare_ActualSurprise`

**Growth & Estimates (4):**
- `LongTermGrowth_Mean` - Long-term growth estimate
- `Estpricegrowth_percent` - Estimated price growth
- `PriceTarget_Median` - Analyst price target
- `Dividend_Per_Share_SmartEstimate` - Dividend estimate

**Profitability Ratios (3):**
- `ReturnOnEquity_SmartEstimat` - ROE
- `ReturnOnAssets_SmartEstimate` - ROA
- `GrossProfitMargin_ActualSurprise` - Gross profit margin surprise

**Rankings & Quality (4):**
- `CombinedAlphaModelRegionRank` - Regional alpha rank
- `CombinedAlphaModelSectorRank` - Sector alpha rank
- `CombinedAlphaModelSectorRankChange` - Rank change
- `EarningsQualityRegionRank_Current` - Earnings quality rank

**Analyst Recommendations (1):**
- `Recommendation_Median_1_5_` - Analyst recommendation (1=Strong Buy, 5=Sell)

**3. Sharadar Metadata - 2 columns**
- `sharadar_scalemarketcap` - Market cap scale (1=Nano, 6=Mega)
- `sharadar_is_adr` - Is American Depositary Receipt

**4. Other Metadata**
- `tradedate`, `fiscalyear_fmp`, `period_fmp`, `reportedcurrency_fmp`

**Total Input**: ~240 raw columns from multiple data sources

> **Note**: Your CSV doesn't need all these columns! The script automatically detects and processes whatever fundamental columns you provide. A minimal dataset needs only: `Date`, `Symbol`, `RefPriceClose`, `RefVolume`, `CompanyMarketCap`, plus any fundamentals you want to use for prediction.

### Step 2: Lag ALL Fundamentals by 1 Day

**Critical for preventing look-ahead bias:**

```python
# For EVERY fundamental column in your CSV, create lagged version
RefPriceClose_lag1 = shift(RefPriceClose, 1 day)
CompanyMarketCap_lag1 = shift(CompanyMarketCap, 1 day)
ReturnOnEquity_SmartEstimat_lag1 = shift(ReturnOnEquity_SmartEstimat, 1 day)
growthrevenue_fmp_lag1 = shift(growthrevenue_fmp, 1 day)
# ... for ALL ~240 columns in production dataset
```

**Why?** On day T, you only know fundamentals from day T-1. Using same-day fundamentals would create look-ahead bias.

**Result**: ~240 `_lag1` columns created (in production dataset)

### Step 3: Engineer Features from Lagged Data

#### A. Price Momentum Features (6 features)

From `RefPriceClose_lag1`:
```python
return_5d   = 5-day return (%)
return_10d  = 10-day return (%)
return_20d  = 20-day return (%)

volatility_5d   = 5-day annualized volatility (%)
volatility_10d  = 10-day annualized volatility (%)
volatility_20d  = 20-day annualized volatility (%)
```

#### B. Volume Features (1 feature)

From `RefVolume_lag1`:
```python
volume_ratio = current_volume / 20-day_moving_average
```

#### C. Fundamental Ratios (20+ features from LSEG data)

From lagged LSEG fundamentals:

**Profitability:**
```python
roa = ReturnOnAssets_SmartEstimate_lag1
roe = ReturnOnEquity_SmartEstimat_lag1
```

**Valuation:**
```python
ev_to_ebitda = EnterpriseValueToEBITDA_DailyTimeSeriesRatio__lag1
ev_to_ebit = EnterpriseValueToEBIT_DailyTimeSeriesRatio__lag1
ev_to_sales = EnterpriseValueToSales_DailyTimeSeriesRatio__lag1
forward_peg = ForwardPEG_DailyTimeSeriesRatio__lag1
pe_to_growth = PriceEarningsToGrowthRatio_SmartEstimate__lag1
```

#### D. All Other Lagged Fundamentals (~210+ features from FMP + other sources)

All additional fundamental columns from your CSV are automatically included as features:
- FMP income statement growth metrics (_lag1 versions)
- FMP balance sheet growth metrics (_lag1 versions)
- FMP cash flow growth metrics (_lag1 versions)
- Any other fundamental columns you provide

**Growth:**
```python
ltg = LongTermGrowth_Mean_lag1
price_growth_est = Estpricegrowth_percent_lag1
```

**Quality/Momentum:**
```python
alpha_sector_rank = CombinedAlphaModelSectorRank_lag1
alpha_region_rank = CombinedAlphaModelRegionRank_lag1
alpha_sector_change = CombinedAlphaModelSectorRankChange_lag1
earnings_quality = EarningsQualityRegionRank_Current_lag1
```

**Financial Health** (normalized by market cap):
```python
debt_to_marketcap = Debt_Total_lag1 / CompanyMarketCap_lag1
cash_to_marketcap = CashCashEquivalents_Total_lag1 / CompanyMarketCap_lag1
fcf_to_marketcap = FOCFExDividends_Discrete_lag1 / CompanyMarketCap_lag1
```

**Earnings:**
```python
eps_actual = EarningsPerShare_Actual_lag1
eps_surprise = EarningsPerShare_ActualSurprise_lag1
eps_estimate_current = EarningsPerShare_SmartEstimate_current_Q_lag1
```

**Analyst Metrics:**
```python
recommendation = Recommendation_Median_1_5__lag1
price_target = PriceTarget_Median_lag1
upside_to_target = (PriceTarget_Median_lag1 / RefPriceClose_lag1 - 1) * 100
```

**Size Factor:**
```python
log_marketcap = log(1 + CompanyMarketCap_lag1)
```

#### E. Cross-Sectional Rankings (7 features)

For each date, rank all stocks by percentile (0-100%):
```python
CompanyMarketCap_lag1_rank = percentile_rank(CompanyMarketCap_lag1)
return_20d_rank = percentile_rank(return_20d)
volatility_20d_rank = percentile_rank(volatility_20d)
roe_rank = percentile_rank(roe)
roa_rank = percentile_rank(roa)
ev_to_ebitda_rank = percentile_rank(ev_to_ebitda)
ltg_rank = percentile_rank(ltg)
```

**Why ranks?** Captures relative positioning - is this stock cheap/expensive compared to peers today?

#### F. Additional Lags (5 features)

T-2 versions of key metrics:
```python
roa_lag2 = shift(roa, 1 day)  # T-2 ROA
roe_lag2 = shift(roe, 1 day)  # T-2 ROE
ev_to_ebitda_lag2 = shift(ev_to_ebitda, 1 day)
ev_to_sales_lag2 = shift(ev_to_sales, 1 day)
ltg_lag2 = shift(ltg, 1 day)
```

**Why?** Captures changes in fundamentals over time.

### Step 4: Feature Selection (What's EXCLUDED)

**Automatically excluded from training (NOT used as features):**

1. **Metadata columns**: Date, Symbol, Instrument, TradeDate, CompanyCommonName, GICSSectorName
2. **Sharadar metadata**: sharadar_exchange, sharadar_category, sharadar_location, etc.
3. **Target variable**: forward_return (what we're predicting)
4. **Intermediate calculations**: volume_ma_20 (only used to create volume_ratio)
5. **ALL original (non-lagged) columns**: RefPriceClose, CompanyMarketCap, etc. (unless using `--no-lag` mode)

**What's INCLUDED (the X matrix for training):**

‚úÖ All `_lag1` columns (~240 lagged fundamentals in production dataset)
‚úÖ All momentum features (return_5d, return_10d, return_20d, volatility_5d, volatility_10d, volatility_20d)
‚úÖ Volume features (volume_ratio)
‚úÖ All fundamental ratios (roa, roe, ev_to_ebitda, ltg, etc.)
‚úÖ All cross-sectional ranks (7 rank features)
‚úÖ All lag-2 features (5 additional lags)

**Total Features (Production Dataset with LSEG + FMP + Sharadar):**
- **Input**: ~240 raw fundamental columns
- **After lagging + engineering**: **290 features** used for training
- **Feature breakdown**:
  - ~240 lagged fundamentals (_lag1 versions)
  - 6 price momentum features
  - 1 volume feature
  - ~20 derived fundamental ratios
  - 7 cross-sectional ranks
  - 5 lag-2 features
  - ~11 other engineered features (log_marketcap, upside_to_target, etc.)

> **Note**: If you use a minimal LSEG-only dataset (32 columns), you'll get ~70-100 features instead of 290. The script automatically adapts to your data!

### Step 5: Missing Value Handling (Forward-Fill - NO LOOK-AHEAD BIAS)

**CRITICAL UPDATE (2025-12-28):** Changed from median-fill to forward-fill to eliminate look-ahead bias!

```python
For numeric features:
  - Forward-fill per symbol (use last known value)
  - Fill remaining NaNs (first rows) with 0

For categorical features:
  - Fill with 'Unknown'
  - Convert to numeric codes
```

**Why forward-fill per symbol?**
- **Point-in-time accurate**: Uses only data available at that date
- **No look-ahead bias**: Never uses future data (median used future values!)
- **Realistic**: Matches what you'd actually know when trading

**Example:**
```
Symbol: AAPL
Date       | EarningsPerShare | After Forward-Fill
-----------|------------------|------------------
2024-01-15 | 2.18            | 2.18 (reported)
2024-01-16 | NaN             | 2.18 (use last known)
2024-01-17 | NaN             | 2.18 (use last known)
2024-04-25 | 1.53            | 1.53 (new report)
2024-04-26 | NaN             | 1.53 (use last known)
```

This ensures fundamentals persist until the next report - exactly like real trading!

### Step 6: Sample Weighting

Not all stocks are weighted equally:

```python
Top 2000 by market cap:    weight = 1.0  (full importance)
Rank 2001-4000:            weight = 0.5  (medium importance)
Rank 4000+:                weight = 0.1  (low but not ignored)
```

**Effect**: Model focuses on large-cap liquid stocks but still learns from entire universe.

### Final Training Matrix

```
X = 70-290+ features (depending on input data - all lagged or engineered, NO raw fundamentals)
y = forward_return (90-day return starting 10 days ahead)
sample_weights = market cap based weights

Training rows: ~8.6M rows with valid forward_return
Prediction rows: 9.0M+ rows (includes recent dates without forward_return)

Example production dataset:
  ‚Ä¢ Input: ~240 raw fundamental columns
  ‚Ä¢ Output: 290 features (240 lagged + 50 engineered)
```

### Feature Importance

The model automatically learns which features are most predictive. Typically:

**Most Important:**
1. Long-term growth estimates (`ltg`)
2. Analyst price targets (`price_target`, `upside_to_target`)
3. Valuation ratios (`ev_to_ebitda`, `forward_peg`)
4. Alpha rankings (`alpha_sector_rank`)
5. Recent momentum (`return_20d`)

**Moderately Important:**
6. Profitability metrics (`roe`, `roa`)
7. Earnings surprises (`eps_surprise`)
8. Cross-sectional ranks
9. Volatility measures

**Less Important but Still Used:**
10. Volume ratios
11. Short-term momentum (5-day)
12. Lag-2 features

### Key Principles

1. ‚úÖ **NO LOOK-AHEAD BIAS**: All data lagged by 1 day
2. ‚úÖ **Automatic selection**: Exclude metadata, keep all engineered features
3. ‚úÖ **Market-cap weighting**: Focus on tradeable large-caps
4. ‚úÖ **Robust to missing data**: Median imputation
5. ‚úÖ **Cross-sectional context**: Ranks capture relative positioning
6. ‚úÖ **Temporal features**: Momentum and changes over time

**The model learns from all features simultaneously to predict future returns!**
- LSEG-only dataset: ~70-100 features
- Production dataset (LSEG + FMP + Sharadar): 290 features
- Your custom dataset: Features scale automatically with your input columns

## üöÄ Quick Start

### Basic Usage

```bash
# Predict 10-day returns (default)
python forecast_returns_ml.py your_data.csv

# Output: your_data_predictions_NO_LOOKAHEAD.csv
```

### Common Use Cases

#### 1. Weekly Rebalancing Strategy (10-day returns)
```bash
python forecast_returns_ml.py data.csv --forecast-days 10 --no-cv
```
- **Use case**: Rebalance weekly, hold 2 weeks
- **Correlation**: ~80%
- **Direction accuracy**: ~73%

#### 2. Monthly Strategy with Lead Time (90-day returns, 10 days ahead)
```bash
python forecast_returns_ml.py data.csv \
  --forecast-days 10 \
  --target-return-days 90 \
  --no-cv
```
- **Use case**: Predict quarterly performance with 2-week lead time
- **Correlation**: ~95% üî•
- **Direction accuracy**: ~87%

#### 3. Quarterly Rebalancing (180-day returns)
```bash
python forecast_returns_ml.py data.csv \
  --forecast-days 20 \
  --target-return-days 180 \
  --n-estimators 500 \
  --no-cv
```
- **Use case**: Long-term holdings, predict 6-month returns
- **Better for**: Fundamental-driven strategies

#### 4. Export for Trading System Integration
```bash
python forecast_returns_ml.py data.csv \
  --no-cv \
  --export-predictions
```
- **Use case**: Daily predictions for automated trading system
- **Output**: Full file + lightweight 3-column predictions file
- **Benefits**: Fast lookups, easy database integration

#### 5. Fast Training with Logging (NEW in v3.1)
```bash
python forecast_returns_ml_walk_forward.py data.csv \
  --sample-fraction 0.5 \
  --log-file my_run.log \
  --forecast-days 10 \
  --target-return-days 90
```
- **Use case**: Quick experiments with full audit trail
- **Speed**: ~2x faster with 50% sampling
- **Output**: Predictions + complete log file for reproducibility

#### 6. Pre-Lagged Data Pipeline (NEW in v3.1)
```bash
# You've already lagged your data by 1 day
python forecast_returns_ml.py pre_lagged_data.csv \
  --no-lag \
  --no-cv
```
- **Use case**: Custom data preparation pipeline
- **Benefits**: You control lagging logic, script uses data as-is
- **IMPORTANT**: Only use if ALL columns are already lagged by 1 day!

### All Options

```bash
python forecast_returns_ml.py <input.csv> [options]

Required:
  input.csv              Path to CSV file with fundamental data

Optional Arguments:
  --output, -o           Output CSV path (default: auto-generated)
  --lookback             NOT IMPLEMENTED - Reserved for future use (default: 10)
                         Note: Rolling windows are currently hardcoded to [5, 10, 20] days

  # Return Period Configuration
  --forecast-days        Days ahead to start measuring (default: 10)
  --target-return-days   Return period to predict (default: same as forecast-days)

  # Model Parameters
  --n-estimators         Number of boosting rounds (default: 300)
  --learning-rate        Learning rate (default: 0.05)
  --max-depth            Maximum tree depth (default: 7)

  # Data Handling (NEW in v3.1)
  --no-lag               Skip automatic lagging (use when input data is already lagged)
                         IMPORTANT: Only use if you've pre-lagged all columns by 1 day!

  # Performance Optimization (NEW in v3.1)
  --sample-fraction      Fraction of training data to use (0.0-1.0, default: 1.0)
                         Example: 0.5 = 50% sampling for ~2x speedup
                         Predictions still made for ALL rows

  # Logging (NEW in v3.1)
  --log-file             Path to log file (default: auto-generated with timestamp)
                         All console output saved to file for reproducibility

  # Execution Options
  --no-cv                Skip cross-validation (faster)
  --keep-features        Keep all engineered features in output (large file)
  --export-predictions   Export separate file with only Symbol, Date, predicted_return
  --skip-feature-importance  Skip feature importance calculation (saves 1-3 minutes)
```

## üìã Input Data Requirements

### Required Columns

Your CSV must include:

1. **Identifiers:**
   - `Date` - Date in YYYY-MM-DD format
   - `Symbol` - Stock ticker symbol
   - `RefPriceClose` - Closing price

2. **Market Cap:**
   - `CompanyMarketCap` - Market capitalization (for weighting)

3. **Fundamentals (at least some of these):**
   - Valuation: `EnterpriseValueToEBITDA_DailyTimeSeriesRatio_`, `ForwardPEG_DailyTimeSeriesRatio_`
   - Profitability: `ReturnOnEquity_SmartEstimat`, `ReturnOnAssets_SmartEstimate`
   - Growth: `LongTermGrowth_Mean`, `Estpricegrowth_percent`
   - Earnings: `EarningsPerShare_Actual`, `EarningsPerShare_SmartEstimate_current_Q`
   - Quality: `CombinedAlphaModelSectorRank`, `EarningsQualityRegionRank_Current`
   - Others: See full list in script

### Example Input Format

```csv
Date,Symbol,RefPriceClose,CompanyMarketCap,ReturnOnEquity_SmartEstimat,...
2024-01-01,AAPL,182.68,2900000000000,147.5,...
2024-01-01,MSFT,376.04,2800000000000,38.2,...
2024-01-02,AAPL,185.64,2920000000000,148.1,...
```

## üì§ Output

### Columns Added (2 new columns)

The script adds exactly **2 columns** to your original data:

1. **`forward_return`** - Actual returns for validation (in **percentages**)
   - Example: For 90-day returns starting day 10, this is the actual T+10 to T+100 return
   - Value of `5.0` means a 5% gain, `-3.5` means a 3.5% loss
   - Use this to validate model accuracy

2. **`predicted_return`** - ML predictions (in **percentages**, use for trading)
   - The model's predicted return
   - Value of `12.3` means predicted 12.3% gain
   - Use this for stock selection and ranking

**Important:** Both columns are in **percentage units**, not decimal. A 10% return is represented as `10.0`, not `0.10`.

### File Size
Output size ‚âà Input size √ó 1.05 (only 5% larger, not 2x!)

**Example:**
- Input: 4 GB, 47 columns
- Output: 4.2 GB, 49 columns

### Predictions-Only Export (Optional)

Use `--export-predictions` to create a **separate lightweight file** with only the essential columns:

```bash
python forecast_returns_ml.py data.csv --export-predictions
```

**Output files:**
1. `data_predictions_NO_LOOKAHEAD.csv` - Full file (49 columns)
2. `data_predictions_NO_LOOKAHEAD_predictions_only.csv` - Lean file (3 columns)

**Predictions-only file contains:**
- `Symbol` - Stock ticker
- `Date` - Date in YYYY-MM-DD format
- `predicted_return` - ML predicted return (in **percentages**: `10.5` = 10.5% gain)

**Benefits:**
- **16x smaller file size** (1.3 MB vs 21 MB for 40K rows)
- **Fast lookups** in trading systems
- **Easy integration** with databases
- **Sorted by Date, Symbol** for efficient queries
- **No NaN rows** (only predictions, no missing values)

### Log Files

All runs automatically create a log file in the `./logs/` directory:

```bash
python forecast_returns_ml_walk_forward.py --input data.csv --output pred.parquet

# Creates: ./logs/forecast_ml_walk_forward_20260107_143022.log
```

**Log file contains:**
- Complete console output (all prints, warnings, errors)
- Run parameters (model settings, feature counts, date ranges)
- Training progress (time per month, prediction counts)
- File paths (input, output, resume files)
- Duplicate row detection results
- Feature descriptions (all 290+ features logged)

**Benefits:**
- ‚úÖ **Full audit trail** for reproducibility
- ‚úÖ **Automatic naming** with timestamp (no conflicts)
- ‚úÖ **Organized storage** in `./logs/` directory (not cluttering data directory)
- ‚úÖ **Share logs** for debugging without sharing data

**Custom log location:**
```bash
python forecast_returns_ml_walk_forward.py --input data.csv --log-file /path/to/custom.log
```

**Example output:**
```csv
Symbol,Date,predicted_return
AAPL,2024-12-16,8.5
GOOGL,2024-12-16,12.3
MSFT,2024-12-16,-2.1
```
This means: AAPL predicted to gain 8.5%, GOOGL to gain 12.3%, MSFT to decline 2.1%.

**Use cases:**
```python
# Quick lookup for today's predictions
predictions = pd.read_csv('predictions_only.csv')
today_preds = predictions[predictions['Date'] == '2024-12-16']

# Find stocks predicted to gain > 10%
strong_picks = predictions[predictions['predicted_return'] > 10]

# Merge with portfolio
portfolio = portfolio.merge(predictions, on=['Symbol', 'Date'])

# Database import
predictions.to_sql('ml_predictions', engine, if_exists='append')
```

## üìà Model Performance

### Typical Results

| Return Period | Correlation | Direction Accuracy | MAE | RMSE |
|--------------|-------------|-------------------|-----|------|
| 10-day | 80.7% | 72.8% | 2.9% | 4.1% |
| 30-day | 85-90% | 75-80% | 3-4% | 5-6% |
| 90-day | 95.1% | 87.5% | 4.7% | 6.3% |

**Key Finding:** Fundamentals predict long-term returns much better than short-term!

### Cross-Validation

The script uses **time-series cross-validation** to ensure:
- No future information leakage
- Realistic out-of-sample performance
- Proper handling of temporal dependencies

## üîß Advanced Usage

### Custom Model Parameters

```bash
# More aggressive model (higher accuracy, slower)
python forecast_returns_ml.py data.csv \
  --n-estimators 500 \
  --max-depth 9 \
  --learning-rate 0.03

# Faster model (lower accuracy, faster training)
python forecast_returns_ml.py data.csv \
  --n-estimators 100 \
  --max-depth 5 \
  --learning-rate 0.1 \
  --no-cv
```

### Keep All Features for Analysis

```bash
# Save all engineered features (useful for debugging/analysis)
python forecast_returns_ml.py data.csv --keep-features

# Warning: Output will be ~2x larger (124 columns vs 49)
```

### Feature Importance Analysis

The walk-forward script automatically displays the **top 15 most important features** after training completes, using permutation importance (industry-standard method).

**Example output:**
```
üî¨ TOP 15 MOST IMPORTANT FEATURES:
   (Based on final trained model using permutation importance)
  ltg                                       0.012345  (¬±0.001234)
  price_target                              0.010234  (¬±0.001123)
  marketcap                                 0.009876  (¬±0.000987)
  ...
```

**Performance impact:**
- Adds **1-3 minutes** to total runtime (after all training is complete)
- No impact on model training or prediction accuracy
- Based on 10,000 sampled rows with 5 permutation repeats

**Skip feature importance** to save time during testing:
```bash
# Skip feature importance calculation (saves 1-3 minutes)
python forecast_returns_ml_walk_forward.py data.csv --skip-feature-importance
```

**What it measures:**
- Higher values = more important for predictions
- Shows impact on model performance when each feature is shuffled
- Based on the final month's trained model (most recent)

### Pipeline Integration

```python
import pandas as pd
from forecast_returns_ml import ReturnForecaster

# Load data
df = pd.read_csv('your_data.csv')

# Initialize forecaster
forecaster = ReturnForecaster(
    forecast_days=10,
    target_return_days=90,
    n_estimators=300
)

# Train and predict
predictions_df = forecaster.fit_predict(df, use_cv=False)

# Use predictions
top_stocks = predictions_df.nlargest(50, 'predicted_return')
```

## üéì Understanding the Output

### Return Value Units

**All returns are in PERCENTAGES, not decimals:**

| Value | Meaning | Decimal Equivalent |
|-------|---------|-------------------|
| `10.0` | 10% gain | 0.10 |
| `5.5` | 5.5% gain | 0.055 |
| `-3.2` | 3.2% loss | -0.032 |
| `0.0` | No change | 0.00 |
| `100.0` | 100% gain (doubled) | 1.00 |

**In trading code, remember to convert:**
```python
# Get predicted return in percentage
predicted_pct = 12.5  # 12.5%

# Convert to decimal for calculations
predicted_decimal = predicted_pct / 100  # 0.125

# Calculate expected price
current_price = 100
expected_price = current_price * (1 + predicted_decimal)  # 112.50
```

### Example Output Summary

```
üéØ PREDICTION SETUP:
  ‚Ä¢ Using features from: T-1 (lagged by 1 day)
  ‚Ä¢ Predicting returns from: T+10 to T+100
  ‚Ä¢ Total return period: 90 days

üìä Sample Weighting:
  ‚Ä¢ Top 2000 stocks (weight=1.0): 2,500,000 samples
  ‚Ä¢ Mid-cap stocks (weight=0.5): 800,000 samples
  ‚Ä¢ Small-cap stocks (weight=0.1): 1,200,000 samples

üéØ MODEL PERFORMANCE (Out-of-sample, NO LOOK-AHEAD BIAS):
  ‚Ä¢ Correlation with actual returns: 0.9510 (95.1%)
  ‚Ä¢ Mean Absolute Error: 4.65%
  ‚Ä¢ Root Mean Squared Error: 6.29%
  ‚Ä¢ Direction accuracy: 87.48%

üìã COLUMN SUMMARY:
  ‚Ä¢ Original columns: 47
  ‚Ä¢ New columns added: 2
  ‚Ä¢ Total output columns: 49
```

### How to Use Predictions

**For Stock Selection:**
```python
# Top 50 predicted performers
top_50 = df.nlargest(50, 'predicted_return')

# Long/short strategy
longs = df.nlargest(100, 'predicted_return')
shorts = df.nsmallest(100, 'predicted_return')
```

**For Portfolio Optimization:**
```python
# Weight by predicted return
df['weight'] = df['predicted_return'].clip(lower=0) / df['predicted_return'].sum()
```

**For Risk Management:**
```python
# Avoid stocks with large predicted declines
safe_stocks = df[df['predicted_return'] > -5]
```

## ‚ö†Ô∏è Important Notes

### Look-Ahead Bias Prevention

The model uses **T-1 data to predict T+forecast_days** returns. This means:

‚úÖ **Safe for real trading:**
- On day T, use fundamentals from T-1
- Predict returns starting T+10
- Trade at market close on day T

‚ùå **Would be look-ahead bias:**
- Using same-day fundamentals (T data to predict T+10)
- Not lagging features by at least 1 day

### Data Quality Matters

- **Survivorship bias**: Include delisted stocks in training
- **Point-in-time data**: Ensure fundamentals reflect what was known at each date
- **Consistent definitions**: Use same fundamental definitions throughout

### Computational Requirements

| Dataset Size | Training Time | RAM Required |
|-------------|---------------|--------------|
| 40K rows | ~5 seconds | 2 GB |
| 1M rows | ~2 minutes | 8 GB |
| 10M rows | ~20 minutes | 32 GB |

## üêõ Troubleshooting

### "Excel shows 690%, 702%, 756% - looks wrong!"
**This is an Excel formatting issue, not a data problem!**

The CSV file stores values correctly as **6.9**, **7.0**, **7.6** (meaning 6.9%, 7.0%, 7.6%).

Excel auto-detects the column as "Percentage" and multiplies by 100, showing:
- 6.9 ‚Üí 690% (wrong display)
- 7.0 ‚Üí 702% (wrong display)
- 7.6 ‚Üí 756% (wrong display)

**Solutions:**
1. In Excel: Select column ‚Üí Format Cells ‚Üí Number (not Percentage) ‚Üí 2 decimal places
2. Use Python/pandas to view: `pd.read_csv('file.csv')`
3. Open in text editor to see raw values

**The data is correct** - just format as Number in Excel!

### "Recent dates have NaN forward_return but have predictions"
**This is correct and expected!**

- Recent dates don't have future prices yet (can't calculate `forward_return`)
- But they DO have fundamentals, so the model CAN and SHOULD predict
- These predictions are what you use for actual trading
- Example: Data through 2024-12-09, last 20 dates have predictions but no validation

**This is the whole point** - getting forecasts for dates you can trade on!

### "No such column: forward_return"
- Old output file format. Re-run the script to generate new predictions.

### "Correlation is very low (<0.3)"
- Check for look-ahead bias in input data
- Ensure sufficient data quality
- Try longer return periods (90-day vs 10-day)

### "All samples have weight 1.0"
- Your dataset has <2000 stocks per date
- This is fine! Weighting only matters with many stocks

### Output file is huge
- Don't use `--keep-features` flag
- Default output adds only 2 columns

### "--lookback parameter doesn't change anything"
**This is expected - the parameter is not implemented.**

The `--lookback` parameter is a placeholder for future functionality. Currently:
- Rolling momentum windows are hardcoded to: 5, 10, 20 days
- Rolling volatility windows are hardcoded to: 5, 10, 20 days
- Volume moving average is hardcoded to: 20 days

Changing `--lookback` has no effect. This may be implemented in a future version.

## üìö References

### Methodology
- Gradient Boosting: scikit-learn HistGradientBoostingRegressor
- Feature engineering: Based on Fama-French factors and momentum
- Cross-validation: Time-series split with walk-forward validation

### Data Sources
This tool is designed for:
- LSEG (Refinitiv) fundamental data
- NASDAQ Data Link (Sharadar) fundamentals
- Custom fundamental databases

### Related Documentation
- See `/docs/MULTI_SOURCE_DATA.md` for data integration
- See `/examples/lseg_fundamentals/` for data preparation
- See `/examples/strategies/` for strategy examples using predictions

## üìû Support

For issues or questions:
1. Check this README first
2. Review the inline code documentation
3. Check example usage in the script header
4. File an issue with sample data and error message

## üìÑ License

Part of the Zipline-Reloaded project by Hidden Point Capital.

---

**Version:** 3.1.0
**Last Updated:** 2025-12-28
**Author:** Hidden Point Capital (with Claude Code assistance)

---

## Changelog

### v3.1.0 (2025-12-31) - Production Safety Release

**PRODUCTION-CRITICAL FIXES:**
- **CRITICAL**: Eliminated look-ahead bias in walk-forward mode by disabling statistical outlier clipping
- **CRITICAL**: Z-score and quantile clipping now skipped in walk-forward (was using future month statistics)
- **CRITICAL**: Added warnings when using PCA with walk-forward mode (has look-ahead bias)
- **CRITICAL**: Changed missing value handling from median-fill to forward-fill per symbol (eliminates look-ahead bias)

**Production Deployment:**
- **NEW**: Production Deployment Guide section with safety checklist
- **NEW**: Look-ahead bias protection table documenting all safe operations
- **NEW**: Clear warnings in code and documentation about PCA in production
- **WARNING**: PCA not recommended for production walk-forward (use all 290 features)

**Bug Fixes:**
- **BUGFIX**: Raw LSEG fundamentals now included when using `--no-lag` mode
- **BUGFIX**: Fixed z-score NaN handling for zero-variance columns
- **BUGFIX**: Fixed Series boolean comparison in extreme outlier detection
- **BUGFIX**: Added StandardScaler before PCA (was producing NaN variance)

**New Features:**
- **NEW**: Added `--pca N` for PCA dimensionality reduction (exploration only, not for production)
- **NEW**: Added feature descriptions logging - All training features listed with human-readable descriptions at start
- **NEW**: Added `--log-file` for automatic logging with timestamps
- **NEW**: Added `--sample-fraction` for training speedup (2-5x faster)
- **NEW**: Added `--no-lag` for pre-lagged data support
- **NEW**: Automatic column name normalization (lowercase ‚Üí PascalCase)

**Documentation:**
- **UPDATED**: Comprehensive production deployment guide
- **UPDATED**: PCA section with production warnings
- **UPDATED**: Walk-forward mode guarantees zero look-ahead bias

### v3.0.2 (2024-12-17)
- Initial walk-forward implementation
- Market cap weighted training
- Feature importance analysis

---

## üìö Detailed Documentation

For detailed technical documentation, design rationales, and troubleshooting guides, see the **[Docs/](Docs/)** directory:

- **[Docs/INDEX.md](Docs/INDEX.md)** - Complete documentation index
- **[Docs/DETERMINISTIC_DESIGN.md](Docs/DETERMINISTIC_DESIGN.md)** - Why zero randomness is better
- **[Docs/REPRODUCIBILITY_FIX.md](Docs/REPRODUCIBILITY_FIX.md)** - Root cause analysis of non-determinism
- **[Docs/MERGE_OPTIMIZATION_ANALYSIS.md](Docs/MERGE_OPTIMIZATION_ANALYSIS.md)** - Memory optimization details
- **[Docs/SUMMARY_v3.2.2.md](Docs/SUMMARY_v3.2.2.md)** - Latest version summary

**See [Docs/INDEX.md](Docs/INDEX.md) for complete documentation index and recommended reading order.**
