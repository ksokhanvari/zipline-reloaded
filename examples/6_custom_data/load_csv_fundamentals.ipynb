{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CSV Fundamental Data into Zipline Custom Database\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load fundamental data from CSV files\n",
    "2. Map symbols to Zipline SIDs\n",
    "3. Create a custom SQLite database\n",
    "4. Use the data in Zipline Pipeline\n",
    "\n",
    "This is a zipline-reloaded native approach (no QuantRocket dependencies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Registered Sharadar bundle\n",
      "\u2713 Imports complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Zipline imports\n",
    "from zipline.data.bundles import load as load_bundle, register\n",
    "from zipline.data.bundles.sharadar_bundle import sharadar_bundle\n",
    "from zipline.pipeline import Pipeline\n",
    "from zipline.pipeline.data.db import Database, Column\n",
    "\n",
    "# Register Sharadar bundle (in case extension.py didn't load)\n",
    "try:\n",
    "    # Try to register the bundle\n",
    "    register(\n",
    "        'sharadar',\n",
    "        sharadar_bundle(\n",
    "            tickers=None,\n",
    "            incremental=True,\n",
    "            include_funds=True,\n",
    "        ),\n",
    "    )\n",
    "    print(\"\u2713 Registered Sharadar bundle\")\n",
    "except Exception as e:\n",
    "    # Bundle may already be registered\n",
    "    print(f\"\u2713 Sharadar bundle already registered (or error: {e})\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', 120)\n",
    "\n",
    "print(\"\u2713 Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set your database name and data directory paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database will be created at: /root/.zipline/data/custom/fundamentals.sqlite\n",
      "Update mode: fresh\n",
      "  - 'fresh': Drop and recreate database\n",
      "  - 'replace': Update existing records with new data\n",
      "  - 'ignore': Skip records that already exist\n",
      "\n",
      "Looking for CSV files in: /data/csv/\n",
      "\n",
      "\ud83d\udca1 Tip: Place your CSV files in /data/csv/ (inside container)\n",
      "   or ./data/csv/ (on host machine) for persistent storage\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATABASE_NAME = \"fundamentals\"  # Name for your custom database\n",
    "DATA_DIR = \"/data/csv/\"  # Directory with CSV files (persistent across Docker restarts)\n",
    "VIX_SIGNAL_PATH = \"/data/csv/vix_flag.csv\"  # Optional VIX signal data\n",
    "\n",
    "# Database will be created in ~/.zipline/data/custom/\n",
    "DB_DIR = Path('/root/.zipline/data/custom')\n",
    "DB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DB_PATH = DB_DIR / f\"{DATABASE_NAME}.sqlite\"\n",
    "\n",
    "# Database update mode:\n",
    "# 'fresh' - Drop and recreate database (default)\n",
    "# 'replace' - Insert or replace existing records (updates duplicates)\n",
    "# 'ignore' - Insert or ignore (skips duplicates, keeps existing data)\n",
    "UPDATE_MODE = 'fresh'  # Change to 'fresh', 'replace', or 'ignore'\n",
    "\n",
    "print(f\"Database will be created at: {DB_PATH}\")\n",
    "print(f\"Update mode: {UPDATE_MODE}\")\n",
    "print(f\"  - 'fresh': Drop and recreate database\")\n",
    "print(f\"  - 'replace': Update existing records with new data\")\n",
    "print(f\"  - 'ignore': Skip records that already exist\")\n",
    "print(f\"\\nLooking for CSV files in: {DATA_DIR}\")\n",
    "print(f\"\\n\ud83d\udca1 Tip: Place your CSV files in /data/csv/ (inside container)\")\n",
    "print(f\"   or ./data/csv/ (on host machine) for persistent storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Database Schema\n",
    "\n",
    "Define the columns that will be in your custom database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Schema defined with 50 columns\n"
     ]
    }
   ],
   "source": [
    "# Define your database schema\n",
    "# This matches the columns from the QuantRocket example\n",
    "SCHEMA = {\n",
    "    'Symbol': 'TEXT',\n",
    "    'Sid': 'INTEGER',\n",
    "    'Date': 'TEXT',\n",
    "    'RefPriceClose': 'REAL',\n",
    "    'RefVolume': 'REAL',\n",
    "    'CompanyCommonName': 'TEXT',\n",
    "    'EnterpriseValue_DailyTimeSeries_': 'REAL',\n",
    "    'CompanyMarketCap': 'REAL',\n",
    "    'GICSSectorName': 'TEXT',\n",
    "    'FOCFExDividends_Discrete': 'REAL',\n",
    "    'InterestExpense_NetofCapitalizedInterest': 'REAL',\n",
    "    'Debt_Total': 'REAL',\n",
    "    'EarningsPerShare_Actual': 'REAL',\n",
    "    'EarningsPerShare_SmartEstimate_prev_Q': 'REAL',\n",
    "    'EarningsPerShare_ActualSurprise': 'REAL',\n",
    "    'EarningsPerShare_SmartEstimate_current_Q': 'REAL',\n",
    "    'LongTermGrowth_Mean': 'REAL',\n",
    "    'PriceTarget_Median': 'REAL',\n",
    "    'CombinedAlphaModelSectorRank': 'REAL',\n",
    "    'CombinedAlphaModelSectorRankChange': 'REAL',\n",
    "    'CombinedAlphaModelRegionRank': 'REAL',\n",
    "    'TradeDate': 'TEXT',\n",
    "    'EPS_SurpirsePrct_prev_Q': 'REAL',\n",
    "    'Estpricegrowth_percent': 'REAL',\n",
    "    'CashFlowComponent_Current': 'REAL',\n",
    "    'EarningsQualityRegionRank_Current': 'REAL',\n",
    "    'EnterpriseValueToEBIT_DailyTimeSeriesRatio_': 'REAL',\n",
    "    'EnterpriseValueToEBITDA_DailyTimeSeriesRatio_': 'REAL',\n",
    "    'EnterpriseValueToSales_DailyTimeSeriesRatio_': 'REAL',\n",
    "    'Dividend_Per_Share_SmartEstimate': 'REAL',\n",
    "    'CashFlowPerShare_BrokerEstimate': 'REAL',\n",
    "    'FreeCashFlowPerShare_BrokerEstimate': 'REAL',\n",
    "    'ForwardPEG_DailyTimeSeriesRatio_': 'REAL',\n",
    "    'PriceEarningsToGrowthRatio_SmartEstimate_': 'REAL',\n",
    "    'ReturnOnInvestedCapital_BrokerEstimate': 'REAL',\n",
    "    'Recommendation_NumberOfTotal': 'REAL',\n",
    "    'Recommendation_Median_1_5_': 'REAL',\n",
    "    'Recommendation_NumberOfStrongBuy': 'REAL',\n",
    "    'Recommendation_NumberOfBuy': 'REAL',\n",
    "    'Recommendation_Mean_1_5_': 'REAL',\n",
    "    'ReturnOnCapitalEmployed_Actual': 'REAL',\n",
    "    'GrossProfitMargin_': 'REAL',\n",
    "    'ReturnOnEquity_SmartEstimat': 'REAL',\n",
    "    'ReturnOnAssets_SmartEstimate': 'REAL',\n",
    "    'CashCashEquivalents_Total': 'REAL',\n",
    "    'ForwardPriceToCashFlowPerShare_DailyTimeSeriesRatio_': 'REAL',\n",
    "    'ForwardPriceToSalesPerShare_DailyTimeSeriesRatio_': 'REAL',\n",
    "    'ForwardEnterpriseValueToOperatingCashFlow_DailyTimeSeriesRatio_': 'REAL',\n",
    "    'GrossProfitMargin_ActualSurprise': 'REAL',\n",
    "    'pred': 'REAL',  # VIX signal\n",
    "}\n",
    "\n",
    "print(f\"\u2713 Schema defined with {len(SCHEMA)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load CSV Files\n",
    "\n",
    "Load all CSV files from the data directory and concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CSV files:\n",
      "  - LSEG_20091231_20251111.csv\n",
      "\n",
      "Loading CSV files...\n",
      "  Loading LSEG_20091231_20251111.csv...\n",
      "\n",
      "\u2713 Loaded 8,995,098 total rows\n",
      "Date range: 2008-10-16 to 2025-11-11\n",
      "Unique symbols: 4426\n",
      "\n",
      "Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Instrument</th>\n",
       "      <th>RefPriceClose</th>\n",
       "      <th>RefVolume</th>\n",
       "      <th>CompanyCommonName</th>\n",
       "      <th>EnterpriseValue_DailyTimeSeries_</th>\n",
       "      <th>CompanyMarketCap</th>\n",
       "      <th>GICSSectorName</th>\n",
       "      <th>FOCFExDividends_Discrete</th>\n",
       "      <th>InterestExpense_NetofCapitalizedInterest</th>\n",
       "      <th>Debt_Total</th>\n",
       "      <th>EarningsPerShare_Actual</th>\n",
       "      <th>EarningsPerShare_SmartEstimate_prev_Q</th>\n",
       "      <th>EarningsPerShare_ActualSurprise</th>\n",
       "      <th>EarningsPerShare_SmartEstimate_current_Q</th>\n",
       "      <th>LongTermGrowth_Mean</th>\n",
       "      <th>PriceTarget_Median</th>\n",
       "      <th>CombinedAlphaModelSectorRank</th>\n",
       "      <th>CombinedAlphaModelSectorRankChange</th>\n",
       "      <th>CombinedAlphaModelRegionRank</th>\n",
       "      <th>EarningsQualityRegionRank_Current</th>\n",
       "      <th>EnterpriseValueToEBIT_DailyTimeSeriesRatio_</th>\n",
       "      <th>EnterpriseValueToEBITDA_DailyTimeSeriesRatio_</th>\n",
       "      <th>EnterpriseValueToSales_DailyTimeSeriesRatio_</th>\n",
       "      <th>Dividend_Per_Share_SmartEstimate</th>\n",
       "      <th>CashCashEquivalents_Total</th>\n",
       "      <th>ForwardPEG_DailyTimeSeriesRatio_</th>\n",
       "      <th>PriceEarningsToGrowthRatio_SmartEstimate_</th>\n",
       "      <th>Recommendation_Median_1_5_</th>\n",
       "      <th>ReturnOnEquity_SmartEstimat</th>\n",
       "      <th>ReturnOnAssets_SmartEstimate</th>\n",
       "      <th>ForwardPriceToCashFlowPerShare_DailyTimeSeriesRatio_</th>\n",
       "      <th>ForwardPriceToSalesPerShare_DailyTimeSeriesRatio_</th>\n",
       "      <th>ForwardEnterpriseValueToOperatingCashFlow_DailyTimeSeriesRatio_</th>\n",
       "      <th>GrossProfitMargin_ActualSurprise</th>\n",
       "      <th>Estpricegrowth_percent</th>\n",
       "      <th>TradeDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-10-16</td>\n",
       "      <td>APGT</td>\n",
       "      <td>APGT.PK</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Appgate Inc</td>\n",
       "      <td>13205550.0</td>\n",
       "      <td>3.439388e+05</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>-14000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008-10-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-04-22</td>\n",
       "      <td>FIZN</td>\n",
       "      <td>FIZN.PK</td>\n",
       "      <td>20.389298</td>\n",
       "      <td>NaN</td>\n",
       "      <td>First Citizens Bancshares Inc (Tennessee)</td>\n",
       "      <td>126856629.0</td>\n",
       "      <td>7.793763e+07</td>\n",
       "      <td>Financials</td>\n",
       "      <td>-753000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110211000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.229962</td>\n",
       "      <td>7.304464</td>\n",
       "      <td>2.417837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-04-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-08-26</td>\n",
       "      <td>EMOR</td>\n",
       "      <td>EMOR.PK</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Healixa Inc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.061619e+04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-08-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-08-26</td>\n",
       "      <td>UCIX</td>\n",
       "      <td>UCIX.PK</td>\n",
       "      <td>121.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Umbra Companies Inc</td>\n",
       "      <td>4970010.0</td>\n",
       "      <td>4.949000e+06</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>8960.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5299.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3427.593103</td>\n",
       "      <td>3427.593103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-08-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-11-23</td>\n",
       "      <td>HBIA</td>\n",
       "      <td>HBIA.PK</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hills Bancorp</td>\n",
       "      <td>188291469.0</td>\n",
       "      <td>2.160205e+08</td>\n",
       "      <td>Financials</td>\n",
       "      <td>13470000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>504874000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.767320</td>\n",
       "      <td>5.379756</td>\n",
       "      <td>1.953353</td>\n",
       "      <td>NaN</td>\n",
       "      <td>114066000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-11-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date Symbol Instrument  RefPriceClose  RefVolume                          CompanyCommonName  \\\n",
       "0  2008-10-16   APGT    APGT.PK       0.025000        NaN                                Appgate Inc   \n",
       "1  2009-04-22   FIZN    FIZN.PK      20.389298        NaN  First Citizens Bancshares Inc (Tennessee)   \n",
       "2  2009-08-26   EMOR    EMOR.PK       0.040000        NaN                                Healixa Inc   \n",
       "3  2009-08-26   UCIX    UCIX.PK     121.200000        NaN                        Umbra Companies Inc   \n",
       "4  2009-11-23   HBIA    HBIA.PK      24.500000        NaN                              Hills Bancorp   \n",
       "\n",
       "   EnterpriseValue_DailyTimeSeries_  CompanyMarketCap          GICSSectorName  FOCFExDividends_Discrete  \\\n",
       "0                        13205550.0      3.439388e+05  Information Technology                  -14000.0   \n",
       "1                       126856629.0      7.793763e+07              Financials                 -753000.0   \n",
       "2                               NaN      9.061619e+04                     NaN                       NaN   \n",
       "3                         4970010.0      4.949000e+06             Industrials                    8960.0   \n",
       "4                       188291469.0      2.160205e+08              Financials                13470000.0   \n",
       "\n",
       "   InterestExpense_NetofCapitalizedInterest   Debt_Total  EarningsPerShare_Actual  \\\n",
       "0                                       NaN          NaN                      NaN   \n",
       "1                                       NaN  110211000.0                      NaN   \n",
       "2                                       NaN          NaN                      NaN   \n",
       "3                                       NaN       5299.0                      NaN   \n",
       "4                                       NaN  504874000.0                      NaN   \n",
       "\n",
       "   EarningsPerShare_SmartEstimate_prev_Q  EarningsPerShare_ActualSurprise  EarningsPerShare_SmartEstimate_current_Q  \\\n",
       "0                                    NaN                              NaN                                       NaN   \n",
       "1                                    NaN                              NaN                                       NaN   \n",
       "2                                    NaN                              NaN                                       NaN   \n",
       "3                                    NaN                              NaN                                       NaN   \n",
       "4                                    NaN                              NaN                                       NaN   \n",
       "\n",
       "   LongTermGrowth_Mean  PriceTarget_Median  CombinedAlphaModelSectorRank  CombinedAlphaModelSectorRankChange  \\\n",
       "0                  NaN                 NaN                           NaN                                 NaN   \n",
       "1                  NaN                 NaN                           NaN                                 NaN   \n",
       "2                  NaN                 NaN                           NaN                                 NaN   \n",
       "3                  NaN                 NaN                           NaN                                 NaN   \n",
       "4                  NaN                 NaN                           NaN                                 NaN   \n",
       "\n",
       "   CombinedAlphaModelRegionRank  EarningsQualityRegionRank_Current  EnterpriseValueToEBIT_DailyTimeSeriesRatio_  \\\n",
       "0                           NaN                                NaN                                          NaN   \n",
       "1                           NaN                                NaN                                     8.229962   \n",
       "2                           NaN                                NaN                                          NaN   \n",
       "3                           NaN                                NaN                                  3427.593103   \n",
       "4                           NaN                                NaN                                     5.767320   \n",
       "\n",
       "   EnterpriseValueToEBITDA_DailyTimeSeriesRatio_  EnterpriseValueToSales_DailyTimeSeriesRatio_  \\\n",
       "0                                            NaN                                           NaN   \n",
       "1                                       7.304464                                      2.417837   \n",
       "2                                            NaN                                           NaN   \n",
       "3                                    3427.593103                                           NaN   \n",
       "4                                       5.379756                                      1.953353   \n",
       "\n",
       "   Dividend_Per_Share_SmartEstimate  CashCashEquivalents_Total  ForwardPEG_DailyTimeSeriesRatio_  \\\n",
       "0                               NaN                     2000.0                               NaN   \n",
       "1                               NaN                        NaN                               NaN   \n",
       "2                               NaN                        NaN                               NaN   \n",
       "3                               NaN                        NaN                               NaN   \n",
       "4                               NaN                114066000.0                               NaN   \n",
       "\n",
       "   PriceEarningsToGrowthRatio_SmartEstimate_  Recommendation_Median_1_5_  ReturnOnEquity_SmartEstimat  \\\n",
       "0                                        NaN                         NaN                          NaN   \n",
       "1                                        NaN                         NaN                          NaN   \n",
       "2                                        NaN                         NaN                          NaN   \n",
       "3                                        NaN                         NaN                          NaN   \n",
       "4                                        NaN                         NaN                          NaN   \n",
       "\n",
       "   ReturnOnAssets_SmartEstimate  ForwardPriceToCashFlowPerShare_DailyTimeSeriesRatio_  \\\n",
       "0                           NaN                                                NaN      \n",
       "1                           NaN                                                NaN      \n",
       "2                           NaN                                                NaN      \n",
       "3                           NaN                                                NaN      \n",
       "4                           NaN                                                NaN      \n",
       "\n",
       "   ForwardPriceToSalesPerShare_DailyTimeSeriesRatio_  ForwardEnterpriseValueToOperatingCashFlow_DailyTimeSeriesRatio_  \\\n",
       "0                                                NaN                                                NaN                 \n",
       "1                                                NaN                                                NaN                 \n",
       "2                                                NaN                                                NaN                 \n",
       "3                                                NaN                                                NaN                 \n",
       "4                                                NaN                                                NaN                 \n",
       "\n",
       "   GrossProfitMargin_ActualSurprise  Estpricegrowth_percent   TradeDate  \n",
       "0                               NaN                     NaN  2008-10-16  \n",
       "1                               NaN                     NaN  2009-04-22  \n",
       "2                               NaN                     NaN  2009-08-26  \n",
       "3                               NaN                     NaN  2009-08-26  \n",
       "4                               NaN                     NaN  2009-11-23  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all CSV files\n",
    "os.chdir(DATA_DIR)\n",
    "csv_files = sorted(glob.glob('LSEG_*.csv'))\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV files:\")\n",
    "for f in csv_files[:5]:  # Show first 5\n",
    "    print(f\"  - {f}\")\n",
    "if len(csv_files) > 5:\n",
    "    print(f\"  ... and {len(csv_files) - 5} more\")\n",
    "\n",
    "# Load and concatenate all CSV files\n",
    "print(\"\\nLoading CSV files...\")\n",
    "custom_data = pd.DataFrame()\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    print(f\"  Loading {csv_file}...\")\n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, csv_file))\n",
    "    custom_data = pd.concat([custom_data, df], ignore_index=True)\n",
    "\n",
    "print(f\"\\n\u2713 Loaded {len(custom_data):,} total rows\")\n",
    "print(f\"Date range: {custom_data['Date'].min()} to {custom_data['Date'].max()}\")\n",
    "print(f\"Unique symbols: {custom_data['Symbol'].nunique()}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample data:\")\n",
    "custom_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optional: Load Recent Data Only\n",
    "\n",
    "To reduce memory usage, you can filter to recent data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Keep only recent data (e.g., last 600,000 rows)\n",
    "# Comment out if you want all historical data\n",
    "# RECENT_ROWS = 600000\n",
    "\n",
    "# if len(custom_data) > RECENT_ROWS:\n",
    "#     print(f\"Filtering to most recent {RECENT_ROWS:,} rows...\")\n",
    "#     custom_data = custom_data.tail(RECENT_ROWS).copy()\n",
    "#     print(f\"\u2713 Filtered. New date range: {custom_data['Date'].min()} to {custom_data['Date'].max()}\")\n",
    "# else:\n",
    "#     print(f\"Dataset has {len(custom_data):,} rows - no filtering needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Map Symbols to Zipline SIDs (WITH TEMPORAL MAPPING)\n",
    "\n",
    "Map your symbols to Zipline Security IDs (SIDs) using **temporal lookups** to handle symbol changes.\n",
    "\n",
    "**IMPORTANT UPDATE**: This notebook now uses **temporal SID mapping** which correctly handles:\n",
    "- Company name changes (FB \u2192 META, etc.)\n",
    "- Ticker symbol changes over time\n",
    "- Mergers and acquisitions\n",
    "\n",
    "The temporal mapper uses `asset_finder.lookup_symbol(symbol, as_of_date)` to get the correct SID for each row's date, ensuring continuous data for companies that changed symbols.\n",
    "\n",
    "**How it works:**\n",
    "- For a row with Symbol='FB', Date='2020-01-01' \u2192 Returns META's SID (the company)\n",
    "- For a row with Symbol='META', Date='2023-01-01' \u2192 Returns same META SID\n",
    "- Result: Continuous data under one SID, no breaks at symbol changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEMPORAL SID MAPPING\n",
      "================================================================================\n",
      "Loading Sharadar bundle...\n",
      "\u2713 Asset finder loaded with 30,701 securities\n",
      "\n",
      "Mapping 8,995,098 rows to SIDs...\n",
      "Strategy: Using temporal lookups (handles FB\u2192META, etc.)\n",
      "Dataset > 1M rows: Using parallel processing\n",
      "Mapping 8,995,098 rows using 16 parallel workers...\n",
      "  Split into 17 chunks of ~562,193 rows each\n",
      "  Completed chunk 1/17\n",
      "  Completed chunk 2/17\n",
      "  Completed chunk 3/17\n",
      "  Completed chunk 4/17\n",
      "  Completed chunk 5/17\n",
      "  Completed chunk 6/17\n",
      "  Completed chunk 7/17\n",
      "  Completed chunk 8/17\n",
      "  Completed chunk 9/17\n",
      "  Completed chunk 10/17\n",
      "  Completed chunk 11/17\n",
      "  Completed chunk 12/17\n",
      "  Completed chunk 13/17\n",
      "  Completed chunk 14/17\n",
      "  Completed chunk 15/17\n",
      "  Completed chunk 16/17\n",
      "  Completed chunk 17/17\n",
      "\n",
      "================================================================================\n",
      "MAPPING RESULTS\n",
      "================================================================================\n",
      "Mapped:   8,549,158 rows (95.0%)\n",
      "Unmapped: 445,940 rows (5.0%)\n",
      "\n",
      "Unmapped symbols (first 10): ['APGT', 'FIZN', 'EMOR', 'UCIX', 'HBIA', 'KAHL', 'ABIT', 'DBIN', 'CLPE', 'CNND']\n",
      "Note: These symbols may not exist in the Sharadar bundle\n",
      "\n",
      "Removing unmapped rows...\n",
      "\u2713 Final dataset: 8,549,158 rows with valid SIDs\n",
      "================================================================================\n",
      "\n",
      "Verifying FB\u2192META continuity:\n",
      "  FB (2020-01-01):   SID 194817\n",
      "  META (2023-01-01): SID 194817\n",
      "  \u2713 Same SID - continuous data maintained!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TEMPORAL SID MAPPING - Handles symbol changes automatically\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/app/examples/custom_data')\n",
    "\n",
    "# Load the Sharadar bundle to get the asset finder\n",
    "print(\"=\"*80)\n",
    "print(\"TEMPORAL SID MAPPING\")\n",
    "print(\"=\"*80)\n",
    "print(\"Loading Sharadar bundle...\")\n",
    "\n",
    "bundle_timestamp = pd.Timestamp.now(tz='UTC')\n",
    "bundle_data = load_bundle('sharadar', timestamp=bundle_timestamp)\n",
    "asset_finder = bundle_data.asset_finder\n",
    "\n",
    "print(f\"\u2713 Asset finder loaded with {len(asset_finder.sids):,} securities\")\n",
    "\n",
    "# Import temporal mapper\n",
    "from temporal_sid_mapper import TemporalSIDMapper\n",
    "\n",
    "print(f\"\\nMapping {len(custom_data):,} rows to SIDs...\")\n",
    "print(\"Strategy: Using temporal lookups (handles FB\u2192META, etc.)\")\n",
    "\n",
    "# Create temporal mapper\n",
    "mapper = TemporalSIDMapper(asset_finder)\n",
    "\n",
    "# Map SIDs using automatic strategy selection\n",
    "custom_data['Sid'] = mapper.map_dataframe_auto(\n",
    "    custom_data,\n",
    "    symbol_col='Symbol',\n",
    "    date_col='Date',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Report results\n",
    "mapped = custom_data['Sid'].notna().sum()\n",
    "unmapped = custom_data['Sid'].isna().sum()\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"MAPPING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Mapped:   {mapped:,} rows ({mapped/len(custom_data)*100:.1f}%)\")\n",
    "print(f\"Unmapped: {unmapped:,} rows ({unmapped/len(custom_data)*100:.1f}%)\")\n",
    "\n",
    "if unmapped > 0:\n",
    "    unmapped_symbols = custom_data[custom_data['Sid'].isna()]['Symbol'].unique()\n",
    "    print(f\"\\nUnmapped symbols (first 10): {list(unmapped_symbols[:10])}\")\n",
    "    print(\"Note: These symbols may not exist in the Sharadar bundle\")\n",
    "\n",
    "# Remove unmapped rows\n",
    "print(f\"\\nRemoving unmapped rows...\")\n",
    "custom_data = custom_data[custom_data['Sid'].notna()].copy()\n",
    "custom_data['Sid'] = custom_data['Sid'].astype(int)\n",
    "\n",
    "print(f\"\u2713 Final dataset: {len(custom_data):,} rows with valid SIDs\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Verify continuity for a known symbol change\n",
    "print(\"\\nVerifying FB\u2192META continuity:\")\n",
    "try:\n",
    "    fb_2020_sid = mapper.map_single_row('FB', '2020-01-01')\n",
    "    meta_2023_sid = mapper.map_single_row('META', '2023-01-01')\n",
    "    \n",
    "    print(f\"  FB (2020-01-01):   SID {fb_2020_sid}\")\n",
    "    print(f\"  META (2023-01-01): SID {meta_2023_sid}\")\n",
    "    \n",
    "    if fb_2020_sid == meta_2023_sid:\n",
    "        print(f\"  \u2713 Same SID - continuous data maintained!\")\n",
    "    else:\n",
    "        print(f\"  \u26a0 Different SIDs - this shouldn't happen\")\n",
    "except Exception as e:\n",
    "    print(f\"  Note: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Merge VIX Signal Data (Optional)\n",
    "\n",
    "If you have additional data like VIX signals, merge it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VIX signal from /data/csv/vix_flag.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_106/2775032704.py:8: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  vix_signal['Date'] = pd.to_datetime(vix_signal['Date'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Merged VIX signal data\n"
     ]
    }
   ],
   "source": [
    "# Load VIX signal data if available\n",
    "if os.path.exists(VIX_SIGNAL_PATH):\n",
    "    print(f\"Loading VIX signal from {VIX_SIGNAL_PATH}...\")\n",
    "    vix_signal = pd.read_csv(VIX_SIGNAL_PATH)\n",
    "    \n",
    "    # Standardize column names\n",
    "    vix_signal.rename(columns={'symbol': 'Symbol', 'date': 'Date'}, inplace=True)\n",
    "    vix_signal['Date'] = pd.to_datetime(vix_signal['Date'])\n",
    "    \n",
    "    # Merge with custom data\n",
    "    custom_data['Date'] = pd.to_datetime(custom_data['Date'])\n",
    "    custom_data = pd.merge(custom_data, vix_signal[['Symbol', 'Date', 'pred']], \n",
    "                          on=['Symbol', 'Date'], how='left')\n",
    "    \n",
    "    print(f\"\u2713 Merged VIX signal data\")\n",
    "else:\n",
    "    print(f\"VIX signal file not found at {VIX_SIGNAL_PATH}\")\n",
    "    print(\"Skipping VIX merge (this is optional)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Cleaning\n",
    "\n",
    "Clean and prepare data for database insertion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data...\n",
      "  Forward filling missing values by symbol...\n",
      "  Filling remaining NaN values...\n",
      "\u2713 Data cleaned\n",
      "\n",
      "Final dataset:\n",
      "  Rows: 8,549,158\n",
      "  Columns: 40\n",
      "  Date range: 2009-12-29 00:00:00 to 2025-11-11 00:00:00\n",
      "  Symbols: 3953\n",
      "  \u2713 GICSSectorName: No numeric zeros, 676555 empty strings\n",
      "  \u2713 CompanyCommonName: No numeric zeros, 14125 empty strings\n",
      "\n",
      "Sample cleaned data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Instrument</th>\n",
       "      <th>RefPriceClose</th>\n",
       "      <th>RefVolume</th>\n",
       "      <th>CompanyCommonName</th>\n",
       "      <th>EnterpriseValue_DailyTimeSeries_</th>\n",
       "      <th>CompanyMarketCap</th>\n",
       "      <th>GICSSectorName</th>\n",
       "      <th>FOCFExDividends_Discrete</th>\n",
       "      <th>InterestExpense_NetofCapitalizedInterest</th>\n",
       "      <th>Debt_Total</th>\n",
       "      <th>EarningsPerShare_Actual</th>\n",
       "      <th>EarningsPerShare_SmartEstimate_prev_Q</th>\n",
       "      <th>EarningsPerShare_ActualSurprise</th>\n",
       "      <th>EarningsPerShare_SmartEstimate_current_Q</th>\n",
       "      <th>LongTermGrowth_Mean</th>\n",
       "      <th>PriceTarget_Median</th>\n",
       "      <th>CombinedAlphaModelSectorRank</th>\n",
       "      <th>CombinedAlphaModelSectorRankChange</th>\n",
       "      <th>CombinedAlphaModelRegionRank</th>\n",
       "      <th>EarningsQualityRegionRank_Current</th>\n",
       "      <th>EnterpriseValueToEBIT_DailyTimeSeriesRatio_</th>\n",
       "      <th>EnterpriseValueToEBITDA_DailyTimeSeriesRatio_</th>\n",
       "      <th>EnterpriseValueToSales_DailyTimeSeriesRatio_</th>\n",
       "      <th>Dividend_Per_Share_SmartEstimate</th>\n",
       "      <th>CashCashEquivalents_Total</th>\n",
       "      <th>ForwardPEG_DailyTimeSeriesRatio_</th>\n",
       "      <th>PriceEarningsToGrowthRatio_SmartEstimate_</th>\n",
       "      <th>Recommendation_Median_1_5_</th>\n",
       "      <th>ReturnOnEquity_SmartEstimat</th>\n",
       "      <th>ReturnOnAssets_SmartEstimate</th>\n",
       "      <th>ForwardPriceToCashFlowPerShare_DailyTimeSeriesRatio_</th>\n",
       "      <th>ForwardPriceToSalesPerShare_DailyTimeSeriesRatio_</th>\n",
       "      <th>ForwardEnterpriseValueToOperatingCashFlow_DailyTimeSeriesRatio_</th>\n",
       "      <th>GrossProfitMargin_ActualSurprise</th>\n",
       "      <th>Estpricegrowth_percent</th>\n",
       "      <th>TradeDate</th>\n",
       "      <th>Sid</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-12-29</td>\n",
       "      <td>PRG</td>\n",
       "      <td>PRG.N</td>\n",
       "      <td>12.905048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>PROG Holdings Inc</td>\n",
       "      <td>1.417796e+08</td>\n",
       "      <td>1.745266e+08</td>\n",
       "      <td>Financials</td>\n",
       "      <td>29367000.0</td>\n",
       "      <td>2020000.0</td>\n",
       "      <td>1.148170e+08</td>\n",
       "      <td>0.30000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>23.33333</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.808409</td>\n",
       "      <td>0.651261</td>\n",
       "      <td>0.082844</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.808500e+07</td>\n",
       "      <td>0.678299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078117</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.608</td>\n",
       "      <td>0.808078</td>\n",
       "      <td>2009-12-29</td>\n",
       "      <td>198929</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-12-30</td>\n",
       "      <td>HCI</td>\n",
       "      <td>HCI.N</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hci Group Inc</td>\n",
       "      <td>-1.052045e+07</td>\n",
       "      <td>5.058155e+07</td>\n",
       "      <td>Financials</td>\n",
       "      <td>-5146000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>-0.420314</td>\n",
       "      <td>-0.419258</td>\n",
       "      <td>-0.140396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.110200e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2009-12-30</td>\n",
       "      <td>193989</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>A</td>\n",
       "      <td>A.N</td>\n",
       "      <td>22.217753</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Agilent Technologies Inc</td>\n",
       "      <td>1.125719e+10</td>\n",
       "      <td>1.083819e+10</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>183000000.0</td>\n",
       "      <td>21000000.0</td>\n",
       "      <td>2.904000e+09</td>\n",
       "      <td>0.32000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>33.00000</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>32.535225</td>\n",
       "      <td>22.335690</td>\n",
       "      <td>2.512204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.479000e+09</td>\n",
       "      <td>0.993966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.785078</td>\n",
       "      <td>1.609831</td>\n",
       "      <td>20.022743</td>\n",
       "      <td>4.081</td>\n",
       "      <td>0.485299</td>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>196290</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>AAON</td>\n",
       "      <td>AAON.OQ</td>\n",
       "      <td>5.774823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Aaon Inc</td>\n",
       "      <td>3.177753e+08</td>\n",
       "      <td>3.355783e+08</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>7037000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.064000e+06</td>\n",
       "      <td>0.13333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.51852</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>7.359657</td>\n",
       "      <td>6.096876</td>\n",
       "      <td>1.264712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.789400e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.465819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.990</td>\n",
       "      <td>0.128783</td>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>198259</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>AAP</td>\n",
       "      <td>AAP.N</td>\n",
       "      <td>40.480000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Advance Auto Parts Inc</td>\n",
       "      <td>3.945866e+09</td>\n",
       "      <td>3.830672e+09</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>152861000.0</td>\n",
       "      <td>10678000.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.69000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>47.00000</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>8.886985</td>\n",
       "      <td>6.663693</td>\n",
       "      <td>0.722495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.162150e+08</td>\n",
       "      <td>0.937168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.585366</td>\n",
       "      <td>0.679843</td>\n",
       "      <td>8.843541</td>\n",
       "      <td>-0.367</td>\n",
       "      <td>0.161067</td>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>195735</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date Symbol Instrument  RefPriceClose  RefVolume         CompanyCommonName  EnterpriseValue_DailyTimeSeries_  \\\n",
       "0 2009-12-29    PRG      PRG.N      12.905048        0.0         PROG Holdings Inc                      1.417796e+08   \n",
       "1 2009-12-30    HCI      HCI.N       7.750000        0.0             Hci Group Inc                     -1.052045e+07   \n",
       "2 2009-12-31      A        A.N      22.217753        0.0  Agilent Technologies Inc                      1.125719e+10   \n",
       "3 2009-12-31   AAON    AAON.OQ       5.774823        0.0                  Aaon Inc                      3.177753e+08   \n",
       "4 2009-12-31    AAP      AAP.N      40.480000        0.0    Advance Auto Parts Inc                      3.945866e+09   \n",
       "\n",
       "   CompanyMarketCap          GICSSectorName  FOCFExDividends_Discrete  InterestExpense_NetofCapitalizedInterest  \\\n",
       "0      1.745266e+08              Financials                29367000.0                                 2020000.0   \n",
       "1      5.058155e+07              Financials                -5146000.0                                       0.0   \n",
       "2      1.083819e+10             Health Care               183000000.0                                21000000.0   \n",
       "3      3.355783e+08             Industrials                 7037000.0                                       0.0   \n",
       "4      3.830672e+09  Consumer Discretionary               152861000.0                                10678000.0   \n",
       "\n",
       "     Debt_Total  EarningsPerShare_Actual  EarningsPerShare_SmartEstimate_prev_Q  EarningsPerShare_ActualSurprise  \\\n",
       "0  1.148170e+08                  0.30000                                    0.0                            5.996   \n",
       "1  0.000000e+00                  0.00000                                    0.0                            0.000   \n",
       "2  2.904000e+09                  0.32000                                    0.0                           37.404   \n",
       "3  3.064000e+06                  0.13333                                    0.0                           17.389   \n",
       "4  0.000000e+00                  0.69000                                    0.0                            4.625   \n",
       "\n",
       "   EarningsPerShare_SmartEstimate_current_Q  LongTermGrowth_Mean  PriceTarget_Median  CombinedAlphaModelSectorRank  \\\n",
       "0                                       0.0                 12.5            23.33333                          20.0   \n",
       "1                                       0.0                  0.0             0.00000                           0.0   \n",
       "2                                       0.0                 15.0            33.00000                          40.0   \n",
       "3                                       0.0                  0.0             6.51852                          47.0   \n",
       "4                                       0.0                 13.2            47.00000                          62.0   \n",
       "\n",
       "   CombinedAlphaModelSectorRankChange  CombinedAlphaModelRegionRank  EarningsQualityRegionRank_Current  \\\n",
       "0                                 0.0                          25.0                               55.0   \n",
       "1                                 0.0                           0.0                               83.0   \n",
       "2                                 0.0                          41.0                               40.0   \n",
       "3                                 0.0                          46.0                               70.0   \n",
       "4                                 0.0                          65.0                               99.0   \n",
       "\n",
       "   EnterpriseValueToEBIT_DailyTimeSeriesRatio_  EnterpriseValueToEBITDA_DailyTimeSeriesRatio_  \\\n",
       "0                                     0.808409                                       0.651261   \n",
       "1                                    -0.420314                                      -0.419258   \n",
       "2                                    32.535225                                      22.335690   \n",
       "3                                     7.359657                                       6.096876   \n",
       "4                                     8.886985                                       6.663693   \n",
       "\n",
       "   EnterpriseValueToSales_DailyTimeSeriesRatio_  Dividend_Per_Share_SmartEstimate  CashCashEquivalents_Total  \\\n",
       "0                                      0.082844                               0.0               8.808500e+07   \n",
       "1                                     -0.140396                               0.0               6.110200e+07   \n",
       "2                                      2.512204                               0.0               2.479000e+09   \n",
       "3                                      1.264712                               0.0               1.789400e+07   \n",
       "4                                      0.722495                               0.0               2.162150e+08   \n",
       "\n",
       "   ForwardPEG_DailyTimeSeriesRatio_  PriceEarningsToGrowthRatio_SmartEstimate_  Recommendation_Median_1_5_  \\\n",
       "0                          0.678299                                        0.0                         1.0   \n",
       "1                          0.000000                                        0.0                         0.0   \n",
       "2                          0.993966                                        0.0                         2.0   \n",
       "3                          0.000000                                        0.0                         3.0   \n",
       "4                          0.937168                                        0.0                         2.0   \n",
       "\n",
       "   ReturnOnEquity_SmartEstimat  ReturnOnAssets_SmartEstimate  ForwardPriceToCashFlowPerShare_DailyTimeSeriesRatio_  \\\n",
       "0                          0.0                           0.0                                           0.000000      \n",
       "1                          0.0                           0.0                                           0.000000      \n",
       "2                          0.0                           0.0                                          13.785078      \n",
       "3                          0.0                           0.0                                           0.000000      \n",
       "4                          0.0                           0.0                                           8.585366      \n",
       "\n",
       "   ForwardPriceToSalesPerShare_DailyTimeSeriesRatio_  ForwardEnterpriseValueToOperatingCashFlow_DailyTimeSeriesRatio_  \\\n",
       "0                                           0.078117                                           0.000000                 \n",
       "1                                           0.000000                                           0.000000                 \n",
       "2                                           1.609831                                          20.022743                 \n",
       "3                                           1.465819                                           0.000000                 \n",
       "4                                           0.679843                                           8.843541                 \n",
       "\n",
       "   GrossProfitMargin_ActualSurprise  Estpricegrowth_percent   TradeDate     Sid  pred  \n",
       "0                             1.608                0.808078  2009-12-29  198929   0.0  \n",
       "1                             0.000                0.000000  2009-12-30  193989   0.0  \n",
       "2                             4.081                0.485299  2009-12-31  196290   0.0  \n",
       "3                            24.990                0.128783  2009-12-31  198259   0.0  \n",
       "4                            -0.367                0.161067  2009-12-31  195735   0.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Cleaning data...\")\n",
    "\n",
    "# Ensure Date is datetime\n",
    "custom_data['Date'] = pd.to_datetime(custom_data['Date'])\n",
    "\n",
    "# Forward fill missing values by symbol\n",
    "print(\"  Forward filling missing values by symbol...\")\n",
    "for col in custom_data.columns:\n",
    "    if col not in ['Symbol', 'Sid', 'Date']:\n",
    "        custom_data[col] = custom_data.groupby('Symbol')[col].transform(lambda x: x.ffill())\n",
    "\n",
    "# Handle text columns vs numeric columns differently when filling remaining NaNs\n",
    "print(\"  Filling remaining NaN values...\")\n",
    "for col in custom_data.columns:\n",
    "    if col not in ['Symbol', 'Sid', 'Date']:\n",
    "        # Check if column is text/object type\n",
    "        if custom_data[col].dtype == 'object' or col in ['GICSSectorName', 'CompanyCommonName', 'TradeDate']:\n",
    "            # Fill text columns with empty string\n",
    "            custom_data[col] = custom_data[col].fillna('')\n",
    "        else:\n",
    "            # Fill numeric columns with 0\n",
    "            custom_data[col] = custom_data[col].fillna(0)\n",
    "\n",
    "# Convert Sid to integer\n",
    "custom_data['Sid'] = custom_data['Sid'].astype(int)\n",
    "\n",
    "# Sort by date and symbol\n",
    "custom_data = custom_data.sort_values(['Date', 'Symbol'])\n",
    "\n",
    "print(f\"\u2713 Data cleaned\")\n",
    "print(f\"\\nFinal dataset:\")\n",
    "print(f\"  Rows: {len(custom_data):,}\")\n",
    "print(f\"  Columns: {len(custom_data.columns)}\")\n",
    "print(f\"  Date range: {custom_data['Date'].min()} to {custom_data['Date'].max()}\")\n",
    "print(f\"  Symbols: {custom_data['Symbol'].nunique()}\")\n",
    "\n",
    "# Verify text columns don't have numeric 0\n",
    "text_cols = ['GICSSectorName', 'CompanyCommonName']\n",
    "for col in text_cols:\n",
    "    if col in custom_data.columns:\n",
    "        zero_count = (custom_data[col] == 0).sum() + (custom_data[col] == '0').sum()\n",
    "        if zero_count > 0:\n",
    "            print(f\"  WARNING: {col} has {zero_count} rows with numeric 0!\")\n",
    "        else:\n",
    "            print(f\"  \u2713 {col}: No numeric zeros, {(custom_data[col] == '').sum()} empty strings\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample cleaned data:\")\n",
    "custom_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create SQLite Database\n",
    "\n",
    "Create the custom SQLite database in Zipline format.\n",
    "\n",
    "The notebook supports three update modes (configured in Cell 2):\n",
    "- **`fresh`**: Drop and recreate the database (default for initial load)\n",
    "- **`replace`**: INSERT OR REPLACE - Updates existing records based on (Sid, Date) key\n",
    "- **`ignore`**: INSERT OR IGNORE - Skips records that already exist, keeps existing data\n",
    "\n",
    "Use `replace` mode to update data with newer values, or `ignore` mode to only add new data without overwriting existing records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================",
    "# DEDUPLICATE DATA - Fix for UNIQUE constraint failures",
    "# =============================================================================",
    "",
    "print(\"\\n\" + \"=\" * 60)",
    "print(\"DEDUPLICATING DATA\")",
    "print(\"=\" * 60)",
    "",
    "# Count before",
    "rows_before = len(custom_data)",
    "print(f\"Rows before deduplication: {rows_before:,}\")",
    "",
    "# Check for duplicates",
    "duplicates = custom_data[custom_data.duplicated(subset=['Sid', 'Date'], keep=False)]",
    "if len(duplicates) > 0:",
    "    print(f\"\u26a0\ufe0f  Found {len(duplicates):,} rows with duplicate (Sid, Date) pairs\")",
    "    ",
    "    # Show sample",
    "    dup_counts = duplicates.groupby(['Sid', 'Date']).size().reset_index(name='count')",
    "    dup_counts = dup_counts.sort_values('count', ascending=False).head(5)",
    "    print(\"\\nTop 5 most duplicated (Sid, Date) pairs:\")",
    "    print(dup_counts.to_string(index=False))",
    "",
    "# Deduplicate - keep last occurrence (most recent data)",
    "custom_data = custom_data.drop_duplicates(subset=['Sid', 'Date'], keep='last')",
    "",
    "# Count after",
    "rows_after = len(custom_data)",
    "duplicates_removed = rows_before - rows_after",
    "",
    "print(f\"\\nRows after deduplication: {rows_after:,}\")",
    "print(f\"Duplicates removed: {duplicates_removed:,}\")",
    "",
    "if duplicates_removed == 0:",
    "    print(\"\u2713 No duplicates found - data is clean!\")",
    "else:",
    "    print(f\"\u2713 Removed {duplicates_removed:,} duplicate records\")",
    "    print(\"  Strategy: Kept 'last' occurrence for each (Sid, Date) pair\")",
    "",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating database at /root/.zipline/data/custom/fundamentals.sqlite...\n",
      "  Removing existing database (mode='fresh')...\n",
      "  Creating table...\n",
      "  Creating indices...\n",
      "  Inserting 8,549,158 rows with mode='fresh'...\n",
      "  Added 11 missing columns with default values\n",
      "    Processed chunk 10/855 (100,000 rows)...\n",
      "    Processed chunk 20/855 (200,000 rows)...\n",
      "    Processed chunk 30/855 (300,000 rows)...\n",
      "    Processed chunk 40/855 (400,000 rows)...\n",
      "    Processed chunk 50/855 (500,000 rows)...\n",
      "    Processed chunk 60/855 (600,000 rows)...\n",
      "    Processed chunk 70/855 (700,000 rows)...\n",
      "    Processed chunk 80/855 (800,000 rows)...\n",
      "    Processed chunk 90/855 (900,000 rows)...\n",
      "    Processed chunk 100/855 (1,000,000 rows)...\n",
      "    Processed chunk 110/855 (1,100,000 rows)...\n",
      "    Processed chunk 120/855 (1,200,000 rows)...\n",
      "    Processed chunk 130/855 (1,300,000 rows)...\n",
      "    Processed chunk 140/855 (1,400,000 rows)...\n",
      "    Processed chunk 150/855 (1,500,000 rows)...\n",
      "    Processed chunk 160/855 (1,600,000 rows)...\n",
      "    Processed chunk 170/855 (1,700,000 rows)...\n",
      "    Processed chunk 180/855 (1,800,000 rows)...\n",
      "    Processed chunk 190/855 (1,900,000 rows)...\n",
      "    Processed chunk 200/855 (2,000,000 rows)...\n",
      "    Processed chunk 210/855 (2,100,000 rows)...\n",
      "    Processed chunk 220/855 (2,200,000 rows)...\n",
      "    Processed chunk 230/855 (2,300,000 rows)...\n",
      "    Processed chunk 240/855 (2,400,000 rows)...\n",
      "    Processed chunk 250/855 (2,500,000 rows)...\n",
      "    Processed chunk 260/855 (2,600,000 rows)...\n",
      "    Processed chunk 270/855 (2,700,000 rows)...\n",
      "    Processed chunk 280/855 (2,800,000 rows)...\n",
      "    Processed chunk 290/855 (2,900,000 rows)...\n",
      "    Processed chunk 300/855 (3,000,000 rows)...\n",
      "    Processed chunk 310/855 (3,100,000 rows)...\n",
      "    Processed chunk 320/855 (3,200,000 rows)...\n",
      "    Processed chunk 330/855 (3,300,000 rows)...\n",
      "    Processed chunk 340/855 (3,400,000 rows)...\n",
      "    Processed chunk 350/855 (3,500,000 rows)...\n",
      "    Processed chunk 360/855 (3,600,000 rows)...\n",
      "    Processed chunk 370/855 (3,700,000 rows)...\n",
      "    Processed chunk 380/855 (3,800,000 rows)...\n",
      "    Processed chunk 390/855 (3,900,000 rows)...\n",
      "    Processed chunk 400/855 (4,000,000 rows)...\n",
      "    Processed chunk 410/855 (4,100,000 rows)...\n",
      "    Processed chunk 420/855 (4,200,000 rows)...\n",
      "    Processed chunk 430/855 (4,300,000 rows)...\n",
      "    Processed chunk 440/855 (4,400,000 rows)...\n",
      "    Processed chunk 450/855 (4,500,000 rows)...\n",
      "    Processed chunk 460/855 (4,600,000 rows)...\n",
      "    Processed chunk 470/855 (4,700,000 rows)...\n",
      "    Processed chunk 480/855 (4,800,000 rows)...\n",
      "    Processed chunk 490/855 (4,900,000 rows)...\n",
      "    Processed chunk 500/855 (5,000,000 rows)...\n",
      "    Processed chunk 510/855 (5,100,000 rows)...\n",
      "    Processed chunk 520/855 (5,200,000 rows)...\n",
      "    Processed chunk 530/855 (5,300,000 rows)...\n",
      "    Processed chunk 540/855 (5,400,000 rows)...\n",
      "    Processed chunk 550/855 (5,500,000 rows)...\n",
      "    Processed chunk 560/855 (5,600,000 rows)...\n",
      "    Processed chunk 570/855 (5,700,000 rows)...\n"
     ]
    },
    {
     "ename": "IntegrityError",
     "evalue": "UNIQUE constraint failed: Price.Sid, Price.Date",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIntegrityError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[32m     83\u001b[39m chunk = insert_data.iloc[i:i+chunk_size]\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Execute batch insert\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m cursor.executemany(insert_sql, chunk.values.tolist())\n\u001b[32m     88\u001b[39m rows_affected = cursor.rowcount\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m UPDATE_MODE == \u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     90\u001b[39m     \u001b[38;5;66;03m# With INSERT OR IGNORE, rowcount shows actual inserts (not skipped)\u001b[39;00m\n",
      "\u001b[31mIntegrityError\u001b[39m: UNIQUE constraint failed: Price.Sid, Price.Date"
     ]
    }
   ],
   "source": [
    "print(f\"Creating database at {DB_PATH}...\")\n",
    "\n",
    "# Handle database based on update mode\n",
    "db_exists = DB_PATH.exists()\n",
    "\n",
    "if UPDATE_MODE == 'fresh' and db_exists:\n",
    "    print(f\"  Removing existing database (mode='fresh')...\")\n",
    "    DB_PATH.unlink()\n",
    "    db_exists = False\n",
    "elif db_exists:\n",
    "    print(f\"  Database exists - will {UPDATE_MODE} existing records...\")\n",
    "\n",
    "# Create database connection\n",
    "conn = sqlite3.connect(str(DB_PATH))\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create table if it doesn't exist (with UNIQUE constraint for upserts)\n",
    "if not db_exists or UPDATE_MODE == 'fresh':\n",
    "    columns_def = ', '.join([f'\"{col}\" {dtype}' for col, dtype in SCHEMA.items()])\n",
    "    create_table_sql = f'''\n",
    "    CREATE TABLE IF NOT EXISTS Price (\n",
    "        {columns_def},\n",
    "        UNIQUE(Sid, Date)\n",
    "    );\n",
    "    '''\n",
    "    \n",
    "    print(\"  Creating table...\")\n",
    "    cursor.execute(create_table_sql)\n",
    "    \n",
    "    # Create indices for fast lookups\n",
    "    print(\"  Creating indices...\")\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_sid ON Price(Sid);')\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_date ON Price(Date);')\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_symbol ON Price(Symbol);')\n",
    "\n",
    "# Insert data\n",
    "print(f\"  Inserting {len(custom_data):,} rows with mode='{UPDATE_MODE}'...\")\n",
    "\n",
    "# Prepare data for insertion - only use columns that exist in custom_data\n",
    "# Add missing columns with default values (0 for numeric, empty string for text)\n",
    "insert_data = custom_data.copy()\n",
    "\n",
    "# Add any missing schema columns with appropriate defaults\n",
    "missing_cols = []\n",
    "for col, dtype in SCHEMA.items():\n",
    "    if col not in insert_data.columns:\n",
    "        if dtype == 'TEXT':\n",
    "            insert_data[col] = ''\n",
    "        else:  # REAL or INTEGER\n",
    "            insert_data[col] = 0\n",
    "        missing_cols.append(col)\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"  Added {len(missing_cols)} missing columns with default values\")\n",
    "\n",
    "# Select only the columns in the schema (in the correct order)\n",
    "insert_data = insert_data[list(SCHEMA.keys())].copy()\n",
    "\n",
    "# Convert Date to string format for SQLite\n",
    "insert_data['Date'] = insert_data['Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Choose SQL command based on update mode\n",
    "if UPDATE_MODE == 'replace':\n",
    "    sql_command = 'INSERT OR REPLACE'\n",
    "elif UPDATE_MODE == 'ignore':\n",
    "    sql_command = 'INSERT OR IGNORE'\n",
    "else:  # 'fresh' or default\n",
    "    sql_command = 'INSERT'\n",
    "\n",
    "# Create parameterized INSERT statement\n",
    "columns = list(SCHEMA.keys())\n",
    "placeholders = ', '.join(['?' for _ in columns])\n",
    "column_names = ', '.join([f'\"{col}\"' for col in columns])\n",
    "insert_sql = f'{sql_command} INTO Price ({column_names}) VALUES ({placeholders})'\n",
    "\n",
    "# Insert in chunks for better performance\n",
    "chunk_size = 10000\n",
    "total_chunks = (len(insert_data) + chunk_size - 1) // chunk_size\n",
    "total_inserted = 0\n",
    "total_skipped = 0\n",
    "\n",
    "for i in range(0, len(insert_data), chunk_size):\n",
    "    chunk = insert_data.iloc[i:i+chunk_size]\n",
    "    \n",
    "    # Execute batch insert\n",
    "    cursor.executemany(insert_sql, chunk.values.tolist())\n",
    "    \n",
    "    rows_affected = cursor.rowcount\n",
    "    if UPDATE_MODE == 'ignore':\n",
    "        # With INSERT OR IGNORE, rowcount shows actual inserts (not skipped)\n",
    "        total_inserted += rows_affected\n",
    "        total_skipped += len(chunk) - rows_affected\n",
    "    else:\n",
    "        total_inserted += rows_affected\n",
    "    \n",
    "    chunk_num = i // chunk_size + 1\n",
    "    if chunk_num % 10 == 0 or chunk_num == total_chunks:\n",
    "        print(f\"    Processed chunk {chunk_num}/{total_chunks} ({i+len(chunk):,} rows)...\")\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "# Report results\n",
    "print(f\"\\n\u2713 Database operation completed!\")\n",
    "print(f\"  Path: {DB_PATH}\")\n",
    "print(f\"  Mode: {UPDATE_MODE}\")\n",
    "print(f\"  Rows processed: {len(insert_data):,}\")\n",
    "\n",
    "if UPDATE_MODE == 'ignore' and total_skipped > 0:\n",
    "    print(f\"  Rows inserted: {total_inserted:,}\")\n",
    "    print(f\"  Rows skipped (already existed): {total_skipped:,}\")\n",
    "\n",
    "# Get final row count\n",
    "cursor.execute(\"SELECT COUNT(*) FROM Price\")\n",
    "total_rows = cursor.fetchone()[0]\n",
    "print(f\"  Total rows in database: {total_rows:,}\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(f\"  Size: {DB_PATH.stat().st_size / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Define Database Class\n",
    "\n",
    "Create a Database class to use this data in Zipline Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Database class\n",
    "class CustomFundamentals(Database):\n",
    "    \"\"\"\n",
    "    Custom Custom fundamentals database.\n",
    "    \n",
    "    Usage in Pipeline:\n",
    "        roe = CustomFundamentals.ReturnOnEquity_SmartEstimat.latest\n",
    "        sector = CustomFundamentals.GICSSectorName.latest\n",
    "    \"\"\"\n",
    "    \n",
    "    CODE = DATABASE_NAME\n",
    "    LOOKBACK_WINDOW = 252  # Days to look back\n",
    "    \n",
    "    # Price and volume\n",
    "    RefPriceClose = Column(float)\n",
    "    RefVolume = Column(float)\n",
    "    \n",
    "    # Company info\n",
    "    CompanyCommonName = Column(str)\n",
    "    GICSSectorName = Column(str)\n",
    "    \n",
    "    # Valuation metrics\n",
    "    EnterpriseValue_DailyTimeSeries_ = Column(float)\n",
    "    CompanyMarketCap = Column(float)\n",
    "    \n",
    "    # Cash flow\n",
    "    FOCFExDividends_Discrete = Column(float)\n",
    "    CashFlowComponent_Current = Column(float)\n",
    "    CashFlowPerShare_BrokerEstimate = Column(float)\n",
    "    FreeCashFlowPerShare_BrokerEstimate = Column(float)\n",
    "    \n",
    "    # Debt and interest\n",
    "    InterestExpense_NetofCapitalizedInterest = Column(float)\n",
    "    Debt_Total = Column(float)\n",
    "    \n",
    "    # Earnings\n",
    "    EarningsPerShare_Actual = Column(float)\n",
    "    EarningsPerShare_SmartEstimate_prev_Q = Column(float)\n",
    "    EarningsPerShare_ActualSurprise = Column(float)\n",
    "    EarningsPerShare_SmartEstimate_current_Q = Column(float)\n",
    "    EPS_SurpirsePrct_prev_Q = Column(float)\n",
    "    \n",
    "    # Growth and targets\n",
    "    LongTermGrowth_Mean = Column(float)\n",
    "    PriceTarget_Median = Column(float)\n",
    "    Estpricegrowth_percent = Column(float)\n",
    "    \n",
    "    # Rankings\n",
    "    CombinedAlphaModelSectorRank = Column(float)\n",
    "    CombinedAlphaModelSectorRankChange = Column(float)\n",
    "    CombinedAlphaModelRegionRank = Column(float)\n",
    "    EarningsQualityRegionRank_Current = Column(float)\n",
    "    \n",
    "    # Ratios\n",
    "    EnterpriseValueToEBIT_DailyTimeSeriesRatio_ = Column(float)\n",
    "    EnterpriseValueToEBITDA_DailyTimeSeriesRatio_ = Column(float)\n",
    "    EnterpriseValueToSales_DailyTimeSeriesRatio_ = Column(float)\n",
    "    ForwardPEG_DailyTimeSeriesRatio_ = Column(float)\n",
    "    PriceEarningsToGrowthRatio_SmartEstimate_ = Column(float)\n",
    "    ForwardPriceToCashFlowPerShare_DailyTimeSeriesRatio_ = Column(float)\n",
    "    ForwardPriceToSalesPerShare_DailyTimeSeriesRatio_ = Column(float)\n",
    "    ForwardEnterpriseValueToOperatingCashFlow_DailyTimeSeriesRatio_ = Column(float)\n",
    "    \n",
    "    # Returns\n",
    "    ReturnOnInvestedCapital_BrokerEstimate = Column(float)\n",
    "    ReturnOnCapitalEmployed_Actual = Column(float)\n",
    "    ReturnOnEquity_SmartEstimat = Column(float)\n",
    "    ReturnOnAssets_SmartEstimate = Column(float)\n",
    "    \n",
    "    # Margins\n",
    "    GrossProfitMargin_ = Column(float)\n",
    "    GrossProfitMargin_ActualSurprise = Column(float)\n",
    "    \n",
    "    # Analyst recommendations\n",
    "    Recommendation_NumberOfTotal = Column(float)\n",
    "    Recommendation_Median_1_5_ = Column(float)\n",
    "    Recommendation_NumberOfStrongBuy = Column(float)\n",
    "    Recommendation_NumberOfBuy = Column(float)\n",
    "    Recommendation_Mean_1_5_ = Column(float)\n",
    "    \n",
    "    # Cash\n",
    "    CashCashEquivalents_Total = Column(float)\n",
    "    \n",
    "    # Dividends\n",
    "    Dividend_Per_Share_SmartEstimate = Column(float)\n",
    "    \n",
    "    # VIX prediction signal\n",
    "    pred = Column(float)\n",
    "\n",
    "\n",
    "print(\"\u2713 CustomFundamentals Database class defined\")\n",
    "print(f\"  Database code: {CustomFundamentals.CODE}\")\n",
    "print(f\"  Lookback window: {CustomFundamentals.LOOKBACK_WINDOW} days\")\n",
    "# Count columns by checking for 'dataset' attribute (BoundColumn instances have this)\n",
    "print(f\"  Columns defined: {len([attr for attr in dir(CustomFundamentals) if hasattr(getattr(CustomFundamentals, attr, None), 'dataset')])}\")\n",
    "\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"  roe = CustomFundamentals.ReturnOnEquity_SmartEstimat.latest\")\n",
    "print(\"  pe_growth = CustomFundamentals.PriceEarningsToGrowthRatio_SmartEstimate_.latest\")\n",
    "print(\"  sector = CustomFundamentals.GICSSectorName.latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Verify Database\n",
    "\n",
    "Query the database to verify data was loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect and query\n",
    "conn = sqlite3.connect(str(DB_PATH))\n",
    "\n",
    "# Get row count\n",
    "row_count = pd.read_sql(\"SELECT COUNT(*) as count FROM Price\", conn).iloc[0, 0]\n",
    "print(f\"Total rows in database: {row_count:,}\")\n",
    "\n",
    "# Get date range\n",
    "date_range = pd.read_sql(\"SELECT MIN(Date) as min_date, MAX(Date) as max_date FROM Price\", conn)\n",
    "print(f\"Date range: {date_range.iloc[0, 0]} to {date_range.iloc[0, 1]}\")\n",
    "\n",
    "# Get symbol count\n",
    "symbol_count = pd.read_sql(\"SELECT COUNT(DISTINCT Symbol) as count FROM Price\", conn).iloc[0, 0]\n",
    "print(f\"Unique symbols: {symbol_count:,}\")\n",
    "\n",
    "# Show sample data for a specific symbol\n",
    "print(\"\\nSample data for AAPL:\")\n",
    "aapl_data = pd.read_sql(\"\"\"\n",
    "    SELECT Date, Symbol, RefPriceClose, CompanyMarketCap, \n",
    "           ReturnOnEquity_SmartEstimat, PriceTarget_Median\n",
    "    FROM Price \n",
    "    WHERE Symbol = 'AAPL' \n",
    "    ORDER BY Date DESC \n",
    "    LIMIT 5\n",
    "\"\"\", conn)\n",
    "print(aapl_data)\n",
    "\n",
    "print(\"\\nSample data for IBM:\")\n",
    "ibm_data = pd.read_sql(\"\"\"\n",
    "    SELECT Date, Symbol, RefPriceClose, CompanyMarketCap, \n",
    "           ReturnOnEquity_SmartEstimat, GICSSectorName\n",
    "    FROM Price \n",
    "    WHERE Symbol = 'IBM' \n",
    "    ORDER BY Date DESC \n",
    "    LIMIT 5\n",
    "\"\"\", conn)\n",
    "print(ibm_data)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"\\n\u2713 Database verification complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Usage Example\n",
    "\n",
    "Example of how to use this database in a backtest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"To use this database in your backtests:\")\n",
    "print(\"\\n1. Import the Database class:\")\n",
    "print(\"   from zipline.pipeline.data.db import Database, Column\")\n",
    "print(\"\\n2. Define the CustomFundamentals class (from cell 10 above)\")\n",
    "print(\"\\n3. Use in your pipeline:\")\n",
    "print(\"   \")\n",
    "print(\"   def make_pipeline():\")\n",
    "print(\"       roe = CustomFundamentals.ReturnOnEquity_SmartEstimat.latest\")\n",
    "print(\"       growth = CustomFundamentals.LongTermGrowth_Mean.latest\")\n",
    "print(\"       sector = CustomFundamentals.GICSSectorName.latest\")\n",
    "print(\"       \")\n",
    "print(\"       # Screen for quality companies\")\n",
    "print(\"       quality = (roe > 15) & (growth > 10)\")\n",
    "print(\"       \")\n",
    "print(\"       return Pipeline(\")\n",
    "print(\"           columns={\")\n",
    "print(\"               'ROE': roe,\")\n",
    "print(\"               'Growth': growth,\")\n",
    "print(\"               'Sector': sector,\")\n",
    "print(\"           },\")\n",
    "print(\"           screen=quality\")\n",
    "print(\"       )\")\n",
    "print(\"\\n4. The CustomSQLiteLoader will automatically load data based on CustomFundamentals.CODE\")\n",
    "\n",
    "print(\"\\n\u2713 Setup complete! Your custom fundamentals database is ready to use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "1. \u2705 Loaded CSV files with fundamental data\n",
    "2. \u2705 Mapped symbols to Zipline SIDs using the asset finder\n",
    "3. \u2705 Cleaned and prepared the data\n",
    "4. \u2705 Created a custom SQLite database in ~/.zipline/data/custom/\n",
    "5. \u2705 Defined a Database class for use in Pipeline\n",
    "6. \u2705 Verified the database contents\n",
    "\n",
    "The database is now ready to use in your Zipline backtests with the CustomSQLiteLoader.\n",
    "\n",
    "**Next steps:**\n",
    "- See the examples below for using the data with Pipeline\n",
    "- Copy the CustomFundamentals class definition to your backtest algorithm\n",
    "- Use CustomFundamentals.ColumnName.latest in your pipeline\n",
    "- The backtest_helpers.py will automatically detect and load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Pipeline Examples\n",
    "\n",
    "Now let's demonstrate how to query and analyze the fundamentals data using Zipline Pipeline.\n",
    "\n",
    "These examples show:\n",
    "- Creating a pipeline with custom fundamentals\n",
    "- Running the pipeline over date ranges\n",
    "- Filtering stocks by fundamental criteria\n",
    "- Extracting time series data for specific symbols\n",
    "- Combining multiple fundamental factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Pipeline Examples\n",
    "\n",
    "Now let's demonstrate how to query and analyze the fundamentals data using Zipline Pipeline.\n",
    "\n",
    "These examples show:\n",
    "- Creating a pipeline with custom fundamentals\n",
    "- Running the pipeline over date ranges\n",
    "- Filtering stocks by fundamental criteria\n",
    "- Extracting time series data for specific symbols\n",
    "- Combining multiple fundamental factors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Setup Pipeline Engine\n",
    "\n",
    "First, we need to set up the Pipeline engine to load our custom data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.pipeline import Pipeline\n",
    "from zipline.pipeline.engine import SimplePipelineEngine\n",
    "from zipline.pipeline.loaders import USEquityPricingLoader\n",
    "from zipline.pipeline.data import USEquityPricing\n",
    "from zipline.pipeline.domain import US_EQUITIES\n",
    "from zipline.utils.calendar_utils import get_calendar\n",
    "from pathlib import Path\n",
    "\n",
    "# Import the custom loader from zipline\n",
    "from zipline.data.custom import CustomSQLiteLoader\n",
    "\n",
    "# Get the trading calendar\n",
    "trading_calendar = get_calendar('NYSE')\n",
    "\n",
    "# Cache loader instances so we return the same object for all columns from a dataset\n",
    "_loader_cache = {}\n",
    "\n",
    "# Set up the pipeline engine with our custom loaders\n",
    "def get_pipeline_loader(column):\n",
    "    \"\"\"\n",
    "    Pipeline loader factory that routes columns to appropriate loaders.\n",
    "    Returns the same loader instance for all columns from the same dataset.\n",
    "    \"\"\"\n",
    "    # Route custom fundamentals to CustomSQLiteLoader\n",
    "    # Domain-bound datasets don't have CODE attribute, so check the dataset's __name__\n",
    "    dataset = column.dataset\n",
    "    \n",
    "    # Check if this is our CustomFundamentals dataset\n",
    "    # Domain-bound datasets have a __name__ attribute with the dataset class name\n",
    "    dataset_name = getattr(dataset, '__name__', '')\n",
    "    \n",
    "    if 'CustomFundamentals' in dataset_name or 'CustomFundamentals' in str(dataset):\n",
    "        # Return cached loader instance for this database\n",
    "        cache_key = CustomFundamentals.CODE\n",
    "        if cache_key not in _loader_cache:\n",
    "            # Specify the correct database directory where we created the database\n",
    "            db_dir = Path('/root/.zipline/data/custom')\n",
    "            _loader_cache[cache_key] = CustomSQLiteLoader(\n",
    "                db_code=CustomFundamentals.CODE,\n",
    "                db_dir=db_dir\n",
    "            )\n",
    "        return _loader_cache[cache_key]\n",
    "    \n",
    "    # Route pricing data to bundle\n",
    "    if column in USEquityPricing.columns:\n",
    "        # Use cached pricing loader\n",
    "        if 'pricing' not in _loader_cache:\n",
    "            _loader_cache['pricing'] = USEquityPricingLoader(\n",
    "                bundle_data.equity_daily_bar_reader, \n",
    "                bundle_data.adjustment_reader\n",
    "            )\n",
    "        return _loader_cache['pricing']\n",
    "    \n",
    "    raise ValueError(f\"No loader for {column}\")\n",
    "\n",
    "# Create the pipeline engine\n",
    "engine = SimplePipelineEngine(\n",
    "    get_loader=get_pipeline_loader,\n",
    "    asset_finder=asset_finder,\n",
    "    default_domain=US_EQUITIES,\n",
    ")\n",
    "\n",
    "print(\"\u2713 Pipeline engine configured with custom fundamentals loader\")\n",
    "print(f\"  Trading calendar: {trading_calendar.name}\")\n",
    "print(f\"  Asset finder: {len(asset_finder.sids):,} securities\")\n",
    "print(f\"  Database directory: {Path('/root/.zipline/data/custom')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Basic Pipeline - Get Latest Fundamentals\n",
    "\n",
    "Create a simple pipeline to get the latest fundamentals for all stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pipeline with market cap filter for top 100 stocks\n",
    "def make_basic_pipeline():\n",
    "    \"\"\"\n",
    "    Get latest fundamentals for top 100 stocks by market cap.\n",
    "    This reduces sparse data issues.\n",
    "    \"\"\"\n",
    "    # Get fundamentals\n",
    "    roe = CustomFundamentals.ReturnOnEquity_SmartEstimat.latest\n",
    "    roa = CustomFundamentals.ReturnOnAssets_SmartEstimate.latest\n",
    "    market_cap = CustomFundamentals.CompanyMarketCap.latest\n",
    "    price = CustomFundamentals.RefPriceClose.latest\n",
    "    sector = CustomFundamentals.GICSSectorName.latest\n",
    "    ev_to_ebitda = CustomFundamentals.EnterpriseValueToEBITDA_DailyTimeSeriesRatio_.latest\n",
    "    \n",
    "    # Screen for top 100 stocks by market cap\n",
    "    # This eliminates sparse data issues with small/inactive stocks\n",
    "    top_100_by_mcap = market_cap.top(100)\n",
    "    \n",
    "    return Pipeline(\n",
    "        columns={\n",
    "            'ROE': roe,\n",
    "            'ROA': roa,\n",
    "            'Market_Cap': market_cap,\n",
    "            'Price': price,\n",
    "            'Sector': sector,\n",
    "            'EV_to_EBITDA': ev_to_ebitda,\n",
    "        },\n",
    "        screen=top_100_by_mcap,\n",
    "    )\n",
    "\n",
    "# Run the pipeline for a single date\n",
    "# Get a recent valid trading session from the bundle (last 3 months)\n",
    "pipeline = make_basic_pipeline()\n",
    "\n",
    "# Use recent trading sessions (last 3 months of data)\n",
    "# Note: sessions_in_range expects timezone-naive dates at midnight\n",
    "end_search = pd.Timestamp.now().normalize()\n",
    "start_search = (end_search - pd.DateOffset(months=3)).normalize()\n",
    "\n",
    "sessions = trading_calendar.sessions_in_range(start_search, end_search)\n",
    "start_date = sessions[-5]  # Use 5 days back from the end\n",
    "end_date = start_date\n",
    "\n",
    "print(f\"Using date: {start_date.date()}\")\n",
    "print(f\"Running pipeline with top 100 stocks by market cap filter...\")\n",
    "\n",
    "result = engine.run_pipeline(pipeline, start_date, end_date)\n",
    "\n",
    "print(f\"\\n\u2713 Pipeline run complete\")\n",
    "print(f\"  Date: {start_date.date()}\")\n",
    "print(f\"  Stocks in universe: {len(result):,}\")\n",
    "print(f\"  Sector breakdown:\")\n",
    "print(result['Sector'].value_counts())\n",
    "\n",
    "print(f\"\\nTop 10 stocks by ROE:\")\n",
    "print(result.nlargest(10, 'ROE')[['ROE', 'ROA', 'Market_Cap', 'Sector']])\n",
    "\n",
    "print(f\"\\nTop 10 stocks by Market Cap:\")\n",
    "top_mcap = result.nlargest(10, 'Market_Cap')[['Market_Cap', 'ROE', 'Price', 'Sector']]\n",
    "top_mcap['Market_Cap_B'] = top_mcap['Market_Cap'] / 1e9  # Convert to billions\n",
    "print(top_mcap[['Market_Cap_B', 'ROE', 'Price', 'Sector']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Filtered Pipeline - Quality Stocks\n",
    "\n",
    "Filter stocks based on fundamental criteria (e.g., high ROE, profitable, large cap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_quality_pipeline():\n",
    "    \"\"\"\n",
    "    Screen for quality stocks with strong fundamentals.\n",
    "    Limited to top 500 by market cap to avoid sparse data.\n",
    "    \"\"\"\n",
    "    # Get fundamentals\n",
    "    roe = CustomFundamentals.ReturnOnEquity_SmartEstimat.latest\n",
    "    roa = CustomFundamentals.ReturnOnAssets_SmartEstimate.latest\n",
    "    market_cap = CustomFundamentals.CompanyMarketCap.latest\n",
    "    growth = CustomFundamentals.LongTermGrowth_Mean.latest\n",
    "    price_target = CustomFundamentals.PriceTarget_Median.latest\n",
    "    current_price = CustomFundamentals.RefPriceClose.latest\n",
    "    sector = CustomFundamentals.GICSSectorName.latest\n",
    "    \n",
    "    # Calculate upside potential\n",
    "    upside = ((price_target - current_price) / current_price) * 100\n",
    "    \n",
    "    # First filter: top 500 stocks by market cap (reduces sparse data)\n",
    "    top_500_by_mcap = market_cap.top(500)\n",
    "    \n",
    "    # Quality criteria (applied to top 500)\n",
    "    quality_screen = (\n",
    "        top_500_by_mcap &\n",
    "        (roe > 15) &  # Strong return on equity\n",
    "        (roa > 5) &   # Profitable\n",
    "        (market_cap > 1_000_000_000) &  # Large cap ($1B+)\n",
    "        (growth > 10) &  # Double-digit growth\n",
    "        (upside > 10)  # At least 10% upside\n",
    "    )\n",
    "    \n",
    "    return Pipeline(\n",
    "        columns={\n",
    "            'ROE': roe,\n",
    "            'ROA': roa,\n",
    "            'Market_Cap': market_cap,\n",
    "            'Growth': growth,\n",
    "            'Price': current_price,\n",
    "            'Target': price_target,\n",
    "            'Upside_%': upside,\n",
    "            'Sector': sector,\n",
    "        },\n",
    "        screen=quality_screen,\n",
    "    )\n",
    "\n",
    "# Run the filtered pipeline\n",
    "# Use a recent valid trading session (last 3 months)\n",
    "# Note: sessions_in_range expects timezone-naive dates at midnight\n",
    "end_search = pd.Timestamp.now().normalize()\n",
    "start_search = (end_search - pd.DateOffset(months=3)).normalize()\n",
    "\n",
    "sessions = trading_calendar.sessions_in_range(start_search, end_search)\n",
    "start_date = sessions[-5]  # Use 5 days back from the end\n",
    "\n",
    "print(f\"Running quality screen on top 500 stocks by market cap...\")\n",
    "pipeline = make_quality_pipeline()\n",
    "result = engine.run_pipeline(pipeline, start_date, start_date)\n",
    "\n",
    "print(f\"\\n\u2713 Quality screen results:\")\n",
    "print(f\"  Date: {start_date.date()}\")\n",
    "print(f\"  Stocks passing screen: {len(result)}\")\n",
    "\n",
    "if len(result) > 0:\n",
    "    print(f\"  Sector breakdown:\")\n",
    "    print(result['Sector'].value_counts())\n",
    "    \n",
    "    print(f\"\\nTop 10 by upside potential:\")\n",
    "    top_upside = result.nlargest(10, 'Upside_%')[['ROE', 'Growth', 'Price', 'Target', 'Upside_%', 'Sector']]\n",
    "    print(top_upside)\n",
    "else:\n",
    "    print(\"\\n  No stocks passed the quality screen criteria.\")\n",
    "    print(\"  Try relaxing the filters (e.g., ROE > 10, Growth > 5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Time Series Data - Track Fundamentals Over Time\n",
    "\n",
    "Get historical fundamental data for specific symbols to analyze trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define symbols to track\n",
    "symbols = ['AAPL', 'MSFT', 'GOOGL']\n",
    "\n",
    "# Get the assets\n",
    "assets = [asset_finder.lookup_symbol(sym, as_of_date=None) for sym in symbols]\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(\n",
    "    columns={\n",
    "        'ROE': CustomFundamentals.ReturnOnEquity_SmartEstimat.latest,\n",
    "        'Market_Cap': CustomFundamentals.CompanyMarketCap.latest,\n",
    "        'Price': CustomFundamentals.RefPriceClose.latest,\n",
    "        'Growth': CustomFundamentals.LongTermGrowth_Mean.latest,\n",
    "        'EV_EBITDA': CustomFundamentals.EnterpriseValueToEBITDA_DailyTimeSeriesRatio_.latest,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Run over a date range (last 2 months of available data)\n",
    "# Get valid trading sessions from the calendar\n",
    "# Note: sessions_in_range expects timezone-naive dates at midnight\n",
    "end_search = pd.Timestamp.now().normalize()\n",
    "start_search = (end_search - pd.DateOffset(months=3)).normalize()\n",
    "\n",
    "sessions = trading_calendar.sessions_in_range(start_search, end_search)\n",
    "end_date = sessions[-5]  # Use 5 days back from the end\n",
    "start_date = (end_date - pd.DateOffset(months=2)).normalize()\n",
    "\n",
    "# Ensure start_date is a valid trading session\n",
    "start_date = trading_calendar.sessions_in_range(start_date, end_date)[0]\n",
    "\n",
    "print(f\"Date range: {start_date.date()} to {end_date.date()}\")\n",
    "\n",
    "result = engine.run_pipeline(pipeline, start_date, end_date)\n",
    "\n",
    "print(f\"\u2713 Time series data extracted\")\n",
    "print(f\"  Period: {start_date.date()} to {end_date.date()}\")\n",
    "print(f\"  Total observations: {len(result):,}\")\n",
    "\n",
    "# Filter to our symbols of interest\n",
    "symbol_data = result[result.index.get_level_values(1).isin(assets)]\n",
    "\n",
    "print(f\"  Observations for {symbols}: {len(symbol_data):,}\")\n",
    "\n",
    "# Show AAPL time series\n",
    "aapl_asset = assets[0]\n",
    "aapl_data = symbol_data.loc[pd.IndexSlice[:, aapl_asset], :]\n",
    "\n",
    "print(f\"\\nAAPL Fundamental Trends (last 10 observations):\")\n",
    "print(aapl_data.tail(10)[['ROE', 'Market_Cap', 'Price', 'Growth']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: Visualize Time Series - Plot Fundamental Trends\n",
    "\n",
    "Create charts to visualize how fundamentals change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Define symbols to track (re-define in case previous cell wasn't run)\n",
    "symbols = ['AAPL', 'MSFT', 'GOOGL']\n",
    "assets = [asset_finder.lookup_symbol(sym, as_of_date=None) for sym in symbols]\n",
    "\n",
    "# Create pipeline if not already created in previous cell\n",
    "if 'symbol_data' not in locals():\n",
    "    print(\"Fetching time series data...\")\n",
    "    pipeline = Pipeline(\n",
    "        columns={\n",
    "            'ROE': CustomFundamentals.ReturnOnEquity_SmartEstimat.latest,\n",
    "            'Market_Cap': CustomFundamentals.CompanyMarketCap.latest,\n",
    "            'Price': CustomFundamentals.RefPriceClose.latest,\n",
    "            'Growth': CustomFundamentals.LongTermGrowth_Mean.latest,\n",
    "            'EV_EBITDA': CustomFundamentals.EnterpriseValueToEBITDA_DailyTimeSeriesRatio_.latest,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Get valid trading sessions (last 3 months, timezone-naive at midnight)\n",
    "    end_search = pd.Timestamp.now().normalize()\n",
    "    start_search = (end_search - pd.DateOffset(months=3)).normalize()\n",
    "    \n",
    "    sessions = trading_calendar.sessions_in_range(start_search, end_search)\n",
    "    end_date = sessions[-5]\n",
    "    start_date = (end_date - pd.DateOffset(months=2)).normalize()\n",
    "    start_date = trading_calendar.sessions_in_range(start_date, end_date)[0]\n",
    "    \n",
    "    result = engine.run_pipeline(pipeline, start_date, end_date)\n",
    "    symbol_data = result[result.index.get_level_values(1).isin(assets)]\n",
    "    print(f\"\u2713 Fetched {len(symbol_data):,} observations\")\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Fundamental Trends: AAPL, MSFT, GOOGL', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Prepare data for each symbol\n",
    "symbol_colors = {'AAPL': 'blue', 'MSFT': 'green', 'GOOGL': 'red'}\n",
    "\n",
    "for idx, (symbol, asset) in enumerate(zip(symbols, assets)):\n",
    "    sym_data = symbol_data.loc[pd.IndexSlice[:, asset], :]\n",
    "    sym_data = sym_data.reset_index(names=['date', 'asset'])\n",
    "    \n",
    "    color = symbol_colors[symbol]\n",
    "    \n",
    "    # Plot 1: ROE over time\n",
    "    axes[0, 0].plot(sym_data['date'], sym_data['ROE'], \n",
    "                    label=symbol, marker='o', color=color, alpha=0.7)\n",
    "    \n",
    "    # Plot 2: Market Cap over time\n",
    "    axes[0, 1].plot(sym_data['date'], sym_data['Market_Cap'] / 1e9, \n",
    "                    label=symbol, marker='s', color=color, alpha=0.7)\n",
    "    \n",
    "    # Plot 3: Growth Rate over time\n",
    "    axes[1, 0].plot(sym_data['date'], sym_data['Growth'], \n",
    "                    label=symbol, marker='^', color=color, alpha=0.7)\n",
    "    \n",
    "    # Plot 4: EV/EBITDA over time\n",
    "    axes[1, 1].plot(sym_data['date'], sym_data['EV_EBITDA'], \n",
    "                    label=symbol, marker='D', color=color, alpha=0.7)\n",
    "\n",
    "# Customize subplots\n",
    "axes[0, 0].set_title('Return on Equity (%)', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('ROE (%)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].set_title('Market Capitalization', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Market Cap ($B)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].set_title('Long-term Growth Rate', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Growth (%)')\n",
    "axes[1, 0].set_xlabel('Date')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].set_title('Enterprise Value / EBITDA', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('EV/EBITDA Ratio')\n",
    "axes[1, 1].set_xlabel('Date')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Format x-axis dates\n",
    "for ax in axes.flat:\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2713 Fundamental trends visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Examples Summary\n",
    "\n",
    "You now know how to:\n",
    "- \u2705 Set up a Pipeline engine with custom fundamentals\n",
    "- \u2705 Query latest fundamentals for all stocks\n",
    "- \u2705 Filter stocks using fundamental criteria\n",
    "- \u2705 Extract time series data for specific symbols\n",
    "- \u2705 Visualize fundamental trends over time\n",
    "\n",
    "**Key takeaways:**\n",
    "- Use `CustomFundamentals.ColumnName.latest` to access any fundamental metric\n",
    "- Combine multiple metrics with boolean operators (`&`, `|`) for screening\n",
    "- Run pipelines over date ranges to analyze trends\n",
    "- Filter results by asset to focus on specific symbols\n",
    "- Integrate with matplotlib for visualization\n",
    "\n",
    "**Next steps:**\n",
    "- Use these patterns in your backtesting algorithms\n",
    "- Create custom factors combining multiple fundamentals\n",
    "- Integrate with price data from USEquityPricing\n",
    "- Build sophisticated stock selection strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Running Backtests with Custom Fundamentals\n",
    "\n",
    "### Using the Strategy File\n",
    "\n",
    "I've created a working strategy file: **`strategy_top5_roe.py`**\n",
    "\n",
    "This strategy:\n",
    "- \u2705 Uses your custom Custom fundamentals\n",
    "- \u2705 Filters to top 100 stocks by market cap\n",
    "- \u2705 Selects top 5 stocks by ROE\n",
    "- \u2705 Rebalances weekly (every Monday)\n",
    "- \u2705 Equal weights (20% each)\n",
    "\n",
    "### How to Run\n",
    "\n",
    "**From terminal/command line:**\n",
    "```bash\n",
    "cd /notebooks\n",
    "python strategy_top5_roe.py\n",
    "```\n",
    "\n",
    "**From Jupyter:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the strategy\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "result = subprocess.run(\n",
    "    [sys.executable, 'strategy_top5_roe.py'],\n",
    "    cwd='/notebooks',\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load saved results\n",
    "results = pd.read_pickle('/notebooks/backtest_results.pkl')\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "# Portfolio value\n",
    "axes[0].plot(results.index, results['portfolio_value'], linewidth=2)\n",
    "axes[0].set_ylabel('Portfolio Value ($)', fontsize=12)\n",
    "axes[0].set_title('Top 5 ROE Strategy Performance', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Number of positions\n",
    "axes[1].plot(results.index, results['num_positions'], linewidth=2, color='orange')\n",
    "axes[1].set_ylabel('Number of Positions', fontsize=12)\n",
    "axes[1].set_xlabel('Date', fontsize=12)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPerformance Summary:\")\n",
    "print(f\"  Final Value: ${results.portfolio_value.iloc[-1]:,.2f}\")\n",
    "print(f\"  Total Return: {(results.portfolio_value.iloc[-1]/100000-1)*100:.2f}%\")\n",
    "print(f\"  Avg Positions: {results.num_positions.mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze with pyfolio\n",
    "import pyfolio as pf\n",
    "\n",
    "returns = results.returns\n",
    "pf.create_simple_tear_sheet(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing the Strategy\n",
    "\n",
    "Edit `strategy_top5_roe.py` to:\n",
    "\n",
    "**Change selection criteria:**\n",
    "```python\n",
    "# Select top 10 instead of top 5\n",
    "top_10_roe = roe.top(10, mask=top_100_by_mcap)\n",
    "\n",
    "# Use different fundamentals\n",
    "roa = CustomFundamentals.ReturnOnAssets_SmartEstimate.latest\n",
    "growth = CustomFundamentals.LongTermGrowth_Mean.latest\n",
    "\n",
    "# Combine multiple factors\n",
    "roe_z = roe.zscore(mask=top_100_by_mcap)\n",
    "growth_z = growth.zscore(mask=top_100_by_mcap)\n",
    "combined = (roe_z + growth_z) / 2\n",
    "top_stocks = combined.top(5, mask=top_100_by_mcap)\n",
    "```\n",
    "\n",
    "**Change rebalancing frequency:**\n",
    "```python\n",
    "# Monthly rebalancing\n",
    "schedule_function(\n",
    "    rebalance,\n",
    "    date_rules.month_start(),\n",
    "    time_rules.market_open(hours=1),\n",
    ")\n",
    "\n",
    "# Daily rebalancing\n",
    "schedule_function(\n",
    "    rebalance,\n",
    "    date_rules.every_day(),\n",
    "    time_rules.market_open(hours=1),\n",
    ")\n",
    "```\n",
    "\n",
    "**Change date range:**\n",
    "```python\n",
    "start = pd.Timestamp('2023-01-01', tz='UTC')\n",
    "end = pd.Timestamp('2024-12-31', tz='UTC')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}