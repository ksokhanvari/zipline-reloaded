{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "example-header",
   "metadata": {},
   "source": [
    "# Backtest Helpers Example\n",
    "\n",
    "This notebook demonstrates how to use the `backtest_helpers` module to run backtests and analyze results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## 1. Import Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hpbsljp72q8",
   "source": "## 0. Register Sharadar Bundle\n\n**Important:** Jupyter notebooks don't automatically load bundle extensions, so we need to register the sharadar bundle manually.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "lyg1mn1105p",
   "source": "# Register the sharadar bundle (required for Jupyter notebooks)\nfrom zipline.data.bundles import register\nfrom zipline.data.bundles.sharadar_bundle import sharadar_bundle\n\nregister(\n    'sharadar',\n    sharadar_bundle(\n        tickers=None,\n        incremental=True,\n        include_funds=True,\n    ),\n)\n\n# Verify registration\nfrom zipline.data.bundles import bundles\nprint(f\"âœ“ Sharadar bundle registered!\")\nprint(f\"Available bundles: {list(bundles.keys())}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the helper functions\n",
    "from backtest_helpers import backtest, analyze_results, quick_backtest\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configure display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 120)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-example-header",
   "metadata": {},
   "source": [
    "## 2. Run a Backtest\n",
    "\n",
    "The `backtest()` function runs a backtest and saves results to disk.\n",
    "\n",
    "**Parameters matching your example:**\n",
    "- First parameter: algorithm filename\n",
    "- Second parameter: backtest name/identifier\n",
    "- Last parameter: output filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-backtest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run backtest with your exact signature\n",
    "perf = backtest(\n",
    "    \"backtest_with_fundamentals.py\",  # Algorithm file\n",
    "    \"quality-strategy-v1\",             # Name/identifier\n",
    "    bundle=\"sharadar\",\n",
    "    data_frequency='daily',\n",
    "    progress='D',\n",
    "    start_date=\"2023-04-01\",\n",
    "    end_date=\"2024-01-31\",\n",
    "    capital_base=100000,\n",
    "    filepath_or_buffer=\"quality-strategy-v1.csv\"  # Output filename\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect-results-header",
   "metadata": {},
   "source": [
    "## 3. Inspect Results DataFrame\n",
    "\n",
    "The backtest returns a DataFrame with performance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first few rows\n",
    "print(\"Results DataFrame:\")\n",
    "print(f\"Shape: {perf.shape}\")\n",
    "print(f\"Columns: {list(perf.columns)}\")\n",
    "print()\n",
    "perf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "view-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View summary statistics\n",
    "perf[['portfolio_value', 'returns', 'algorithm_period_return']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyze-header",
   "metadata": {},
   "source": [
    "## 4. Analyze Results with Pyfolio\n",
    "\n",
    "The `analyze_results()` function loads saved results and generates comprehensive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the backtest results\n",
    "results = analyze_results(\n",
    "    \"./backtest_results/quality-strategy-v1.csv\",\n",
    "    benchmark_symbol=\"SPY\",\n",
    "    show_plots=True,\n",
    "    return_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "view-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View detailed metrics\n",
    "print(\"Performance Metrics:\")\n",
    "for metric, value in results['metrics'].items():\n",
    "    print(f\"  {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-backtests-header",
   "metadata": {},
   "source": [
    "## 5. Run Multiple Backtests\n",
    "\n",
    "Example of running multiple backtests with different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-backtests",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different test scenarios\n",
    "scenarios = [\n",
    "    {\n",
    "        'name': 'short-period',\n",
    "        'start_date': '2023-01-01',\n",
    "        'end_date': '2023-06-30',\n",
    "        'capital_base': 100000,\n",
    "    },\n",
    "    {\n",
    "        'name': 'long-period',\n",
    "        'start_date': '2022-01-01',\n",
    "        'end_date': '2023-12-31',\n",
    "        'capital_base': 100000,\n",
    "    },\n",
    "    {\n",
    "        'name': 'high-capital',\n",
    "        'start_date': '2023-01-01',\n",
    "        'end_date': '2023-12-31',\n",
    "        'capital_base': 1000000,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Run all scenarios\n",
    "results_dict = {}\n",
    "\n",
    "for scenario in scenarios:\n",
    "    name = scenario['name']\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Running scenario: {name}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    perf = backtest(\n",
    "        \"backtest_with_fundamentals.py\",\n",
    "        name,\n",
    "        bundle=\"sharadar\",\n",
    "        start_date=scenario['start_date'],\n",
    "        end_date=scenario['end_date'],\n",
    "        capital_base=scenario['capital_base'],\n",
    "        filepath_or_buffer=f\"{name}.csv\"\n",
    "    )\n",
    "    \n",
    "    results_dict[name] = perf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare-header",
   "metadata": {},
   "source": [
    "## 6. Compare Multiple Backtests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-backtests",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare cumulative returns\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "for name, perf in results_dict.items():\n",
    "    # Calculate cumulative returns\n",
    "    if 'returns' in perf.columns:\n",
    "        returns = perf['returns']\n",
    "    else:\n",
    "        returns = perf['portfolio_value'].pct_change().fillna(0)\n",
    "    \n",
    "    cumulative = (returns + 1).cumprod() - 1\n",
    "    ax.plot(perf.index, cumulative * 100, label=name, linewidth=2)\n",
    "\n",
    "ax.set_title('Cumulative Returns Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Return (%)')\n",
    "ax.set_xlabel('Date')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_data = []\n",
    "\n",
    "for name, perf in results_dict.items():\n",
    "    if 'returns' in perf.columns:\n",
    "        returns = perf['returns']\n",
    "    else:\n",
    "        returns = perf['portfolio_value'].pct_change().fillna(0)\n",
    "    \n",
    "    total_return = (returns + 1).cumprod()[-1] - 1\n",
    "    annual_vol = returns.std() * np.sqrt(252)\n",
    "    sharpe = (returns.mean() / returns.std() * np.sqrt(252)) if returns.std() > 0 else 0\n",
    "    \n",
    "    cumulative = (returns + 1).cumprod()\n",
    "    running_max = cumulative.cummax()\n",
    "    drawdown = (cumulative - running_max) / running_max\n",
    "    max_dd = drawdown.min()\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Scenario': name,\n",
    "        'Total Return': f\"{total_return:.2%}\",\n",
    "        'Annual Vol': f\"{annual_vol:.2%}\",\n",
    "        'Sharpe': f\"{sharpe:.3f}\",\n",
    "        'Max DD': f\"{max_dd:.2%}\",\n",
    "        'Final Value': f\"${perf['portfolio_value'].iloc[-1]:,.0f}\",\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nBacktest Comparison:\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-backtest-header",
   "metadata": {},
   "source": [
    "## 7. Quick Backtest\n",
    "\n",
    "For rapid testing, use `quick_backtest()` to run and analyze in one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-backtest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick backtest - runs and analyzes automatically\n",
    "perf = quick_backtest(\n",
    "    \"backtest_with_fundamentals.py\",\n",
    "    name=\"quick-test\",\n",
    "    start_date=\"2023-01-01\",\n",
    "    end_date=\"2023-12-31\",\n",
    "    capital_base=100000,\n",
    "    bundle=\"sharadar\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reload-header",
   "metadata": {},
   "source": [
    "## 8. Reload Previous Results\n",
    "\n",
    "You can analyze previously saved results without re-running the backtest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reload-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze a previously saved backtest\n",
    "analyze_results(\n",
    "    \"./backtest_results/quality-strategy-v1.csv\",\n",
    "    benchmark_symbol=\"SPY\",\n",
    "    show_plots=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom-analysis-header",
   "metadata": {},
   "source": [
    "## 9. Custom Analysis\n",
    "\n",
    "Load results and perform custom analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results with full details\n",
    "results = analyze_results(\n",
    "    \"./backtest_results/quality-strategy-v1.csv\",\n",
    "    show_plots=False,\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "# Access components\n",
    "returns = results['returns']\n",
    "perf = results['perf']\n",
    "metrics = results['metrics']\n",
    "\n",
    "# Custom analysis: Rolling Sharpe ratio\n",
    "rolling_sharpe = (\n",
    "    returns.rolling(window=60)\n",
    "    .apply(lambda x: x.mean() / x.std() * np.sqrt(252) if x.std() > 0 else 0)\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.plot(rolling_sharpe.index, rolling_sharpe, linewidth=2)\n",
    "ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "ax.axhline(y=1, color='green', linestyle='--', alpha=0.3, label='Sharpe = 1')\n",
    "ax.axhline(y=2, color='blue', linestyle='--', alpha=0.3, label='Sharpe = 2')\n",
    "ax.set_title('60-Day Rolling Sharpe Ratio', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Sharpe Ratio')\n",
    "ax.set_xlabel('Date')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The `backtest_helpers` module provides:\n",
    "\n",
    "1. **`backtest()`** - Run backtest and save results\n",
    "   - First param: algorithm filename\n",
    "   - Second param: backtest name\n",
    "   - Last param: output filename\n",
    "   - Saves CSV, pickle, and metadata\n",
    "\n",
    "2. **`analyze_results()`** - Load and analyze saved results\n",
    "   - Generates pyfolio tearsheet\n",
    "   - Creates performance plots\n",
    "   - Calculates comprehensive metrics\n",
    "\n",
    "3. **`quick_backtest()`** - Run and analyze in one step\n",
    "   - Convenient for rapid iteration\n",
    "\n",
    "All results are saved to `./backtest_results/` by default and can be reloaded anytime."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}