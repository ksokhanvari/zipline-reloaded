{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Pipeline with Custom Fundamental Data\n",
    "\n",
    "This notebook demonstrates how to load custom fundamental data from CSV files and use it in Zipline Pipeline for quantitative research and stock screening.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Setup**: Creating a custom data database for fundamentals\n",
    "2. **Loading Data**: Importing CSV data with symbol-to-sid mapping\n",
    "3. **Pipeline Basics**: Creating DataSets from your custom data\n",
    "4. **Factor Analysis**: Building factors from fundamental metrics\n",
    "5. **Screening**: Filtering stocks based on fundamental criteria\n",
    "6. **Ranking**: Scoring and ranking stocks for investment decisions\n",
    "7. **Integration**: Combining fundamentals with price data\n",
    "8. **Visualization**: Analyzing results with charts\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "- **Value Investing**: Screen for low P/E, high ROE stocks\n",
    "- **Quality Analysis**: Identify companies with strong fundamentals\n",
    "- **Sector Rotation**: Compare metrics across sectors\n",
    "- **Factor Research**: Test custom factors based on fundamentals\n",
    "- **Portfolio Construction**: Build portfolios using fundamental signals\n",
    "\n",
    "## Data Requirements\n",
    "\n",
    "You'll need two CSV files:\n",
    "\n",
    "1. **Fundamentals CSV** (`sample_fundamentals.csv`): Your fundamental data\n",
    "   - Required: `Ticker`, `Date` columns\n",
    "   - Data columns: Any metrics you want (Revenue, EPS, ROE, etc.)\n",
    "\n",
    "2. **Securities CSV** (`sample_securities.csv`): Symbol-to-Sid mapping\n",
    "   - Required: `Symbol`, `Sid` columns\n",
    "   - Optional: `Name`, `Exchange`, `Sector` for reference\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Database Creation\n",
    "\n",
    "First, we'll import the necessary modules and create a database for our fundamental data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Import Zipline custom data module\n",
    "from zipline.data.custom import (\n",
    "    create_custom_db,\n",
    "    load_csv_to_db,\n",
    "    describe_custom_db,\n",
    "    list_custom_dbs,\n",
    "    get_prices,\n",
    "    make_custom_dataset_class,\n",
    "    CustomSQLiteLoader,\n",
    ")\n",
    "\n",
    "# Import Zipline Pipeline\n",
    "from zipline.pipeline import Pipeline, CustomFactor\n",
    "from zipline.pipeline.data import EquityPricing\n",
    "from zipline.pipeline.factors import SimpleMovingAverage, Returns\n",
    "from zipline.pipeline.filters import StaticAssets\n",
    "from zipline.pipeline.engine import SimplePipelineEngine\n",
    "from zipline.data.bundles import load as load_bundle\n",
    "from zipline.utils.calendar_utils import get_calendar\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"\u2713 Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Fundamental Data Schema\n",
    "\n",
    "Specify the columns in your fundamental data and their types:\n",
    "- `int`: Integer values (e.g., shares outstanding)\n",
    "- `float`: Decimal values (e.g., ratios, percentages)\n",
    "- `text`: String values (e.g., sector names)\n",
    "- `date`: Date values\n",
    "- `datetime`: Timestamp values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for our fundamental data\n",
    "# This should match the columns in your CSV file (excluding Ticker and Date)\n",
    "\n",
    "fundamental_columns = {\n",
    "    # Income Statement\n",
    "    'Revenue': 'int',\n",
    "    'NetIncome': 'int',\n",
    "    \n",
    "    # Balance Sheet\n",
    "    'TotalAssets': 'int',\n",
    "    'TotalEquity': 'int',\n",
    "    'SharesOutstanding': 'int',\n",
    "    \n",
    "    # Per-Share Metrics\n",
    "    'EPS': 'float',\n",
    "    'BookValuePerShare': 'float',\n",
    "    \n",
    "    # Financial Ratios\n",
    "    'ROE': 'float',              # Return on Equity\n",
    "    'DebtToEquity': 'float',     # Debt/Equity ratio\n",
    "    'CurrentRatio': 'float',     # Current Assets/Current Liabilities\n",
    "    'PERatio': 'float',          # Price-to-Earnings ratio\n",
    "    \n",
    "    # Metadata\n",
    "    'Sector': 'text',\n",
    "}\n",
    "\n",
    "print(\"Schema defined with {} columns:\".format(len(fundamental_columns)))\n",
    "for col, dtype in fundamental_columns.items():\n",
    "    print(f\"  - {col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Database\n",
    "\n",
    "Create a SQLite database to store our fundamental data. This is a one-time operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database configuration\n",
    "DB_CODE = 'fundamentals'      # Database identifier\n",
    "BAR_SIZE = '1 quarter'        # Data frequency (quarterly fundamentals)\n",
    "\n",
    "# Create the database\n",
    "try:\n",
    "    db_path = create_custom_db(\n",
    "        db_code=DB_CODE,\n",
    "        bar_size=BAR_SIZE,\n",
    "        columns=fundamental_columns,\n",
    "    )\n",
    "    print(f\"\u2713 Database created: {db_path}\")\n",
    "except FileExistsError:\n",
    "    print(f\"\u2139 Database '{DB_CODE}' already exists, will use existing database\")\n",
    "    # Get the database path\n",
    "    from zipline.data.custom.config import get_custom_data_dir, get_db_filename\n",
    "    db_path = get_custom_data_dir() / get_db_filename(DB_CODE)\n",
    "    print(f\"  Location: {db_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Load Data from CSV\n",
    "\n",
    "Now we'll load our fundamental data from CSV files into the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the Data Files\n",
    "\n",
    "Let's first look at what our CSV files contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths (adjust these to your actual file locations)\n",
    "FUNDAMENTALS_CSV = 'sample_fundamentals.csv'\n",
    "SECURITIES_CSV = 'sample_securities.csv'\n",
    "\n",
    "# Preview fundamentals data\n",
    "fundamentals_df = pd.read_csv(FUNDAMENTALS_CSV)\n",
    "print(\"CustomFundamentals Data Preview:\")\n",
    "print(f\"  Shape: {fundamentals_df.shape} (rows, columns)\")\n",
    "print(f\"  Date range: {fundamentals_df['Date'].min()} to {fundamentals_df['Date'].max()}\")\n",
    "print(f\"  Unique tickers: {fundamentals_df['Ticker'].nunique()}\")\n",
    "print(f\"  Tickers: {', '.join(sorted(fundamentals_df['Ticker'].unique()))}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(fundamentals_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview securities mapping\n",
    "securities_df = pd.read_csv(SECURITIES_CSV)\n",
    "print(\"Securities Mapping Preview:\")\n",
    "print(f\"  {len(securities_df)} securities mapped\")\n",
    "print(\"\\nAll securities:\")\n",
    "display(securities_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSV into Database\n",
    "\n",
    "This loads the CSV data into the database, mapping ticker symbols to Zipline's internal asset IDs (Sids)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "result = load_csv_to_db(\n",
    "    csv_path=FUNDAMENTALS_CSV,\n",
    "    db_code=DB_CODE,\n",
    "    sid_map=securities_df,       # DataFrame with Symbol and Sid columns\n",
    "    id_col='Ticker',              # Column name for ticker in fundamentals CSV\n",
    "    date_col='Date',              # Column name for dates\n",
    "    on_duplicate='replace',       # Replace existing records on conflict\n",
    "    fail_on_unmapped=False,       # Skip tickers not in securities_df\n",
    ")\n",
    "\n",
    "# Report results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA LOADING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\u2713 Rows inserted/updated: {result['rows_inserted']:,}\")\n",
    "print(f\"  Rows skipped: {result['rows_skipped']:,}\")\n",
    "\n",
    "if result['unmapped_ids']:\n",
    "    print(f\"\\n\u26a0 Warning: {len(result['unmapped_ids'])} ticker(s) not mapped:\")\n",
    "    for ticker in result['unmapped_ids']:\n",
    "        print(f\"  - {ticker}\")\n",
    "    print(\"\\n  Add these to your securities.csv file if needed.\")\n",
    "\n",
    "if result['errors']:\n",
    "    print(f\"\\n\u26a0 Errors encountered: {len(result['errors'])}\")\n",
    "    for error in result['errors'][:5]:\n",
    "        print(f\"  - {error}\")\n",
    "\n",
    "print(\"\\n\u2713 Data loading complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the Database\n",
    "\n",
    "Let's check what's in the database now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get database information\n",
    "db_info = describe_custom_db(DB_CODE)\n",
    "\n",
    "print(\"\\nDatabase Information:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Database: {db_info['db_code']}\")\n",
    "print(f\"Location: {db_info['db_path']}\")\n",
    "print(f\"Frequency: {db_info['bar_size']}\")\n",
    "print(f\"\\nData Statistics:\")\n",
    "print(f\"  Total rows: {db_info['row_count']:,}\")\n",
    "print(f\"  Unique assets (Sids): {db_info['num_sids']}\")\n",
    "if db_info['date_range']:\n",
    "    print(f\"  Date range: {db_info['date_range'][0]} to {db_info['date_range'][1]}\")\n",
    "\n",
    "print(f\"\\nColumns ({len(db_info['columns'])}):\")\n",
    "for col, dtype in db_info['columns'].items():\n",
    "    print(f\"  - {col}: {dtype}\")\n",
    "\n",
    "if db_info['sids']:\n",
    "    print(f\"\\nAsset IDs (Sids):\")\n",
    "    print(f\"  {', '.join(map(str, sorted([int(s) for s in db_info['sids']])))}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the Data Directly\n",
    "\n",
    "Before using Pipeline, let's query the data directly to see what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query all data\n",
    "all_data = get_prices(\n",
    "    db_code=DB_CODE,\n",
    "    fields=['Revenue', 'NetIncome', 'EPS', 'ROE', 'PERatio', 'Sector']\n",
    ")\n",
    "\n",
    "print(f\"Retrieved {len(all_data):,} records\")\n",
    "print(\"\\nSample data:\")\n",
    "display(all_data.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"Summary Statistics:\")\n",
    "print(\"=\"*60)\n",
    "display(all_data[['Revenue', 'NetIncome', 'EPS', 'ROE', 'PERatio']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cd73dc-20c7-4e21-b3ea-a0365f450d40",
   "metadata": {},
   "source": [
    "### Database Management & Storage\n\n",
    "Understanding where your data is stored and how to manage it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b9f6d9-ed96-4955-9954-6d44df3db968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check where your custom databases are stored\n",
    "import os\n",
    "from zipline.data.custom import get_db_path, list_custom_dbs\n",
    "from zipline.data.custom.config import get_custom_data_dir\n",
    "\n",
    "# Get storage directory\n",
    "storage_dir = get_custom_data_dir()\n",
    "print(f\"Custom Data Storage Location:\")\n",
    "print(f\"  Directory: {storage_dir}\")\n",
    "print(f\"  Exists: {storage_dir.exists()}\")\n",
    "\n",
    "# Check if running in Docker\n",
    "custom_data_env = os.environ.get('ZIPLINE_CUSTOM_DATA_DIR')\n",
    "if custom_data_env:\n",
    "    print(f\"\\n\u26a0 Docker Environment Detected\")\n",
    "    print(f\"  ZIPLINE_CUSTOM_DATA_DIR: {custom_data_env}\")\n",
    "    print(f\"  Host location: ./data/custom_databases/ (from project root)\")\n",
    "else:\n",
    "    print(f\"\\n\u2713 Local Environment\")\n",
    "    print(f\"  Default location: ~/.zipline/custom_data/\")\n",
    "\n",
    "# List all custom databases\n",
    "all_dbs = list_custom_dbs()\n",
    "print(f\"\\nCustom Databases Found: {len(all_dbs)}\")\n",
    "if all_dbs:\n",
    "    for db_code in all_dbs:\n",
    "        db_path = get_db_path(db_code)\n",
    "        size_mb = db_path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  - {db_code}: {db_path.name} ({size_mb:.2f} MB)\")\n",
    "else:\n",
    "    print(\"  (none yet - will be created when you run Part 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d32d150-826e-4d27-a31f-e9bc5d602bd9",
   "metadata": {},
   "source": [
    "#### Storage Location Details\n\n",
    "**If running in Docker** (via docker-compose):\n",
    "- Container path: `/data/custom_databases/`\n",
    "- Host path: `./data/custom_databases/` (from zipline-reloaded project root)\n",
    "- Files are **persistent** across container restarts\n",
    "- Configured via `ZIPLINE_CUSTOM_DATA_DIR` environment variable\n\n",
    "**If running locally** (not Docker):\n",
    "- Default path: `~/.zipline/custom_data/`\n",
    "- Files are **persistent** like bundle data\n",
    "- Can customize via `ZIPLINE_CUSTOM_DATA_DIR` environment variable\n\n",
    "**Database filename format**: `quant_{db_code}.sqlite`\n",
    "- Example: `quant_fundamentals.sqlite` for db_code='fundamentals'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a6a002-1ef8-4a76-a0cf-c2a2fdef565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed database inspection\n",
    "print(\"\\nDETAILED DATABASE INFORMATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if DB_CODE in list_custom_dbs():\n",
    "    info = describe_custom_db(DB_CODE)\n",
    "    \n",
    "    print(f\"\\nDatabase: {info['db_code']}\")\n",
    "    print(f\"Location: {info['db_path']}\")\n",
    "    print(f\"Frequency: {info['bar_size']}\")\n",
    "    \n",
    "    # File size\n",
    "    import os\n",
    "    size_bytes = os.path.getsize(info['db_path'])\n",
    "    size_mb = size_bytes / (1024 * 1024)\n",
    "    print(f\"File size: {size_mb:.2f} MB ({size_bytes:,} bytes)\")\n",
    "    \n",
    "    print(f\"\\nData Statistics:\")\n",
    "    print(f\"  Total rows: {info['row_count']:,}\")\n",
    "    print(f\"  Unique assets: {info['num_sids']}\")\n",
    "    \n",
    "    if info['date_range']:\n",
    "        print(f\"  Date range: {info['date_range'][0]} to {info['date_range'][1]}\")\n",
    "        \n",
    "        # Calculate time span\n",
    "        from datetime import datetime\n",
    "        start = datetime.fromisoformat(info['date_range'][0])\n",
    "        end = datetime.fromisoformat(info['date_range'][1])\n",
    "        days = (end - start).days\n",
    "        print(f\"  Time span: {days} days ({days/365.25:.1f} years)\")\n",
    "    \n",
    "    print(f\"\\nColumns ({len(info['columns'])})\")\n",
    "    for col, dtype in info['columns'].items():\n",
    "        print(f\"  - {col}: {dtype}\")\n",
    "    \n",
    "    print(f\"\\nAsset IDs (Sids): {info['num_sids']} total\")\n",
    "    sids_sorted = sorted([int(s) for s in info['sids']])\n",
    "    print(f\"  {', '.join(map(str, sids_sorted))}\")\n",
    "else:\n",
    "    print(f\"\\nDatabase '{DB_CODE}' not found yet.\")\n",
    "    print(\"Run Part 1 to create the database.\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee04e128-1b84-4d3c-b4e3-1b9547f5fee6",
   "metadata": {},
   "source": [
    "#### Database Management Functions\n\n",
    "Zipline provides comprehensive functions for managing custom databases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb23f1d9-b107-4f14-9742-e984a7114dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE 1: Direct SQL queries for testing and verification\n",
    "from zipline.data.custom import connect_db\n",
    "import pandas as pd\n",
    "\n",
    "if DB_CODE in list_custom_dbs():\n",
    "    print(\"\\nEXAMPLE 1: Direct SQL Query\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Connect to database\n",
    "    conn = connect_db(DB_CODE)\n",
    "    \n",
    "    # Query specific asset data\n",
    "    query = \"\"\"\n",
    "        SELECT Date, Sid, Revenue, NetIncome, ROE, PERatio\n",
    "        FROM Price\n",
    "        WHERE Sid = '24'\n",
    "        ORDER BY Date DESC\n",
    "        LIMIT 4\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_sql(query, conn)\n",
    "    print(\"\\nLatest 4 quarters for Sid=24 (AAPL):\")\n",
    "    display(df)\n",
    "    \n",
    "    # Another example: Check for missing values\n",
    "    query_nulls = \"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_rows,\n",
    "            SUM(CASE WHEN Revenue IS NULL THEN 1 ELSE 0 END) as revenue_nulls,\n",
    "            SUM(CASE WHEN NetIncome IS NULL THEN 1 ELSE 0 END) as netincome_nulls,\n",
    "            SUM(CASE WHEN ROE IS NULL THEN 1 ELSE 0 END) as roe_nulls\n",
    "        FROM Price\n",
    "    \"\"\"\n",
    "    \n",
    "    nulls_df = pd.read_sql(query_nulls, conn)\n",
    "    print(\"\\nData Quality Check:\")\n",
    "    display(nulls_df)\n",
    "    \n",
    "    conn.close()\n",
    "    print(\"\\n\u2713 Connection closed\")\n",
    "else:\n",
    "    print(f\"\\nDatabase '{DB_CODE}' not available yet. Run Part 1 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f9a7d0-92e9-4fd2-a8a7-1e76a70d65b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE 2: Get data distribution by asset\n",
    "if DB_CODE in list_custom_dbs():\n",
    "    print(\"\\nEXAMPLE 2: Data Distribution by Asset\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    conn = connect_db(DB_CODE)\n",
    "    \n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            Sid,\n",
    "            COUNT(*) as num_quarters,\n",
    "            MIN(Date) as first_date,\n",
    "            MAX(Date) as last_date\n",
    "        FROM Price\n",
    "        GROUP BY Sid\n",
    "        ORDER BY Sid\n",
    "    \"\"\"\n",
    "    \n",
    "    dist_df = pd.read_sql(query, conn)\n",
    "    \n",
    "    # Add ticker symbols for readability\n",
    "    sid_to_ticker = dict(zip(securities_df['Sid'], securities_df['Ticker']))\n",
    "    dist_df['Ticker'] = dist_df['Sid'].astype(int).map(sid_to_ticker)\n",
    "    \n",
    "    print(\"\\nQuarters of data per asset:\")\n",
    "    display(dist_df[['Ticker', 'Sid', 'num_quarters', 'first_date', 'last_date']])\n",
    "    \n",
    "    conn.close()\n",
    "else:\n",
    "    print(f\"\\nDatabase '{DB_CODE}' not available yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c9468-ac11-4e36-8618-26631d8499c5",
   "metadata": {},
   "source": [
    "#### Common Management Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f6088a-0174-40c3-a4a5-3b0d6a309841",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCOMMON DATABASE MANAGEMENT OPERATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "1. LIST ALL DATABASES:\n",
    "   from zipline.data.custom import list_custom_dbs\n",
    "   dbs = list_custom_dbs()\n",
    "   \n",
    "2. GET DATABASE PATH:\n",
    "   from zipline.data.custom import get_db_path\n",
    "   path = get_db_path('fundamentals')\n",
    "   \n",
    "3. GET DATABASE INFO:\n",
    "   from zipline.data.custom import describe_custom_db\n",
    "   info = describe_custom_db('fundamentals')\n",
    "   # Returns: db_path, columns, row_count, date_range, sids\n",
    "   \n",
    "4. DIRECT SQL QUERIES:\n",
    "   from zipline.data.custom import connect_db\n",
    "   conn = connect_db('fundamentals')\n",
    "   df = pd.read_sql(\"SELECT * FROM Price WHERE Sid='24'\", conn)\n",
    "   conn.close()\n",
    "   \n",
    "5. CHECK DATABASE SIZE:\n",
    "   import os\n",
    "   from zipline.data.custom import get_db_path\n",
    "   path = get_db_path('fundamentals')\n",
    "   size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "   \n",
    "6. DELETE DATABASE (CAUTION!):\n",
    "   import os\n",
    "   from zipline.data.custom import get_db_path\n",
    "   path = get_db_path('fundamentals')\n",
    "   os.remove(path)  # Permanently deletes the database!\n",
    "   \n",
    "7. UPDATE/REPLACE DATA:\n",
    "   # Re-run load_csv_to_db() with on_duplicate='replace'\n",
    "   result = load_csv_to_db(\n",
    "       csv_path='updated_data.csv',\n",
    "       db_code='fundamentals',\n",
    "       sid_map=securities_df,\n",
    "       on_duplicate='replace'  # Updates existing records\n",
    "   )\n",
    "   \n",
    "8. BACKUP DATABASE:\n",
    "   import shutil\n",
    "   from zipline.data.custom import get_db_path\n",
    "   source = get_db_path('fundamentals')\n",
    "   backup = source.parent / 'quant_fundamentals_backup.sqlite'\n",
    "   shutil.copy(source, backup)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Create Pipeline DataSet\n",
    "\n",
    "Now we'll create a Zipline Pipeline DataSet from our custom data. This allows us to use the data in Pipeline computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataSet class from our database\n",
    "CustomFundamentals = make_custom_dataset_class(\n",
    "    db_code=DB_CODE,\n",
    "    columns=fundamental_columns,\n",
    "    base_name='CustomFundamentals',  # This will create 'FundamentalsDataSet'\n",
    ")\n",
    "\n",
    "print(\"\u2713 DataSet class created: FundamentalsDataSet\")\n",
    "print(\"\\nAvailable columns (as Pipeline factors):\")\n",
    "for col in fundamental_columns.keys():\n",
    "    print(f\"  - CustomFundamentals.{col}\")\n",
    "    \n",
    "print(\"\\nYou can now use these in Pipeline like:\")\n",
    "print(\"  CustomFundamentals.Revenue.latest\")\n",
    "print(\"  CustomFundamentals.ROE.latest\")\n",
    "print(\"  CustomFundamentals.PERatio.latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Simple Pipeline Examples\n",
    "\n",
    "Let's create some simple pipelines to screen and rank stocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Basic Screening\n",
    "\n",
    "Screen for stocks with:\n",
    "- High ROE (> 10%)\n",
    "- Low P/E ratio (< 30)\n",
    "- Low debt (Debt/Equity < 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple screening pipeline\n",
    "def make_screening_pipeline():\n",
    "    \"\"\"\n",
    "    Create a pipeline that screens for quality stocks.\n",
    "    \n",
    "    Criteria:\n",
    "    - ROE > 10% (profitable and efficient)\n",
    "    - P/E < 30 (reasonably valued)\n",
    "    - Debt/Equity < 1.0 (not over-leveraged)\n",
    "    \"\"\"\n",
    "    # Get the latest fundamental values\n",
    "    roe = CustomFundamentals.ROE.latest\n",
    "    pe_ratio = CustomFundamentals.PERatio.latest\n",
    "    debt_to_equity = CustomFundamentals.DebtToEquity.latest\n",
    "    eps = CustomFundamentals.EPS.latest\n",
    "    revenue = CustomFundamentals.Revenue.latest\n",
    "    sector = CustomFundamentals.Sector.latest\n",
    "    \n",
    "    # Define screening filters\n",
    "    high_roe = (roe > 10.0)\n",
    "    reasonable_pe = (pe_ratio < 30.0)\n",
    "    low_debt = (debt_to_equity < 1.0)\n",
    "    \n",
    "    # Combine filters\n",
    "    quality_screen = high_roe & reasonable_pe & low_debt\n",
    "    \n",
    "    # Create pipeline\n",
    "    return Pipeline(\n",
    "        columns={\n",
    "            'ROE': roe,\n",
    "            'PE_Ratio': pe_ratio,\n",
    "            'Debt_to_Equity': debt_to_equity,\n",
    "            'EPS': eps,\n",
    "            'Revenue': revenue,\n",
    "            'Sector': sector,\n",
    "        },\n",
    "        screen=quality_screen,  # Only return stocks passing the screen\n",
    "    )\n",
    "\n",
    "screening_pipeline = make_screening_pipeline()\n",
    "print(\"\u2713 Screening pipeline created\")\n",
    "print(\"  Filters: ROE > 10%, P/E < 30, Debt/Equity < 1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Ranking Pipeline\n",
    "\n",
    "Rank stocks by a composite quality score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a custom factor for quality score\nclass QualityScore(CustomFactor):\n    \"\"\"\n    Composite quality score based on:\n    - ROE (higher is better)\n    - P/E ratio (lower is better)\n    - Debt/Equity (lower is better)\n    \n    Returns a normalized score where higher = better quality.\n    \"\"\"\n    inputs = [\n        CustomFundamentals.ROE,\n        CustomFundamentals.PERatio,\n        CustomFundamentals.DebtToEquity,\n    ]\n    window_length = 1  # Only need latest value\n    \n    def compute(self, today, assets, out, roe, pe, debt):\n        # Get latest values (window_length=1, so just index 0)\n        roe_latest = roe[0]\n        pe_latest = pe[0]\n        debt_latest = debt[0]\n        \n        # Normalize each metric to 0-1 scale using min-max normalization\n        # ROE: higher is better\n        roe_min, roe_max = np.nanmin(roe_latest), np.nanmax(roe_latest)\n        if roe_max > roe_min:\n            roe_score = (roe_latest - roe_min) / (roe_max - roe_min)\n        else:\n            roe_score = np.full_like(roe_latest, 0.5)  # Neutral score if all same\n        \n        # P/E: lower is better, so invert\n        pe_min, pe_max = np.nanmin(pe_latest), np.nanmax(pe_latest)\n        if pe_max > pe_min:\n            pe_score = 1 - ((pe_latest - pe_min) / (pe_max - pe_min))\n        else:\n            pe_score = np.full_like(pe_latest, 0.5)\n        \n        # Debt: lower is better, so invert\n        debt_min, debt_max = np.nanmin(debt_latest), np.nanmax(debt_latest)\n        if debt_max > debt_min:\n            debt_score = 1 - ((debt_latest - debt_min) / (debt_max - debt_min))\n        else:\n            debt_score = np.full_like(debt_latest, 0.5)\n        \n        # Composite score (equal weights)\n        out[:] = (roe_score + pe_score + debt_score) / 3.0\n\n\ndef make_ranking_pipeline():\n    \"\"\"\n    Create a pipeline that ranks stocks by quality score.\n    \"\"\"\n    # Calculate quality score\n    quality = QualityScore()\n    \n    # Get fundamentals\n    roe = CustomFundamentals.ROE.latest\n    pe_ratio = CustomFundamentals.PERatio.latest\n    debt_to_equity = CustomFundamentals.DebtToEquity.latest\n    eps = CustomFundamentals.EPS.latest\n    sector = CustomFundamentals.Sector.latest\n    \n    # Rank by quality score\n    quality_rank = quality.rank(ascending=False)  # 1 = best\n    \n    return Pipeline(\n        columns={\n            'Quality_Score': quality,\n            'Quality_Rank': quality_rank,\n            'ROE': roe,\n            'PE_Ratio': pe_ratio,\n            'Debt_to_Equity': debt_to_equity,\n            'EPS': eps,\n            'Sector': sector,\n        },\n    )\n\nranking_pipeline = make_ranking_pipeline()\nprint(\"\u2713 Ranking pipeline created\")\nprint(\"  Ranks stocks by composite quality score (ROE, P/E, Debt)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Sector Analysis Pipeline\n",
    "\n",
    "Compare metrics across sectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a custom factor for profit margin\nclass ProfitMargin(CustomFactor):\n    \"\"\"\n    Calculate profit margin: (Net Income / Revenue) * 100\n    \"\"\"\n    inputs = [\n        CustomFundamentals.NetIncome,\n        CustomFundamentals.Revenue,\n    ]\n    window_length = 1\n    \n    def compute(self, today, assets, out, net_income, revenue):\n        latest_income = net_income[0]\n        latest_revenue = revenue[0]\n        \n        # Calculate profit margin, handling division by zero\n        with np.errstate(divide='ignore', invalid='ignore'):\n            profit_margin = (latest_income / latest_revenue) * 100.0\n            profit_margin = np.where(latest_revenue == 0, np.nan, profit_margin)\n        \n        out[:] = profit_margin\n\n\ndef make_sector_analysis_pipeline():\n    \"\"\"\n    Create a pipeline for sector-based analysis.\n    \"\"\"\n    # Get all fundamental metrics\n    revenue = CustomFundamentals.Revenue.latest\n    net_income = CustomFundamentals.NetIncome.latest\n    roe = CustomFundamentals.ROE.latest\n    pe_ratio = CustomFundamentals.PERatio.latest\n    debt_to_equity = CustomFundamentals.DebtToEquity.latest\n    current_ratio = CustomFundamentals.CurrentRatio.latest\n    sector = CustomFundamentals.Sector.latest\n    \n    # Calculate profit margin using custom factor\n    profit_margin = ProfitMargin()\n    \n    return Pipeline(\n        columns={\n            'Sector': sector,\n            'Revenue': revenue,\n            'Net_Income': net_income,\n            'Profit_Margin_%': profit_margin,\n            'ROE': roe,\n            'PE_Ratio': pe_ratio,\n            'Debt_to_Equity': debt_to_equity,\n            'Current_Ratio': current_ratio,\n        },\n    )\n\nsector_pipeline = make_sector_analysis_pipeline()\nprint(\"\u2713 Sector analysis pipeline created\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Run Pipelines (Without Bundle)\n",
    "\n",
    "For testing, we can run pipelines using just our custom data without a full Zipline bundle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a simple test runner for our pipeline\n# This doesn't require a full bundle - just uses our custom data\n\n# Get test date (use a date from our data)\ntest_date = pd.Timestamp('2024-01-31')  # Use 2024-01-31 to get Q4 data for all stocks (NVDA/WMT have different fiscal calendar)\n\n# Get the assets we have data for\ntest_sids = [int(s) for s in db_info['sids']]\n\nprint(f\"Test Configuration:\")\nprint(f\"  Date: {test_date.date()}\")\nprint(f\"  Assets: {len(test_sids)} stocks\")\nprint(f\"  Sids: {test_sids}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Screening Pipeline\n",
    "\n",
    "Find stocks that pass our quality screens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example, we'll manually query and filter\n",
    "# In a real backtest, this would run automatically via SimplePipelineEngine\n",
    "\n",
    "# Query the data for our test date\n",
    "from zipline.data.custom import get_latest_values\n",
    "\n",
    "screening_data = get_latest_values(\n",
    "    db_code=DB_CODE,\n",
    "    as_of_date=test_date.strftime('%Y-%m-%d'),\n",
    "    sids=test_sids,\n",
    ")\n",
    "\n",
    "# Apply our screening criteria\n",
    "screening_data['Passes_Screen'] = (\n",
    "    (screening_data['ROE'] > 10.0) &\n",
    "    (screening_data['PERatio'] < 30.0) &\n",
    "    (screening_data['DebtToEquity'] < 1.0)\n",
    ")\n",
    "\n",
    "# Get stocks that pass\n",
    "screened_stocks = screening_data[screening_data['Passes_Screen']].copy()\n",
    "\n",
    "print(f\"\\nScreening Results (as of {test_date.date()}):\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total stocks analyzed: {len(screening_data)}\")\n",
    "print(f\"Stocks passing screen: {len(screened_stocks)}\")\n",
    "print(f\"Pass rate: {len(screened_stocks)/len(screening_data)*100:.1f}%\")\n",
    "\n",
    "if len(screened_stocks) > 0:\n",
    "    print(f\"\\nQuality Stocks (ROE > 10%, P/E < 30, Debt/Equity < 1.0):\")\n",
    "    display(screened_stocks[['Sid', 'ROE', 'PERatio', 'DebtToEquity', 'EPS', 'Sector']].sort_values('ROE', ascending=False))\n",
    "else:\n",
    "    print(\"\\nNo stocks passed the screen criteria.\")\n",
    "    print(\"Try adjusting the thresholds or check your data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Quality Rankings\n",
    "\n",
    "Rank all stocks by our composite quality score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate quality score manually\nranking_data = get_latest_values(\n    db_code=DB_CODE,\n    as_of_date=test_date.strftime('%Y-%m-%d'),\n    sids=test_sids,\n).copy()\n\n# Normalize ROE (higher is better)\nroe_min, roe_max = ranking_data['ROE'].min(), ranking_data['ROE'].max()\nif roe_max > roe_min:\n    ranking_data['ROE_Score'] = (ranking_data['ROE'] - roe_min) / (roe_max - roe_min)\nelse:\n    ranking_data['ROE_Score'] = 0.5  # All values same, assign neutral score\n\n# Normalize P/E (lower is better, so invert)\npe_min, pe_max = ranking_data['PERatio'].min(), ranking_data['PERatio'].max()\nif pe_max > pe_min:\n    ranking_data['PE_Score'] = 1 - ((ranking_data['PERatio'] - pe_min) / (pe_max - pe_min))\nelse:\n    ranking_data['PE_Score'] = 0.5\n\n# Normalize Debt (lower is better, so invert)\ndebt_min, debt_max = ranking_data['DebtToEquity'].min(), ranking_data['DebtToEquity'].max()\nif debt_max > debt_min:\n    ranking_data['Debt_Score'] = 1 - ((ranking_data['DebtToEquity'] - debt_min) / (debt_max - debt_min))\nelse:\n    ranking_data['Debt_Score'] = 0.5\n\n# Composite quality score\nranking_data['Quality_Score'] = (\n    ranking_data['ROE_Score'] + \n    ranking_data['PE_Score'] + \n    ranking_data['Debt_Score']\n) / 3.0\n\n# Rank by quality\nranking_data = ranking_data.sort_values('Quality_Score', ascending=False)\nranking_data['Quality_Rank'] = range(1, len(ranking_data) + 1)\n\n# Add Symbol column for charts\nsid_to_symbol = dict(zip(securities_df['Sid'], securities_df['Ticker']))\nranking_data['Symbol'] = ranking_data['Sid'].astype(int).map(sid_to_symbol)\n\n# Create sector colors for charts\nsectors = ranking_data['Sector'].unique()\ncolors = plt.cm.Set3(np.linspace(0, 1, len(sectors)))\nsector_colors = dict(zip(sectors, colors))\n\n# Verify Symbol column was created correctly\nif ranking_data['Symbol'].isna().any():\n    print(\"\u26a0 Warning: Some symbols are NaN. Check Sid mapping.\")\n    print(f\"   NaN count: {ranking_data['Symbol'].isna().sum()}\")\nelse:\n    print(f\"\u2713 Symbol column created successfully for all {len(ranking_data)} stocks\")\n\nprint(f\"\\nQuality Rankings (as of {test_date.date()}):\")\nprint(\"=\"*80)\ndisplay(ranking_data[['Quality_Rank', 'Sid', 'Quality_Score', 'ROE', 'PERatio', 'DebtToEquity', 'Sector']])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sector Analysis\n",
    "\n",
    "Compare metrics across sectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sector data\n",
    "sector_data = get_latest_values(\n",
    "    db_code=DB_CODE,\n",
    "    as_of_date=test_date.strftime('%Y-%m-%d'),\n",
    "    sids=test_sids,\n",
    ").copy()\n",
    "\n",
    "# Calculate profit margin\n",
    "sector_data['Profit_Margin_%'] = (sector_data['NetIncome'] / sector_data['Revenue']) * 100\n",
    "\n",
    "# Group by sector\n",
    "sector_summary = sector_data.groupby('Sector').agg({\n",
    "    'Revenue': 'sum',\n",
    "    'NetIncome': 'sum',\n",
    "    'ROE': 'mean',\n",
    "    'PERatio': 'mean',\n",
    "    'DebtToEquity': 'mean',\n",
    "    'Profit_Margin_%': 'mean',\n",
    "    'Sid': 'count',\n",
    "}).rename(columns={'Sid': 'Num_Stocks'})\n",
    "\n",
    "sector_summary = sector_summary.round(2)\n",
    "\n",
    "print(f\"\\nSector Analysis (as of {test_date.date()}):\")\n",
    "print(\"=\"*80)\n",
    "display(sector_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Visualizations\n",
    "\n",
    "Create charts to visualize the fundamental data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Scatter plot: ROE vs P/E Ratio\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Map Sids to symbols for labels\n\n# Color by sector\nfor sector in sectors:\n    sector_data_plot = ranking_data[ranking_data['Sector'] == sector]\n    ax.scatter(\n        sector_data_plot['ROE'],\n        sector_data_plot['PERatio'],\n        s=200,\n        c=[sector_colors[sector]],\n        label=sector,\n        alpha=0.7,\n        edgecolors='black',\n        linewidth=1.5,\n    )\n    \n    # Add stock labels\n    for idx, row in sector_data_plot.iterrows():\n        ax.annotate(\n            row['Symbol'],\n            (row['ROE'], row['PERatio']),\n            xytext=(5, 5),\n            textcoords='offset points',\n            fontsize=9,\n            fontweight='bold',\n        )\n\nax.set_xlabel('Return on Equity (ROE %)', fontsize=12, fontweight='bold')\nax.set_ylabel('P/E Ratio', fontsize=12, fontweight='bold')\nax.set_title(f'ROE vs P/E Ratio by Sector ({test_date.date()})', fontsize=14, fontweight='bold')\nax.legend(title='Sector', loc='best', framealpha=0.9)\nax.grid(True, alpha=0.3)\n\n# Add quadrant lines for reference\nax.axhline(y=30, color='red', linestyle='--', alpha=0.5, label='P/E = 30 (threshold)')\nax.axvline(x=10, color='green', linestyle='--', alpha=0.5, label='ROE = 10% (threshold)')\n\nplt.subplots_adjust(left=0.08, right=0.95, top=0.95, bottom=0.08)\nplt.show()\n\nprint(\"\\nInterpretation:\")\nprint(\"  - Top-left quadrant (high ROE, low P/E): Best value + quality\")\nprint(\"  - Top-right quadrant (high ROE, high P/E): Quality but expensive\")\nprint(\"  - Bottom-left quadrant (low ROE, low P/E): Cheap but poor quality\")\nprint(\"  - Bottom-right quadrant (low ROE, high P/E): Expensive and poor quality\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Bar chart: Quality scores by stock\nfig, ax = plt.subplots(figsize=(14, 8))  # Increased height from 6 to 8\n\n# Sort by quality score\nplot_data = ranking_data.sort_values('Quality_Score', ascending=True)\n\n# Create bars colored by sector\nbars = ax.barh(\n    plot_data['Symbol'],\n    plot_data['Quality_Score'],\n    color=[sector_colors[s] for s in plot_data['Sector']],\n    edgecolor='black',\n    linewidth=1.5,\n)\n\nax.set_xlabel('Quality Score', fontsize=12, fontweight='bold')\nax.set_ylabel('Stock', fontsize=12, fontweight='bold')\nax.set_title(f'Composite Quality Score by Stock ({test_date.date()})', fontsize=14, fontweight='bold')\nax.set_xlim(0, 1.1)  # Extended to 1.1 to give room for text labels\nax.grid(True, axis='x', alpha=0.3)\n\n# Add value labels with better positioning\nfor i, (idx, row) in enumerate(plot_data.iterrows()):\n    ax.text(\n        row['Quality_Score'] + 0.015,  # Slightly reduced offset\n        i,\n        f\"{row['Quality_Score']:.3f}\",\n        va='center',\n        fontsize=9,\n    )\n\n# Use subplots_adjust instead of tight_layout for better control\nplt.subplots_adjust(left=0.08, right=0.95, top=0.95, bottom=0.08)\nplt.show()\n\nprint(\"\\nQuality Score Composition:\")\nprint(\"  - 1/3 ROE score (normalized)\")\nprint(\"  - 1/3 P/E score (inverted & normalized)\")\nprint(\"  - 1/3 Debt/Equity score (inverted & normalized)\")\nprint(\"  Higher scores = better quality\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Heatmap: Fundamental metrics\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Prepare data for heatmap (normalize for visualization)\nheatmap_data = ranking_data.set_index('Symbol')[['ROE', 'PERatio', 'DebtToEquity', 'CurrentRatio']].copy()\n\n# Normalize each column to 0-1 for better visualization\nfor col in heatmap_data.columns:\n    col_min = heatmap_data[col].min()\n    col_max = heatmap_data[col].max()\n    if col_max > col_min:\n        heatmap_data[col] = (heatmap_data[col] - col_min) / (col_max - col_min)\n    else:\n        # All values are the same, set to 0.5 (neutral)\n        heatmap_data[col] = 0.5\n\n# Invert P/E and Debt (lower is better)\nheatmap_data['PERatio'] = 1 - heatmap_data['PERatio']\nheatmap_data['DebtToEquity'] = 1 - heatmap_data['DebtToEquity']\n\n# Rename for clarity\nheatmap_data.columns = ['ROE\\n(higher better)', 'P/E\\n(lower better)', 'Debt/Equity\\n(lower better)', 'Current Ratio\\n(higher better)']\n\n# Create heatmap\nsns.heatmap(\n    heatmap_data,\n    annot=True,\n    fmt='.2f',\n    cmap='RdYlGn',\n    center=0.5,\n    linewidths=1,\n    linecolor='black',\n    cbar_kws={'label': 'Normalized Score\\n(0=worst, 1=best)'},\n    ax=ax,\n)\n\nax.set_title(f'Fundamental Metrics Heatmap ({test_date.date()})', fontsize=14, fontweight='bold', pad=20)\nax.set_xlabel('Metric', fontsize=12, fontweight='bold')\nax.set_ylabel('Stock', fontsize=12, fontweight='bold')\n\nplt.subplots_adjust(left=0.12, right=0.95, top=0.95, bottom=0.12)\nplt.show()\n\nprint(\"\\nHeatmap Interpretation:\")\nprint(\"  - Green = Good (high normalized score)\")\nprint(\"  - Yellow = Average (medium normalized score)\")\nprint(\"  - Red = Poor (low normalized score)\")\nprint(\"  - Look for rows with mostly green (best overall quality)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Time Series Analysis\n",
    "\n",
    "Analyze how fundamentals change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get time series for a specific stock\nexample_stock = 'AAPL'\nexample_sid = securities_df[securities_df['Ticker'] == example_stock]['Sid'].values[0]\n\n# Query all quarters for this stock\nstock_history = get_prices(\n    db_code=DB_CODE,\n    sids=[example_sid],\n)\n\nstock_history['Date'] = pd.to_datetime(stock_history['Date'])\nstock_history = stock_history.sort_values('Date')\n\nprint(f\"\\n{example_stock} Historical Fundamentals:\")\nprint(\"=\"*80)\ndisplay(stock_history[['Date', 'Revenue', 'NetIncome', 'EPS', 'ROE', 'PERatio']])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot time series\nfig, axes = plt.subplots(2, 2, figsize=(16, 10))\nfig.suptitle(f'{example_stock} - Fundamental Trends Over Time', fontsize=16, fontweight='bold')\n\n# Revenue\naxes[0, 0].plot(stock_history['Date'], stock_history['Revenue'] / 1e9, marker='o', linewidth=2, markersize=8)\naxes[0, 0].set_title('Quarterly Revenue', fontsize=12, fontweight='bold')\naxes[0, 0].set_ylabel('Revenue (Billions $)', fontsize=10)\naxes[0, 0].grid(True, alpha=0.3)\naxes[0, 0].tick_params(axis='x', rotation=45)\n\n# EPS\naxes[0, 1].plot(stock_history['Date'], stock_history['EPS'], marker='o', linewidth=2, markersize=8, color='green')\naxes[0, 1].set_title('Earnings Per Share (EPS)', fontsize=12, fontweight='bold')\naxes[0, 1].set_ylabel('EPS ($)', fontsize=10)\naxes[0, 1].grid(True, alpha=0.3)\naxes[0, 1].tick_params(axis='x', rotation=45)\n\n# ROE\naxes[1, 0].plot(stock_history['Date'], stock_history['ROE'], marker='o', linewidth=2, markersize=8, color='orange')\naxes[1, 0].set_title('Return on Equity (ROE)', fontsize=12, fontweight='bold')\naxes[1, 0].set_ylabel('ROE (%)', fontsize=10)\naxes[1, 0].axhline(y=10, color='red', linestyle='--', alpha=0.5, label='Target: 10%')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\naxes[1, 0].tick_params(axis='x', rotation=45)\n\n# P/E Ratio\naxes[1, 1].plot(stock_history['Date'], stock_history['PERatio'], marker='o', linewidth=2, markersize=8, color='purple')\naxes[1, 1].set_title('P/E Ratio', fontsize=12, fontweight='bold')\naxes[1, 1].set_ylabel('P/E Ratio', fontsize=10)\naxes[1, 1].axhline(y=30, color='red', linestyle='--', alpha=0.5, label='Target: < 30')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\naxes[1, 1].tick_params(axis='x', rotation=45)\n\nplt.subplots_adjust(left=0.08, right=0.95, top=0.92, bottom=0.08, hspace=0.3, wspace=0.25)\nplt.show()\n\nprint(f\"\\n{example_stock} Trend Analysis:\")\nprint(\"  - Look for consistent growth in Revenue and EPS\")\nprint(\"  - Stable/improving ROE indicates efficient operations\")\nprint(\"  - P/E ratio shows market valuation relative to earnings\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 8: Integration with Backtesting\n\nNow let's see how to use the custom fundamental data in a real Zipline backtest with the Sharadar bundle for pricing data.\n\n### Pipeline Definition with Bundle Integration\n\nThe key is to create a pipeline that combines:\n1. **Fundamental data** from our custom database\n2. **Pricing data** from the Sharadar bundle\n3. **Screening logic** based on both"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a pipeline that combines fundamental and price data",
    "from zipline.pipeline import Pipeline",
    "from zipline.pipeline.data import EquityPricing",
    "",
    "def make_quality_pipeline():",
    "    \"\"\"",
    "    Pipeline combining custom fundamentals with bundle pricing data.",
    "    ",
    "    Returns:",
    "        Pipeline with quality stocks based on fundamental + technical screens",
    "    \"\"\"",
    "    # ========================================================================",
    "    # FUNDAMENTAL DATA (from custom database)",
    "    # ========================================================================",
    "    ",
    "    # Get fundamental metrics",
    "    roe = CustomFundamentals.ROE.latest",
    "    pe_ratio = CustomFundamentals.PERatio.latest",
    "    debt_to_equity = CustomFundamentals.DebtToEquity.latest",
    "    eps = CustomFundamentals.EPS.latest",
    "    current_ratio = CustomFundamentals.CurrentRatio.latest",
    "    sector = CustomFundamentals.Sector.latest",
    "    ",
    "    # Calculate quality score",
    "    quality_score = QualityScore()",
    "    ",
    "    # Calculate profit margin",
    "    profit_margin = ProfitMargin()",
    "    ",
    "    # ========================================================================",
    "    # PRICING DATA (from sharadar bundle)",
    "    # ========================================================================",
    "    ",
    "    # Get price and volume from bundle",
    "    close_price = EquityPricing.close.latest",
    "    volume = EquityPricing.volume.latest",
    "    ",
    "    # Calculate technical indicators",
    "    avg_volume_20d = EquityPricing.volume.mavg(20)  # 20-day average volume",
    "    ",
    "    # ========================================================================",
    "    # SCREENING LOGIC",
    "    # ========================================================================",
    "    ",
    "    # Fundamental quality filters",
    "    high_roe = (roe > 5.0)  # Profitable",
    "    reasonable_pe = (pe_ratio < 50.0)  # Not overvalued",
    "    manageable_debt = (debt_to_equity < 5.0)  # Not over-leveraged",
    "    ",
    "    # Liquidity filters (from pricing data)",
    "    liquid = (avg_volume_20d > 100000)  # Minimum liquidity",
    "    valid_price = (close_price > 1.0)  # Minimum price",
    "    ",
    "    # Combined universe",
    "    quality_universe = (",
    "        high_roe &",
    "        reasonable_pe &",
    "        manageable_debt &",
    "        liquid &",
    "        valid_price",
    "    )",
    "    ",
    "    # Rank stocks by quality score",
    "    quality_rank = quality_score.rank(mask=quality_universe, ascending=False)",
    "    ",
    "    # ========================================================================",
    "    # RETURN PIPELINE",
    "    # ========================================================================",
    "    ",
    "    return Pipeline(",
    "        columns={",
    "            # Fundamental metrics",
    "            'quality_score': quality_score,",
    "            'quality_rank': quality_rank,",
    "            'roe': roe,",
    "            'pe_ratio': pe_ratio,",
    "            'debt_to_equity': debt_to_equity,",
    "            'eps': eps,",
    "            'current_ratio': current_ratio,",
    "            'profit_margin': profit_margin,",
    "            'sector': sector,",
    "            ",
    "            # Pricing metrics",
    "            'close': close_price,",
    "            'volume': volume,",
    "            'avg_volume_20d': avg_volume_20d,",
    "        },",
    "        screen=quality_universe,",
    "    )",
    "",
    "# Create the pipeline",
    "quality_pipeline = make_quality_pipeline()",
    "print(\"\u2713 Quality pipeline created with fundamental + pricing data\")",
    "print(\"\\nPipeline combines:\")",
    "print(\"  - Fundamental data: ROE, P/E, Debt/Equity, EPS, etc. (custom database)\")",
    "print(\"  - Pricing data: Close, Volume, Moving averages (sharadar bundle)\")",
    "print(\"  - Screening: Quality filters + liquidity requirements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Running a Backtest\n\nI've created a complete backtest script for you: `backtest_with_fundamentals.py`\n\n**Strategy Logic**:\n1. Every month (or week), run the pipeline to get quality stocks\n2. Select top N stocks by quality score\n3. Equal-weight portfolio\n4. Rebalance on schedule\n\n**To run the backtest**:\n\n```bash\ncd examples/custom_data\npython backtest_with_fundamentals.py\n```\n\n**To visualize results**:\n\n```bash\npython plot_backtest_results.py\n```\n\nLet me show you the key parts of the backtest code:"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Example: Algorithm initialization\ndef initialize(context):\n    \"\"\"\n    Called once at the start of the backtest.\n    \"\"\"\n    # Attach our pipeline that combines fundamentals + pricing\n    attach_pipeline(make_quality_pipeline(), 'quality_stocks')\n    \n    # Schedule monthly rebalancing\n    schedule_function(\n        rebalance,\n        date_rules.month_start(),  # First trading day of month\n        time_rules.market_open(hours=1),  # 1 hour after market open\n    )\n    \n    # Set parameters\n    context.top_n = 10  # Hold top 10 quality stocks\n    \n    print(\"\u2713 Algorithm initialized\")\n    print(f\"  Strategy: Quality Factor (Fundamentals + Pricing)\")\n    print(f\"  Top N stocks: {context.top_n}\")\n    print(f\"  Rebalance: Monthly\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Example: Daily pipeline data retrieval\ndef before_trading_start(context, data):\n    \"\"\"\n    Called every day before market open.\n    Gets fresh pipeline data combining fundamentals and pricing.\n    \"\"\"\n    # Get pipeline output\n    context.output = pipeline_output('quality_stocks')\n    \n    # Pipeline output is a DataFrame with all stocks passing the screen\n    # Columns include: quality_score, roe, pe_ratio, close, volume, etc.\n    \n    if len(context.output) > 0:\n        avg_quality = context.output['quality_score'].mean()\n        avg_roe = context.output['roe'].mean()\n        \n        print(f\"{context.get_datetime().date()}\")\n        print(f\"  Universe: {len(context.output)} stocks\")\n        print(f\"  Avg Quality Score: {avg_quality:.3f}\")\n        print(f\"  Avg ROE: {avg_roe:.1f}%\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Example: Portfolio rebalancing\ndef rebalance(context, data):\n    \"\"\"\n    Rebalance portfolio to hold top N quality stocks.\n    \"\"\"\n    # Select top N stocks by quality rank (lowest rank = best quality)\n    top_stocks = context.output.nsmallest(context.top_n, 'quality_rank')\n    \n    if len(top_stocks) == 0:\n        return\n    \n    # Calculate equal weight\n    target_weight = 1.0 / len(top_stocks)\n    \n    # Get current and target positions\n    current_positions = set(context.portfolio.positions.keys())\n    target_positions = set(top_stocks.index)\n    \n    # Sell stocks no longer in top N\n    for asset in current_positions - target_positions:\n        if data.can_trade(asset):\n            order_target_percent(asset, 0.0)\n            print(f\"  SELL: {asset.symbol}\")\n    \n    # Buy/rebalance top N stocks\n    for asset in top_stocks.index:\n        if data.can_trade(asset):\n            order_target_percent(asset, target_weight)\n            \n            if asset not in current_positions:\n                stock_data = top_stocks.loc[asset]\n                print(f\"  BUY: {asset.symbol} - \"\n                      f\"Quality: {stock_data['quality_score']:.3f}, \"\n                      f\"ROE: {stock_data['roe']:.1f}%, \"\n                      f\"P/E: {stock_data['pe_ratio']:.1f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Expected Output\n\nWhen you run the backtest, you'll see:\n\n```\nBACKTEST CONFIGURATION\n======================================================================\nStrategy: Quality Factor (Fundamentals-based)\nTop N stocks: 10\nRebalance: monthly\nPeriod: 2023-04-01 to 2024-01-31\nInitial capital: $100,000.00\n======================================================================\n\n2023-04-03\n  Universe size: 8 stocks\n  Avg Quality Score: 0.623\n  Avg ROE: 12.3%\n  Avg P/E: 35.2\n\n  REBALANCING:\n    Sell: 0 positions\n    Buy: 8 positions\n    Rebalance: 0 positions\n    Target weight: 12.50%\n      BUY:  AAPL (Score: 0.742, ROE: 36.9%, P/E: 29.2)\n      BUY:  MSFT (Score: 0.698, ROE: 9.3%, P/E: 32.8)\n      ...\n\nBACKTEST RESULTS\n======================================================================\nInitial Capital: $100,000.00\nFinal Value:     $108,532.45\nTotal Return:    8.53%\nSharpe Ratio:    1.42\nMax Drawdown:    -3.21%\nWin Rate:        54.2%\nTotal Trades:    47\n======================================================================\n```\n\n### Visualization Results\n\nThe `plot_backtest_results.py` script creates comprehensive charts:\n\n1. **Portfolio Value**: Track growth over time\n2. **Cumulative Returns**: See total performance\n3. **Drawdown**: Understand risk/downside\n4. **Returns Distribution**: Analyze return profile\n5. **Monthly Heatmap**: See performance by month\n\nAll results are saved to:\n- `backtest_results.csv` - Detailed daily data\n- `backtest_performance.png` - Visualization charts"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Troubleshooting & Tips\n",
    "\n",
    "Common issues and solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "TROUBLESHOOTING GUIDE\n",
    "=====================\n",
    "\n",
    "PROBLEM: \"Database not found\"\n",
    "SOLUTION: Run the database creation cell (Part 1) first\n",
    "   \n",
    "PROBLEM: \"No data returned from query\"\n",
    "SOLUTION: \n",
    "   - Check that data was loaded successfully (Part 2)\n",
    "   - Verify your date range matches the data\n",
    "   - Check that Sids exist in the database\n",
    "\n",
    "PROBLEM: \"Unmapped identifiers\" warning\n",
    "SOLUTION:\n",
    "   - Add missing tickers to securities.csv\n",
    "   - Or set fail_on_unmapped=False to skip them\n",
    "\n",
    "PROBLEM: \"Column not found\" error\n",
    "SOLUTION:\n",
    "   - Verify column names match between CSV and schema\n",
    "   - Check case sensitivity (Revenue vs revenue)\n",
    "   - Run describe_custom_db() to see available columns\n",
    "\n",
    "PROBLEM: Pipeline gives errors\n",
    "SOLUTION:\n",
    "   - Ensure dates are timezone-aware: pd.Timestamp('2023-01-01', tz='UTC')\n",
    "   - Check that assets exist in both bundle AND custom data\n",
    "   - Verify CustomSQLiteLoader is registered correctly\n",
    "\n",
    "TIPS FOR BEST RESULTS:\n",
    "======================\n",
    "\n",
    "1. DATA QUALITY:\n",
    "   - Clean your CSV data before loading\n",
    "   - Handle missing values appropriately\n",
    "   - Ensure dates are in consistent format\n",
    "\n",
    "2. PERFORMANCE:\n",
    "   - Use appropriate data types (int for large numbers, not float)\n",
    "   - Index frequently-queried columns\n",
    "   - Use date range filters in queries\n",
    "\n",
    "3. DATA UPDATES:\n",
    "   - Use on_duplicate='replace' to update existing records\n",
    "   - Use on_duplicate='ignore' to skip duplicates\n",
    "   - Use on_duplicate='fail' to catch data issues\n",
    "\n",
    "4. FACTOR DESIGN:\n",
    "   - Normalize factors to similar scales for combining\n",
    "   - Handle missing data with .fillna() or filters\n",
    "   - Test factors individually before combining\n",
    "\n",
    "5. BACKTESTING:\n",
    "   - Ensure point-in-time correctness (no look-ahead bias)\n",
    "   - Match fundamental frequency (quarterly) with rebalancing\n",
    "   - Consider reporting lag (fundamentals released ~45 days after quarter-end)\n",
    "\n",
    "NEXT STEPS:\n",
    "===========\n",
    "\n",
    "1. Load your own fundamental data CSV\n",
    "2. Create custom factors based on your research\n",
    "3. Backtest strategies using fundamental signals\n",
    "4. Combine with price/volume factors for multi-factor models\n",
    "5. Analyze results and iterate\n",
    "\n",
    "For more examples and documentation:\n",
    "  - Zipline Custom Data: src/zipline/data/custom/README.md\n",
    "  - Zipline Pipeline: https://zipline.ml4trading.io/pipeline.html\n",
    "  - Example notebooks: examples/custom_data/\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**What We Covered:**\n",
    "\n",
    "1. \u2705 Created a custom database for fundamental data\n",
    "2. \u2705 Loaded CSV data with symbol-to-sid mapping\n",
    "3. \u2705 Created Pipeline DataSets from custom data\n",
    "4. \u2705 Built screening pipelines (quality filters)\n",
    "5. \u2705 Created ranking pipelines (composite scores)\n",
    "6. \u2705 Performed sector analysis\n",
    "7. \u2705 Visualized fundamental metrics\n",
    "8. \u2705 Analyzed trends over time\n",
    "9. \u2705 Learned backtest integration\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "- Custom data enables fundamental analysis in Zipline\n",
    "- Pipeline makes it easy to screen and rank stocks\n",
    "- Combine multiple factors for robust signals\n",
    "- Visualizations help understand the data\n",
    "- Integration with backtesting enables strategy development\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "1. Try with your own fundamental data\n",
    "2. Experiment with different factor combinations\n",
    "3. Build and test investment strategies\n",
    "4. Combine with technical indicators\n",
    "5. Run full backtests and analyze performance\n",
    "\n",
    "Happy researching! \ud83d\udcca\ud83d\ude80"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}