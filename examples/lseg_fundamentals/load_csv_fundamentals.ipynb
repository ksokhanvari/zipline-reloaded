{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CSV Fundamental Data into Zipline Custom Database\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load fundamental data from CSV files\n",
    "2. Map symbols to Zipline SIDs\n",
    "3. Create a custom SQLite database\n",
    "4. Use the data in Zipline Pipeline\n",
    "\n",
    "This is a zipline-reloaded native approach (no QuantRocket dependencies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Registered Sharadar bundle\n",
      "\u2713 Imports complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Zipline imports\n",
    "from zipline.data.bundles import load as load_bundle, register\n",
    "from zipline.data.bundles.sharadar_bundle import sharadar_bundle\n",
    "from zipline.pipeline import Pipeline\n",
    "from zipline.pipeline.data.db import Database, Column\n",
    "\n",
    "# Register Sharadar bundle (in case extension.py didn't load)\n",
    "try:\n",
    "    # Try to register the bundle\n",
    "    register(\n",
    "        'sharadar',\n",
    "        sharadar_bundle(\n",
    "            tickers=None,\n",
    "            incremental=True,\n",
    "            include_funds=True,\n",
    "        ),\n",
    "    )\n",
    "    print(\"\u2713 Registered Sharadar bundle\")\n",
    "except Exception as e:\n",
    "    # Bundle may already be registered\n",
    "    print(f\"\u2713 Sharadar bundle already registered (or error: {e})\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', 120)\n",
    "\n",
    "print(\"\u2713 Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set your database name and data directory paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database will be created at: /data/custom_databases/fundamentals.sqlite\n",
      "Update mode: fresh\n",
      "  - 'fresh': Drop and recreate database\n",
      "  - 'replace': Update existing records with new data\n",
      "  - 'ignore': Skip records that already exist\n",
      "\n",
      "Looking for CSV files in: /data/csv/\n",
      "\n",
      "\ud83d\udca1 Tip: Place your CSV files in /data/csv/ (inside container)\n",
      "   or ./data/csv/ (on host machine) for persistent storage\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATABASE_NAME = \"fundamentals\"  # Name for your custom database\n",
    "DATA_DIR = \"/data/csv/\"  # Directory with CSV files (persistent across Docker restarts)\n",
    "VIX_SIGNAL_PATH = \"/data/csv/vix_flag.csv\"  # Optional VIX signal data\n",
    "BC_DATA_PATH = \"/data/csv/bc_data.csv\"  # Optional BC signal data\n",
    "\n",
    "\n",
    "\n",
    "# Database will be created in ZIPLINE_CUSTOM_DATA_DIR (from docker-compose.yml)\n",
    "# This ensures the database is in the same location that the loader expects\n",
    "DB_DIR = Path(os.environ.get('ZIPLINE_CUSTOM_DATA_DIR', '/data/custom_databases'))\n",
    "DB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DB_PATH = DB_DIR / f\"{DATABASE_NAME}.sqlite\"\n",
    "\n",
    "# Database update mode:\n",
    "# 'fresh' - Drop and recreate database (default)\n",
    "# 'replace' - Insert or replace existing records (updates duplicates)\n",
    "# 'ignore' - Insert or ignore (skips duplicates, keeps existing data)\n",
    "UPDATE_MODE = 'fresh'  # Change to 'fresh', 'replace', or 'ignore'\n",
    "\n",
    "print(f\"Database will be created at: {DB_PATH}\")\n",
    "print(f\"Update mode: {UPDATE_MODE}\")\n",
    "print(f\"  - 'fresh': Drop and recreate database\")\n",
    "print(f\"  - 'replace': Update existing records with new data\")\n",
    "print(f\"  - 'ignore': Skip records that already exist\")\n",
    "print(f\"\\nLooking for CSV files in: {DATA_DIR}\")\n",
    "print(f\"\\n\ud83d\udca1 Tip: Place your CSV files in /data/csv/ (inside container)\")\n",
    "print(f\"   or ./data/csv/ (on host machine) for persistent storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to find newest CSV by date in filename\n",
    "def find_newest_csv_by_date(directory, pattern='*_with_metadata*.csv'):\n",
    "    \"\"\"Find CSV with newest end date in filename (format: YYYYMMDD_YYYYMMDD_*.csv)\"\"\"\n",
    "    import glob\n",
    "    import re\n",
    "    from pathlib import Path\n",
    "    \n",
    "    csv_dir = Path(directory)\n",
    "    csv_files = list(csv_dir.glob(pattern))\n",
    "    \n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"No CSV files matching '{pattern}' found in {directory}\")\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV file(s) matching pattern '{pattern}':\")\n",
    "    \n",
    "    # Extract dates from filenames (format: YYYYMMDD_YYYYMMDD)\n",
    "    date_pattern = r'(\\d{8})_(\\d{8})'\n",
    "    files_with_dates = []\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        match = re.search(date_pattern, csv_file.name)\n",
    "        if match:\n",
    "            start_date = match.group(1)\n",
    "            end_date = match.group(2)\n",
    "            files_with_dates.append((csv_file, start_date, end_date))\n",
    "            print(f\"  {csv_file.name}: {start_date} to {end_date}\")\n",
    "        else:\n",
    "            print(f\"  {csv_file.name}: No date pattern found\")\n",
    "    \n",
    "    if not files_with_dates:\n",
    "        # No date pattern found, just use first file\n",
    "        print(f\"\\nNo date patterns found, using first file: {csv_files[0].name}\")\n",
    "        return str(csv_files[0])\n",
    "    \n",
    "    # Sort by end date (most recent last)\n",
    "    files_with_dates.sort(key=lambda x: x[2])\n",
    "    \n",
    "    newest_file = files_with_dates[-1][0]\n",
    "    newest_start = files_with_dates[-1][1]\n",
    "    newest_end = files_with_dates[-1][2]\n",
    "    \n",
    "    print(f\"\\n\u2713 Using newest file: {newest_file.name} ({newest_start} to {newest_end})\")\n",
    "    \n",
    "    return str(newest_file)\n",
    "\n",
    "print(\"\u2713 Helper function loaded: find_newest_csv_by_date()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Database Schema\n",
    "\n",
    "Define the columns that will be in your custom database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your database schema\n",
    "# This matches the columns from the QuantRocket example\n",
    "SCHEMA = {\n",
    "    'Symbol': 'TEXT',\n",
    "    'Sid': 'INTEGER',\n",
    "    'Date': 'TEXT',\n",
    "    'RefPriceClose': 'REAL',\n",
    "    'RefVolume': 'REAL',\n",
    "    'CompanyCommonName': 'TEXT',\n",
    "    'EnterpriseValue_DailyTimeSeries_': 'REAL',\n",
    "    'CompanyMarketCap': 'REAL',\n",
    "    'GICSSectorName': 'TEXT',\n",
    "    'FOCFExDividends_Discrete': 'REAL',\n",
    "    'InterestExpense_NetofCapitalizedInterest': 'REAL',\n",
    "    'Debt_Total': 'REAL',\n",
    "    'EarningsPerShare_Actual': 'REAL',\n",
    "    'EarningsPerShare_SmartEstimate_prev_Q': 'REAL',\n",
    "    'EarningsPerShare_ActualSurprise': 'REAL',\n",
    "    'EarningsPerShare_SmartEstimate_current_Q': 'REAL',\n",
    "    'LongTermGrowth_Mean': 'REAL',\n",
    "    'PriceTarget_Median': 'REAL',\n",
    "    'CombinedAlphaModelSectorRank': 'REAL',\n",
    "    'CombinedAlphaModelSectorRankChange': 'REAL',\n",
    "    'CombinedAlphaModelRegionRank': 'REAL',\n",
    "    'TradeDate': 'TEXT',\n",
    "    'EPS_SurpirsePrct_prev_Q': 'REAL',\n",
    "    'Estpricegrowth_percent': 'REAL',\n",
    "    'CashFlowComponent_Current': 'REAL',\n",
    "    'EarningsQualityRegionRank_Current': 'REAL',\n",
    "    'EnterpriseValueToEBIT_DailyTimeSeriesRatio_': 'REAL',\n",
    "    'EnterpriseValueToEBITDA_DailyTimeSeriesRatio_': 'REAL',\n",
    "    'EnterpriseValueToSales_DailyTimeSeriesRatio_': 'REAL',\n",
    "    'Dividend_Per_Share_SmartEstimate': 'REAL',\n",
    "    'CashFlowPerShare_BrokerEstimate': 'REAL',\n",
    "    'FreeCashFlowPerShare_BrokerEstimate': 'REAL',\n",
    "    'ForwardPEG_DailyTimeSeriesRatio_': 'REAL',\n",
    "    'PriceEarningsToGrowthRatio_SmartEstimate_': 'REAL',\n",
    "    'ReturnOnInvestedCapital_BrokerEstimate': 'REAL',\n",
    "    'Recommendation_NumberOfTotal': 'REAL',\n",
    "    'Recommendation_Median_1_5_': 'REAL',\n",
    "    'Recommendation_NumberOfStrongBuy': 'REAL',\n",
    "    'Recommendation_NumberOfBuy': 'REAL',\n",
    "    'Recommendation_Mean_1_5_': 'REAL',\n",
    "    'ReturnOnCapitalEmployed_Actual': 'REAL',\n",
    "    'GrossProfitMargin_': 'REAL',\n",
    "    'ReturnOnEquity_SmartEstimat': 'REAL',\n",
    "    'ReturnOnAssets_SmartEstimate': 'REAL',\n",
    "    'CashCashEquivalents_Total': 'REAL',\n",
    "    'ForwardPriceToCashFlowPerShare_DailyTimeSeriesRatio_': 'REAL',\n",
    "    'ForwardPriceToSalesPerShare_DailyTimeSeriesRatio_': 'REAL',\n",
    "    'ForwardEnterpriseValueToOperatingCashFlow_DailyTimeSeriesRatio_': 'REAL',\n",
    "    'GrossProfitMargin_ActualSurprise': 'REAL',\n",
    "    'pred': 'REAL',  # VIX signal\n",
    "    'bc1': 'REAL',  # BC signal\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward filling!\n",
      "Original columns: ['Date', 'Symbol', 'Instrument', 'RefPriceClose', 'RefVolume', 'CompanyCommonName', 'EnterpriseValue_DailyTimeSeries_', 'CompanyMarketCap', 'GICSSectorName', 'FOCFExDividends_Discrete']...\n",
      "Post ffil columns: ['Date', 'Symbol', 'Instrument', 'RefPriceClose', 'RefVolume', 'CompanyCommonName', 'EnterpriseValue_DailyTimeSeries_', 'CompanyMarketCap', 'GICSSectorName', 'FOCFExDividends_Discrete']...\n",
      "writing file back\n",
      "Saved 9010487 rows with columns: ['Date', 'Symbol', 'Instrument', 'RefPriceClose', 'RefVolume']...\n"
     ]
    }
   ],
   "source": [
    "# Find newest CSV file with metadata\n",
    "csv_path = find_newest_csv_by_date(DATA_DIR, pattern='*with_metadata.csv')\n",
    "\n",
    "# Read CSV\n",
    "print('\\nForward filling!')\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f'Original columns: {list(df.columns[:10])}...')\n",
    "\n",
    "# Ensure Date is datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# 1. Sort by Symbol then Date\n",
    "df = df.sort_values(['Symbol', 'Date'])\n",
    "\n",
    "# 2. Forward fill missing values PER SYMBOL\n",
    "# Use apply with ffill to preserve all columns including sharadar metadata\n",
    "print(f\"Forward filling missing values per symbol...\")\n",
    "df = df.groupby('Symbol').apply(lambda group: group.ffill())\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# 3. Save forward-filled version\n",
    "output_path = csv_path.replace('.csv', '_ffiled.csv')\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"\\n\u2713 Saved forward-filled CSV to: {output_path}\")\n",
    "print(f\"  Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load CSV Files\n",
    "\n",
    "Load all CSV files from the data directory and concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CSV files:\n",
      "  - 20091231_20251118_with_metadata_ffiled.csv\n",
      "\n",
      "Loading CSV files in chunks (memory-efficient mode)...\n",
      "Columns in CSV: 47\n",
      "Columns in CSV: Index(['Date', 'Symbol', 'Instrument', 'RefPriceClose', 'RefVolume', 'CompanyCommonName',\n",
      "       'EnterpriseValue_DailyTimeSeries_', 'CompanyMarketCap', 'GICSSectorName', 'FOCFExDividends_Discrete',\n",
      "       'InterestExpense_NetofCapitalizedInterest', 'Debt_Total', 'EarningsPerShare_Actual',\n",
      "       'EarningsPerShare_SmartEstimate_prev_Q', 'EarningsPerShare_ActualSurprise',\n",
      "       'EarningsPerShare_SmartEstimate_current_Q', 'LongTermGrowth_Mean', 'PriceTarget_Median',\n",
      "       'CombinedAlphaModelSectorRank', 'CombinedAlphaModelSectorRankChange', 'CombinedAlphaModelRegionRank',\n",
      "       'EarningsQualityRegionRank_Current', 'EnterpriseValueToEBIT_DailyTimeSeriesRatio_',\n",
      "       'EnterpriseValueToEBITDA_DailyTimeSeriesRatio_', 'EnterpriseValueToSales_DailyTimeSeriesRatio_',\n",
      "       'Dividend_Per_Share_SmartEstimate', 'CashCashEquivalents_Total', 'ForwardPEG_DailyTimeSeriesRatio_',\n",
      "       'PriceEarningsToGrowthRatio_SmartEstimate_', 'Recommendation_Median_1_5_', 'ReturnOnEquity_SmartEstimat',\n",
      "       'ReturnOnAssets_SmartEstimate', 'ForwardPriceToCashFlowPerShare_DailyTimeSeriesRatio_',\n",
      "       'ForwardPriceToSalesPerShare_DailyTimeSeriesRatio_',\n",
      "       'ForwardEnterpriseValueToOperatingCashFlow_DailyTimeSeriesRatio_', 'GrossProfitMargin_ActualSurprise',\n",
      "       'Estpricegrowth_percent', 'TradeDate', 'sharadar_exchange', 'sharadar_category', 'sharadar_location',\n",
      "       'sharadar_sector', 'sharadar_industry', 'sharadar_sicsector', 'sharadar_sicindustry', 'sharadar_scalemarketcap',\n",
      "       'sharadar_is_adr'],\n",
      "      dtype='object')\n",
      "Total rows in CSV: 9,010,487\n",
      "\n",
      "\u2713 Loaded sample of 1,000 rows for preview\n",
      "Full dataset has 9,010,487 rows - will be processed in chunks\n",
      "Date range (sample): 2009-12-31 to 2013-12-19\n",
      "Unique symbols (sample): 1\n",
      "\n",
      "Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>RefPriceClose</th>\n",
       "      <th>RefVolume</th>\n",
       "      <th>CompanyCommonName</th>\n",
       "      <th>EnterpriseValue_DailyTimeSeries_</th>\n",
       "      <th>CompanyMarketCap</th>\n",
       "      <th>GICSSectorName</th>\n",
       "      <th>FOCFExDividends_Discrete</th>\n",
       "      <th>InterestExpense_NetofCapitalizedInterest</th>\n",
       "      <th>Debt_Total</th>\n",
       "      <th>EarningsPerShare_Actual</th>\n",
       "      <th>EarningsPerShare_SmartEstimate_prev_Q</th>\n",
       "      <th>EarningsPerShare_ActualSurprise</th>\n",
       "      <th>EarningsPerShare_SmartEstimate_current_Q</th>\n",
       "      <th>LongTermGrowth_Mean</th>\n",
       "      <th>PriceTarget_Median</th>\n",
       "      <th>CombinedAlphaModelSectorRank</th>\n",
       "      <th>CombinedAlphaModelSectorRankChange</th>\n",
       "      <th>CombinedAlphaModelRegionRank</th>\n",
       "      <th>EarningsQualityRegionRank_Current</th>\n",
       "      <th>EnterpriseValueToEBIT_DailyTimeSeriesRatio_</th>\n",
       "      <th>EnterpriseValueToEBITDA_DailyTimeSeriesRatio_</th>\n",
       "      <th>EnterpriseValueToSales_DailyTimeSeriesRatio_</th>\n",
       "      <th>Dividend_Per_Share_SmartEstimate</th>\n",
       "      <th>CashCashEquivalents_Total</th>\n",
       "      <th>ForwardPEG_DailyTimeSeriesRatio_</th>\n",
       "      <th>PriceEarningsToGrowthRatio_SmartEstimate_</th>\n",
       "      <th>Recommendation_Median_1_5_</th>\n",
       "      <th>ReturnOnEquity_SmartEstimat</th>\n",
       "      <th>ReturnOnAssets_SmartEstimate</th>\n",
       "      <th>ForwardPriceToCashFlowPerShare_DailyTimeSeriesRatio_</th>\n",
       "      <th>ForwardPriceToSalesPerShare_DailyTimeSeriesRatio_</th>\n",
       "      <th>ForwardEnterpriseValueToOperatingCashFlow_DailyTimeSeriesRatio_</th>\n",
       "      <th>GrossProfitMargin_ActualSurprise</th>\n",
       "      <th>Estpricegrowth_percent</th>\n",
       "      <th>TradeDate</th>\n",
       "      <th>sharadar_exchange</th>\n",
       "      <th>sharadar_category</th>\n",
       "      <th>sharadar_location</th>\n",
       "      <th>sharadar_sector</th>\n",
       "      <th>sharadar_industry</th>\n",
       "      <th>sharadar_sicsector</th>\n",
       "      <th>sharadar_sicindustry</th>\n",
       "      <th>sharadar_scalemarketcap</th>\n",
       "      <th>sharadar_is_adr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>A</td>\n",
       "      <td>22.217753</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Agilent Technologies Inc</td>\n",
       "      <td>1.125719e+10</td>\n",
       "      <td>1.083819e+10</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>183000000.0</td>\n",
       "      <td>21000000.0</td>\n",
       "      <td>2.904000e+09</td>\n",
       "      <td>0.32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.404</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>32.535225</td>\n",
       "      <td>22.335690</td>\n",
       "      <td>2.512204</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.479000e+09</td>\n",
       "      <td>0.993966</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.785078</td>\n",
       "      <td>1.609831</td>\n",
       "      <td>20.022743</td>\n",
       "      <td>4.081</td>\n",
       "      <td>0.485299</td>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>NYSE</td>\n",
       "      <td>Domestic Common Stock</td>\n",
       "      <td>California; U.S.A</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>Diagnostics &amp; Research</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>Laboratory Analytical Instruments</td>\n",
       "      <td>5 - Large</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>A</td>\n",
       "      <td>22.382223</td>\n",
       "      <td>894600.0</td>\n",
       "      <td>Agilent Technologies Inc</td>\n",
       "      <td>1.133742e+10</td>\n",
       "      <td>1.091842e+10</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>183000000.0</td>\n",
       "      <td>21000000.0</td>\n",
       "      <td>2.904000e+09</td>\n",
       "      <td>0.32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.404</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>32.767107</td>\n",
       "      <td>22.494879</td>\n",
       "      <td>2.530109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.479000e+09</td>\n",
       "      <td>0.999386</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.857960</td>\n",
       "      <td>1.620728</td>\n",
       "      <td>20.123099</td>\n",
       "      <td>4.081</td>\n",
       "      <td>0.474384</td>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>NYSE</td>\n",
       "      <td>Domestic Common Stock</td>\n",
       "      <td>California; U.S.A</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>Diagnostics &amp; Research</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>Laboratory Analytical Instruments</td>\n",
       "      <td>5 - Large</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>A</td>\n",
       "      <td>22.139094</td>\n",
       "      <td>828100.0</td>\n",
       "      <td>Agilent Technologies Inc</td>\n",
       "      <td>1.121882e+10</td>\n",
       "      <td>1.079982e+10</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>183000000.0</td>\n",
       "      <td>21000000.0</td>\n",
       "      <td>2.904000e+09</td>\n",
       "      <td>0.32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.404</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>32.424325</td>\n",
       "      <td>22.259556</td>\n",
       "      <td>2.503641</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.479000e+09</td>\n",
       "      <td>0.988052</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.700236</td>\n",
       "      <td>1.602871</td>\n",
       "      <td>19.902142</td>\n",
       "      <td>4.081</td>\n",
       "      <td>0.490576</td>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>NYSE</td>\n",
       "      <td>Domestic Common Stock</td>\n",
       "      <td>California; U.S.A</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>Diagnostics &amp; Research</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>Laboratory Analytical Instruments</td>\n",
       "      <td>5 - Large</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>A</td>\n",
       "      <td>22.060434</td>\n",
       "      <td>852400.0</td>\n",
       "      <td>Agilent Technologies Inc</td>\n",
       "      <td>1.118232e+10</td>\n",
       "      <td>1.076332e+10</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>183000000.0</td>\n",
       "      <td>21000000.0</td>\n",
       "      <td>2.904000e+09</td>\n",
       "      <td>0.32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.404</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>32.318846</td>\n",
       "      <td>22.187144</td>\n",
       "      <td>2.495497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.479000e+09</td>\n",
       "      <td>0.984066</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.644399</td>\n",
       "      <td>1.597203</td>\n",
       "      <td>19.823539</td>\n",
       "      <td>4.081</td>\n",
       "      <td>0.495891</td>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>NYSE</td>\n",
       "      <td>Domestic Common Stock</td>\n",
       "      <td>California; U.S.A</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>Diagnostics &amp; Research</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>Laboratory Analytical Instruments</td>\n",
       "      <td>5 - Large</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>A</td>\n",
       "      <td>22.031830</td>\n",
       "      <td>603700.0</td>\n",
       "      <td>Agilent Technologies Inc</td>\n",
       "      <td>1.116837e+10</td>\n",
       "      <td>1.074937e+10</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>183000000.0</td>\n",
       "      <td>21000000.0</td>\n",
       "      <td>2.904000e+09</td>\n",
       "      <td>0.32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.404</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>32.278512</td>\n",
       "      <td>22.159454</td>\n",
       "      <td>2.492382</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.479000e+09</td>\n",
       "      <td>0.982315</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.619564</td>\n",
       "      <td>1.594881</td>\n",
       "      <td>19.788420</td>\n",
       "      <td>4.081</td>\n",
       "      <td>0.497833</td>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>NYSE</td>\n",
       "      <td>Domestic Common Stock</td>\n",
       "      <td>California; U.S.A</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>Diagnostics &amp; Research</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>Laboratory Analytical Instruments</td>\n",
       "      <td>5 - Large</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date Symbol  RefPriceClose  RefVolume         CompanyCommonName  EnterpriseValue_DailyTimeSeries_  \\\n",
       "0  2009-12-31      A      22.217753        NaN  Agilent Technologies Inc                      1.125719e+10   \n",
       "1  2010-01-04      A      22.382223   894600.0  Agilent Technologies Inc                      1.133742e+10   \n",
       "2  2010-01-05      A      22.139094   828100.0  Agilent Technologies Inc                      1.121882e+10   \n",
       "3  2010-01-06      A      22.060434   852400.0  Agilent Technologies Inc                      1.118232e+10   \n",
       "4  2010-01-07      A      22.031830   603700.0  Agilent Technologies Inc                      1.116837e+10   \n",
       "\n",
       "   CompanyMarketCap GICSSectorName  FOCFExDividends_Discrete  InterestExpense_NetofCapitalizedInterest    Debt_Total  \\\n",
       "0      1.083819e+10    Health Care               183000000.0                                21000000.0  2.904000e+09   \n",
       "1      1.091842e+10    Health Care               183000000.0                                21000000.0  2.904000e+09   \n",
       "2      1.079982e+10    Health Care               183000000.0                                21000000.0  2.904000e+09   \n",
       "3      1.076332e+10    Health Care               183000000.0                                21000000.0  2.904000e+09   \n",
       "4      1.074937e+10    Health Care               183000000.0                                21000000.0  2.904000e+09   \n",
       "\n",
       "   EarningsPerShare_Actual  EarningsPerShare_SmartEstimate_prev_Q  EarningsPerShare_ActualSurprise  \\\n",
       "0                     0.32                                    NaN                           37.404   \n",
       "1                     0.32                                    NaN                           37.404   \n",
       "2                     0.32                                    NaN                           37.404   \n",
       "3                     0.32                                    NaN                           37.404   \n",
       "4                     0.32                                    NaN                           37.404   \n",
       "\n",
       "   EarningsPerShare_SmartEstimate_current_Q  LongTermGrowth_Mean  PriceTarget_Median  CombinedAlphaModelSectorRank  \\\n",
       "0                                       NaN                 15.0                33.0                          40.0   \n",
       "1                                       NaN                 15.0                33.0                          44.0   \n",
       "2                                       NaN                 15.0                33.0                          42.0   \n",
       "3                                       NaN                 15.0                33.0                          43.0   \n",
       "4                                       NaN                 15.0                33.0                          41.0   \n",
       "\n",
       "   CombinedAlphaModelSectorRankChange  CombinedAlphaModelRegionRank  EarningsQualityRegionRank_Current  \\\n",
       "0                                 NaN                          41.0                               40.0   \n",
       "1                                 2.0                          43.0                               40.0   \n",
       "2                                -1.0                          41.0                               40.0   \n",
       "3                                 2.0                          42.0                               40.0   \n",
       "4                                -1.0                          40.0                               40.0   \n",
       "\n",
       "   EnterpriseValueToEBIT_DailyTimeSeriesRatio_  EnterpriseValueToEBITDA_DailyTimeSeriesRatio_  \\\n",
       "0                                    32.535225                                      22.335690   \n",
       "1                                    32.767107                                      22.494879   \n",
       "2                                    32.424325                                      22.259556   \n",
       "3                                    32.318846                                      22.187144   \n",
       "4                                    32.278512                                      22.159454   \n",
       "\n",
       "   EnterpriseValueToSales_DailyTimeSeriesRatio_  Dividend_Per_Share_SmartEstimate  CashCashEquivalents_Total  \\\n",
       "0                                      2.512204                               NaN               2.479000e+09   \n",
       "1                                      2.530109                               NaN               2.479000e+09   \n",
       "2                                      2.503641                               NaN               2.479000e+09   \n",
       "3                                      2.495497                               NaN               2.479000e+09   \n",
       "4                                      2.492382                               NaN               2.479000e+09   \n",
       "\n",
       "   ForwardPEG_DailyTimeSeriesRatio_  PriceEarningsToGrowthRatio_SmartEstimate_  Recommendation_Median_1_5_  \\\n",
       "0                          0.993966                                        NaN                         2.0   \n",
       "1                          0.999386                                        NaN                         2.0   \n",
       "2                          0.988052                                        NaN                         2.0   \n",
       "3                          0.984066                                        NaN                         2.0   \n",
       "4                          0.982315                                        NaN                         2.0   \n",
       "\n",
       "   ReturnOnEquity_SmartEstimat  ReturnOnAssets_SmartEstimate  ForwardPriceToCashFlowPerShare_DailyTimeSeriesRatio_  \\\n",
       "0                          NaN                           NaN                                          13.785078      \n",
       "1                          NaN                           NaN                                          13.857960      \n",
       "2                          NaN                           NaN                                          13.700236      \n",
       "3                          NaN                           NaN                                          13.644399      \n",
       "4                          NaN                           NaN                                          13.619564      \n",
       "\n",
       "   ForwardPriceToSalesPerShare_DailyTimeSeriesRatio_  ForwardEnterpriseValueToOperatingCashFlow_DailyTimeSeriesRatio_  \\\n",
       "0                                           1.609831                                          20.022743                 \n",
       "1                                           1.620728                                          20.123099                 \n",
       "2                                           1.602871                                          19.902142                 \n",
       "3                                           1.597203                                          19.823539                 \n",
       "4                                           1.594881                                          19.788420                 \n",
       "\n",
       "   GrossProfitMargin_ActualSurprise  Estpricegrowth_percent   TradeDate sharadar_exchange      sharadar_category  \\\n",
       "0                             4.081                0.485299  2009-12-31              NYSE  Domestic Common Stock   \n",
       "1                             4.081                0.474384  2010-01-04              NYSE  Domestic Common Stock   \n",
       "2                             4.081                0.490576  2010-01-05              NYSE  Domestic Common Stock   \n",
       "3                             4.081                0.495891  2010-01-06              NYSE  Domestic Common Stock   \n",
       "4                             4.081                0.497833  2010-01-07              NYSE  Domestic Common Stock   \n",
       "\n",
       "   sharadar_location sharadar_sector       sharadar_industry sharadar_sicsector               sharadar_sicindustry  \\\n",
       "0  California; U.S.A      Healthcare  Diagnostics & Research      Manufacturing  Laboratory Analytical Instruments   \n",
       "1  California; U.S.A      Healthcare  Diagnostics & Research      Manufacturing  Laboratory Analytical Instruments   \n",
       "2  California; U.S.A      Healthcare  Diagnostics & Research      Manufacturing  Laboratory Analytical Instruments   \n",
       "3  California; U.S.A      Healthcare  Diagnostics & Research      Manufacturing  Laboratory Analytical Instruments   \n",
       "4  California; U.S.A      Healthcare  Diagnostics & Research      Manufacturing  Laboratory Analytical Instruments   \n",
       "\n",
       "  sharadar_scalemarketcap  sharadar_is_adr  \n",
       "0               5 - Large                0  \n",
       "1               5 - Large                0  \n",
       "2               5 - Large                0  \n",
       "3               5 - Large                0  \n",
       "4               5 - Large                0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find newest forward-filled CSV file\n",
    "csv_path = find_newest_csv_by_date(DATA_DIR, pattern='*with_metadata_ffiled.csv')\n",
    "\n",
    "print(f\"\\nLoading data from: {csv_path}\")\n",
    "print(\"Loading CSV in chunks (memory-efficient mode)...\")\n",
    "\n",
    "# First pass: just get column info and row count\n",
    "first_chunk = pd.read_csv(csv_path, nrows=1000)\n",
    "all_columns = first_chunk.columns.tolist()\n",
    "\n",
    "print(f\"\\nCSV Structure:\")\n",
    "print(f\"  Total columns: {len(all_columns)}\")\n",
    "print(f\"  First 10 columns: {all_columns[:10]}\")\n",
    "\n",
    "# Define column categories for database tables\n",
    "required_cols = ['Date', 'Sid', 'Symbol']\n",
    "\n",
    "# Identify sharadar metadata columns (with sharadar_ prefix)\n",
    "sharadar_cols = [col for col in all_columns if col.startswith('sharadar_')]\n",
    "if sharadar_cols:\n",
    "    print(f\"\\n\u2713 Found {len(sharadar_cols)} Sharadar metadata columns:\")\n",
    "    for col in sharadar_cols:\n",
    "        print(f\"    - {col}\")\n",
    "else:\n",
    "    print(\"\\n\u26a0\ufe0f  No Sharadar metadata columns found (expected columns with 'sharadar_' prefix)\")\n",
    "\n",
    "# Rest of the columns for fundamentals tables\n",
    "fundamentals_cols = [col for col in all_columns if col not in required_cols and col not in sharadar_cols]\n",
    "\n",
    "print(f\"\\nColumn distribution:\")\n",
    "print(f\"  Required columns: {len(required_cols)} ({', '.join(required_cols)})\")\n",
    "print(f\"  Sharadar metadata: {len(sharadar_cols)}\")\n",
    "print(f\"  Fundamentals data: {len(fundamentals_cols)}\")\n",
    "\n",
    "# Load data in chunks\n",
    "chunk_size = 10000\n",
    "chunks = []\n",
    "total_rows = 0\n",
    "\n",
    "print(f\"\\nReading CSV in chunks of {chunk_size:,} rows...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optional: Load Recent Data Only\n",
    "\n",
    "To reduce memory usage, you can filter to recent data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Keep only recent data (e.g., last 600,000 rows)\n",
    "# Comment out if you want all historical data\n",
    "# RECENT_ROWS = 600000\n",
    "\n",
    "# if len(custom_data) > RECENT_ROWS:\n",
    "#     print(f\"Filtering to most recent {RECENT_ROWS:,} rows...\")\n",
    "#     custom_data = custom_data.tail(RECENT_ROWS).copy()\n",
    "#     print(f\"\u2713 Filtered. New date range: {custom_data['Date'].min()} to {custom_data['Date'].max()}\")\n",
    "# else:\n",
    "#     print(f\"Dataset has {len(custom_data):,} rows - no filtering needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Map Symbols to Zipline SIDs (WITH TEMPORAL MAPPING)\n",
    "\n",
    "Map your symbols to Zipline Security IDs (SIDs) using **temporal lookups** to handle symbol changes.\n",
    "\n",
    "**IMPORTANT UPDATE**: This notebook now uses **temporal SID mapping** which correctly handles:\n",
    "- Company name changes (FB \u2192 META, etc.)\n",
    "- Ticker symbol changes over time\n",
    "- Mergers and acquisitions\n",
    "\n",
    "The temporal mapper uses `asset_finder.lookup_symbol(symbol, as_of_date)` to get the correct SID for each row's date, ensuring continuous data for companies that changed symbols.\n",
    "\n",
    "**How it works:**\n",
    "- For a row with Symbol='FB', Date='2020-01-01' \u2192 Returns META's SID (the company)\n",
    "- For a row with Symbol='META', Date='2023-01-01' \u2192 Returns same META SID\n",
    "- Result: Continuous data under one SID, no breaks at symbol changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MEMORY-EFFICIENT CHUNK PROCESSING\n",
      "================================================================================\n",
      "\n",
      "1. Loading Sharadar bundle...\n",
      "   Asset finder loaded with 30,859 securities\n",
      "\n",
      "2. Loading signal data...\n",
      "          date symbol  pred\n",
      "4666  11/28/25    IBM   -10\n",
      "4667  11/29/25    IBM   -10\n",
      "4668  11/30/25    IBM   -10\n",
      "4669   12/1/25    IBM   -10\n",
      "4670   12/2/25    IBM   -10\n",
      "            Date Symbol  pred\n",
      "4666  2025-11-28    IBM   -10\n",
      "4667  2025-11-29    IBM   -10\n",
      "4668  2025-11-30    IBM   -10\n",
      "4669  2025-12-01    IBM   -10\n",
      "4670  2025-12-02    IBM   -10\n",
      "   VIX signal: 4671 rows\n",
      "   BC signal: 7886 rows\n",
      "\n",
      "3. Preparing database...\n",
      "   Removing existing database (mode='fresh')...\n",
      "\n",
      "4. Processing CSV in chunks of 600,000 rows...\n",
      "   Total rows in CSV: 9,010,487\n",
      "   Expected chunks: 16\n",
      "\n",
      "   Table created with 49 columns\n",
      "\n",
      "   Chunk 1/16 (6.2%) | 566,311 rows | 32,125 rows/sec | ETA: 4.4 min\n",
      "   Chunk 2/16 (12.5%) | 1,141,381 rows | 25,643 rows/sec | ETA: 5.1 min\n",
      "   Chunk 3/16 (18.8%) | 1,715,164 rows | 22,557 rows/sec | ETA: 5.3 min\n",
      "   Chunk 4/16 (25.0%) | 2,292,973 rows | 21,313 rows/sec | ETA: 5.2 min\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MEMORY-EFFICIENT CHUNK PROCESSING\n",
    "# =============================================================================\n",
    "# This cell processes the entire CSV in chunks, maps SIDs, merges signals,\n",
    "# and writes directly to the database without loading everything into memory.\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/app/examples/shared_modules')\n",
    "\n",
    "from zipline.data.bundles import load as load_bundle\n",
    "from temporal_sid_mapper import TemporalSIDMapper\n",
    "import time\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MEMORY-EFFICIENT CHUNK PROCESSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Step 1: Load asset finder\n",
    "print(\"\\n1. Loading Sharadar bundle...\")\n",
    "bundle_timestamp = pd.Timestamp.now(tz='UTC')\n",
    "bundle_data = load_bundle('sharadar', timestamp=bundle_timestamp)\n",
    "asset_finder = bundle_data.asset_finder\n",
    "print(f\"   Asset finder loaded with {len(asset_finder.sids):,} securities\")\n",
    "\n",
    "# Step 2: Load signal data (small files - safe to load fully)\n",
    "print(\"\\n2. Loading signal data...\")\n",
    "vix_signal = None\n",
    "bc_signal = None\n",
    "\n",
    "if os.path.exists(VIX_SIGNAL_PATH):\n",
    "    vix_signal = pd.read_csv(VIX_SIGNAL_PATH)\n",
    "    print(vix_signal.tail())\n",
    "    \n",
    "    vix_signal.rename(columns={'symbol': 'Symbol', 'date': 'Date'}, inplace=True)\n",
    "    # Use format='mixed' to handle various date formats (e.g., 6/15/09 or 2009-06-15)\n",
    "    vix_signal['Date'] = pd.to_datetime(vix_signal['Date'], format='mixed').dt.strftime('%Y-%m-%d')\n",
    "    print(vix_signal.tail())\n",
    "    print(f\"   VIX signal: {len(vix_signal)} rows\")\n",
    "else:\n",
    "    print(f\"   VIX signal not found at {VIX_SIGNAL_PATH}\")\n",
    "\n",
    "if os.path.exists(BC_DATA_PATH):\n",
    "    bc_signal = pd.read_csv(BC_DATA_PATH)\n",
    "    bc_signal.rename(columns={'symbol': 'Symbol', 'date': 'Date'}, inplace=True)\n",
    "    # Use format='mixed' to handle various date formats\n",
    "    bc_signal['Date'] = pd.to_datetime(bc_signal['Date'], format='mixed').dt.strftime('%Y-%m-%d')\n",
    "    print(f\"   BC signal: {len(bc_signal)} rows\")\n",
    "else:\n",
    "    print(f\"   BC signal not found at {BC_DATA_PATH}\")\n",
    "\n",
    "# Step 3: Prepare database\n",
    "print(\"\\n3. Preparing database...\")\n",
    "\n",
    "if UPDATE_MODE == 'fresh' and DB_PATH.exists():\n",
    "    print(f\"   Removing existing database (mode='fresh')...\")\n",
    "    DB_PATH.unlink()\n",
    "\n",
    "conn = sqlite3.connect(str(DB_PATH))\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Step 4: Process CSV in chunks\n",
    "# First, count total rows for progress tracking\n",
    "print(f\"\\n4. Processing CSV in chunks of {CHUNK_SIZE:,} rows...\")\n",
    "total_csv_rows = sum(1 for _ in open(CSV_PATH_FOR_CHUNKS)) - 1  # minus header\n",
    "total_chunks = (total_csv_rows + CHUNK_SIZE - 1) // CHUNK_SIZE\n",
    "print(f\"   Total rows in CSV: {total_csv_rows:,}\")\n",
    "print(f\"   Expected chunks: {total_chunks}\")\n",
    "print(\"\")\n",
    "\n",
    "# Build symbol lookup cache\n",
    "symbol_to_sid = {}\n",
    "table_created = False\n",
    "total_rows = 0\n",
    "total_mapped = 0\n",
    "unmapped_symbols = set()\n",
    "start_time = time.time()\n",
    "\n",
    "for chunk_num, chunk in enumerate(pd.read_csv(CSV_PATH_FOR_CHUNKS, chunksize=CHUNK_SIZE)):\n",
    "    chunk_start = time.time()\n",
    "    \n",
    "    # Drop unwanted columns\n",
    "    cols_to_drop = [c for c in ['BSid', 'Instrument', 'pred'] if c in chunk.columns]\n",
    "    if cols_to_drop:\n",
    "        chunk = chunk.drop(columns=cols_to_drop)\n",
    "    \n",
    "    # Convert date - use mixed format to handle various date formats\n",
    "    chunk['Date'] = pd.to_datetime(chunk['Date'], format='mixed')\n",
    "    \n",
    "    # Map symbols to SIDs (batch lookup for efficiency)\n",
    "    unique_symbols = chunk['Symbol'].unique()\n",
    "    for sym in unique_symbols:\n",
    "        if sym not in symbol_to_sid:\n",
    "            try:\n",
    "                asset = asset_finder.lookup_symbol(sym, as_of_date=None)\n",
    "                if asset:\n",
    "                    symbol_to_sid[sym] = int(asset.sid)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Apply SID mapping\n",
    "    chunk['Sid'] = chunk['Symbol'].map(symbol_to_sid)\n",
    "    \n",
    "    # Track unmapped\n",
    "    unmapped = chunk[chunk['Sid'].isna()]['Symbol'].unique()\n",
    "    unmapped_symbols.update(unmapped)\n",
    "    \n",
    "    # Remove unmapped rows\n",
    "    chunk = chunk[chunk['Sid'].notna()].copy()\n",
    "    if len(chunk) == 0:\n",
    "        continue\n",
    "    chunk['Sid'] = chunk['Sid'].astype(int)\n",
    "    \n",
    "    # Merge VIX signal\n",
    "    if vix_signal is not None:\n",
    "        chunk['Date_str'] = chunk['Date'].dt.strftime('%Y-%m-%d')\n",
    "        chunk = chunk.merge(\n",
    "            vix_signal[['Symbol', 'Date', 'pred']], \n",
    "            left_on=['Symbol', 'Date_str'],\n",
    "            right_on=['Symbol', 'Date'],\n",
    "            how='left',\n",
    "            suffixes=('', '_vix')\n",
    "        )\n",
    "        chunk = chunk.drop(columns=['Date_vix', 'Date_str'], errors='ignore')\n",
    "    else:\n",
    "        chunk['pred'] = np.nan\n",
    "    \n",
    "    # Merge BC signal\n",
    "    if bc_signal is not None:\n",
    "        chunk['Date_str'] = chunk['Date'].dt.strftime('%Y-%m-%d')\n",
    "        chunk = chunk.merge(\n",
    "            bc_signal[['Symbol', 'Date', 'bc1']], \n",
    "            left_on=['Symbol', 'Date_str'],\n",
    "            right_on=['Symbol', 'Date'],\n",
    "            how='left',\n",
    "            suffixes=('', '_bc')\n",
    "        )\n",
    "        chunk = chunk.drop(columns=['Date_bc', 'Date_str'], errors='ignore')\n",
    "    else:\n",
    "        chunk['bc1'] = np.nan\n",
    "    \n",
    "    # Forward fill missing values by symbol\n",
    "    for col in chunk.columns:\n",
    "        if col not in ['Symbol', 'Sid', 'Date']:\n",
    "            chunk[col] = chunk.groupby('Symbol')[col].transform(lambda x: x.ffill())\n",
    "    \n",
    "    # Fill remaining NaNs\n",
    "    for col in chunk.columns:\n",
    "        if col not in ['Symbol', 'Sid', 'Date']:\n",
    "            if chunk[col].dtype == 'object' or col in ['GICSSectorName', 'CompanyCommonName', 'TradeDate']:\n",
    "                chunk[col] = chunk[col].fillna('')\n",
    "            else:\n",
    "                chunk[col] = chunk[col].fillna(0)\n",
    "    \n",
    "    # Convert date to string for SQLite\n",
    "    chunk['Date'] = chunk['Date'].dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Deduplicate\n",
    "    chunk = chunk.drop_duplicates(subset=['Sid', 'Date'], keep='last')\n",
    "    \n",
    "    # Create table on first chunk\n",
    "    if not table_created:\n",
    "        cursor.execute(\"DROP TABLE IF EXISTS Price\")\n",
    "\n",
    "        # Define which columns should be TEXT (not REAL)\n",
    "        TEXT_COLUMNS = {\n",
    "            'Date', 'Symbol', 'CompanyCommonName', 'GICSSectorName', 'TradeDate',\n",
    "            # Sharadar metadata columns - MUST be TEXT for proper Pipeline filtering\n",
    "            'sharadar_exchange', 'sharadar_category', 'sharadar_location',\n",
    "            'sharadar_sector', 'sharadar_industry', 'sharadar_sicsector',\n",
    "            'sharadar_sicindustry', 'sharadar_scalemarketcap'\n",
    "        }\n",
    "\n",
    "        # Build schema from chunk columns\n",
    "        columns = []\n",
    "        for col in chunk.columns:\n",
    "            if col == 'Sid':\n",
    "                columns.append(f'\"{col}\" INTEGER')\n",
    "            elif col in TEXT_COLUMNS:\n",
    "                columns.append(f'\"{col}\" TEXT')\n",
    "            else:\n",
    "                columns.append(f'\"{col}\" REAL')\n",
    "        \n",
    "        create_sql = f'''\n",
    "            CREATE TABLE Price (\n",
    "                {', '.join(columns)},\n",
    "                UNIQUE(Sid, Date)\n",
    "            )\n",
    "        '''\n",
    "        cursor.execute(create_sql)\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_sid ON Price(Sid)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_date ON Price(Date)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_symbol ON Price(Symbol)')\n",
    "        conn.commit()\n",
    "        table_created = True\n",
    "        print(f\"   Table created with {len(columns)} columns\\n\")\n",
    "    \n",
    "    # Insert data\n",
    "    placeholders = ', '.join(['?' for _ in chunk.columns])\n",
    "    column_names = ', '.join([f'\"{c}\"' for c in chunk.columns])\n",
    "    \n",
    "    if UPDATE_MODE == 'replace':\n",
    "        insert_sql = f'INSERT OR REPLACE INTO Price ({column_names}) VALUES ({placeholders})'\n",
    "    elif UPDATE_MODE == 'ignore':\n",
    "        insert_sql = f'INSERT OR IGNORE INTO Price ({column_names}) VALUES ({placeholders})'\n",
    "    else:\n",
    "        insert_sql = f'INSERT OR REPLACE INTO Price ({column_names}) VALUES ({placeholders})'\n",
    "    \n",
    "    cursor.executemany(insert_sql, chunk.values.tolist())\n",
    "    conn.commit()\n",
    "    \n",
    "    total_mapped += len(chunk)\n",
    "    total_rows += len(chunk)\n",
    "    \n",
    "    # Progress update\n",
    "    chunk_time = time.time() - chunk_start\n",
    "    elapsed = time.time() - start_time\n",
    "    progress = (chunk_num + 1) / total_chunks * 100\n",
    "    rows_per_sec = total_rows / elapsed if elapsed > 0 else 0\n",
    "    eta_seconds = (total_csv_rows - (chunk_num + 1) * CHUNK_SIZE) / rows_per_sec if rows_per_sec > 0 else 0\n",
    "    eta_min = eta_seconds / 60\n",
    "    \n",
    "    print(f\"   Chunk {chunk_num + 1}/{total_chunks} ({progress:.1f}%) | \"\n",
    "          f\"{total_mapped:,} rows | \"\n",
    "          f\"{rows_per_sec:,.0f} rows/sec | \"\n",
    "          f\"ETA: {eta_min:.1f} min\")\n",
    "\n",
    "# Final stats\n",
    "cursor.execute(\"SELECT COUNT(*) FROM Price\")\n",
    "db_rows = cursor.fetchone()[0]\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) FROM Price WHERE pred IS NOT NULL AND pred != 0\")\n",
    "pred_rows = cursor.fetchone()[0]\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) FROM Price WHERE bc1 IS NOT NULL AND bc1 != 0\")\n",
    "bc1_rows = cursor.fetchone()[0]\n",
    "\n",
    "conn.close()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROCESSING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Database: {DB_PATH}\")\n",
    "print(f\"Total rows in database: {db_rows:,}\")\n",
    "print(f\"Rows with VIX signal (pred): {pred_rows:,}\")\n",
    "print(f\"Rows with BC signal (bc1): {bc1_rows:,}\")\n",
    "print(f\"Unmapped symbols: {len(unmapped_symbols)}\")\n",
    "if len(unmapped_symbols) > 0:\n",
    "    print(f\"  First 10 unmapped: {list(unmapped_symbols)[:10]}\")\n",
    "print(f\"File size: {DB_PATH.stat().st_size / (1024**3):.2f} GB\")\n",
    "print(f\"Total time: {total_time/60:.1f} minutes ({total_time:.0f} seconds)\")\n",
    "print(f\"Average speed: {db_rows/total_time:,.0f} rows/second\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect('/data/custom_databases/fundamentals.sqlite')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"PRAGMA table_info(Price)\")\n",
    "columns = cursor.fetchall()\n",
    "\n",
    "# Check Sharadar columns\n",
    "for col in columns:\n",
    "  if 'sharadar' in col[1].lower():\n",
    "      print(f\"{col[1]:40} {col[2]}\")  # Should show TEXT, not REAL\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Loading Complete\n",
    "\n",
    "Cell 14 above handles all processing:\n",
    "- Loads CSV in memory-efficient chunks\n",
    "- Maps symbols to SIDs\n",
    "- Merges VIX/BC signal data\n",
    "- Deduplicates data\n",
    "- Creates SQLite database at `/data/custom_databases/fundamentals.sqlite`\n",
    "\n",
    "No additional processing steps needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Define Database Class\n",
    "\n",
    "Create a Database class to use this data in Zipline Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Database class\n",
    "class CustomFundamentals(Database):\n",
    "    \"\"\"\n",
    "    Custom Custom fundamentals database.\n",
    "    \n",
    "    Usage in Pipeline:\n",
    "        roe = CustomFundamentals.ReturnOnEquity_SmartEstimat.latest\n",
    "        sector = CustomFundamentals.GICSSectorName.latest\n",
    "    \"\"\"\n",
    "    \n",
    "    CODE = DATABASE_NAME\n",
    "    LOOKBACK_WINDOW = 252  # Days to look back\n",
    "    \n",
    "    # Price and volume\n",
    "    RefPriceClose = Column(float)\n",
    "    RefVolume = Column(float)\n",
    "    \n",
    "    # Company info\n",
    "    CompanyCommonName = Column(str)\n",
    "    GICSSectorName = Column(str)\n",
    "    \n",
    "    # Valuation metrics\n",
    "    EnterpriseValue_DailyTimeSeries_ = Column(float)\n",
    "    CompanyMarketCap = Column(float)\n",
    "    \n",
    "    # Cash flow\n",
    "    FOCFExDividends_Discrete = Column(float)\n",
    "    CashFlowComponent_Current = Column(float)\n",
    "    CashFlowPerShare_BrokerEstimate = Column(float)\n",
    "    FreeCashFlowPerShare_BrokerEstimate = Column(float)\n",
    "    \n",
    "    # Debt and interest\n",
    "    InterestExpense_NetofCapitalizedInterest = Column(float)\n",
    "    Debt_Total = Column(float)\n",
    "    \n",
    "    # Earnings\n",
    "    EarningsPerShare_Actual = Column(float)\n",
    "    EarningsPerShare_SmartEstimate_prev_Q = Column(float)\n",
    "    EarningsPerShare_ActualSurprise = Column(float)\n",
    "    EarningsPerShare_SmartEstimate_current_Q = Column(float)\n",
    "    EPS_SurpirsePrct_prev_Q = Column(float)\n",
    "    \n",
    "    # Growth and targets\n",
    "    LongTermGrowth_Mean = Column(float)\n",
    "    PriceTarget_Median = Column(float)\n",
    "    Estpricegrowth_percent = Column(float)\n",
    "    \n",
    "    # Rankings\n",
    "    CombinedAlphaModelSectorRank = Column(float)\n",
    "    CombinedAlphaModelSectorRankChange = Column(float)\n",
    "    CombinedAlphaModelRegionRank = Column(float)\n",
    "    EarningsQualityRegionRank_Current = Column(float)\n",
    "    \n",
    "    # Ratios\n",
    "    EnterpriseValueToEBIT_DailyTimeSeriesRatio_ = Column(float)\n",
    "    EnterpriseValueToEBITDA_DailyTimeSeriesRatio_ = Column(float)\n",
    "    EnterpriseValueToSales_DailyTimeSeriesRatio_ = Column(float)\n",
    "    ForwardPEG_DailyTimeSeriesRatio_ = Column(float)\n",
    "    PriceEarningsToGrowthRatio_SmartEstimate_ = Column(float)\n",
    "    ForwardPriceToCashFlowPerShare_DailyTimeSeriesRatio_ = Column(float)\n",
    "    ForwardPriceToSalesPerShare_DailyTimeSeriesRatio_ = Column(float)\n",
    "    ForwardEnterpriseValueToOperatingCashFlow_DailyTimeSeriesRatio_ = Column(float)\n",
    "    \n",
    "    # Returns\n",
    "    ReturnOnInvestedCapital_BrokerEstimate = Column(float)\n",
    "    ReturnOnCapitalEmployed_Actual = Column(float)\n",
    "    ReturnOnEquity_SmartEstimat = Column(float)\n",
    "    ReturnOnAssets_SmartEstimate = Column(float)\n",
    "    \n",
    "    # Margins\n",
    "    GrossProfitMargin_ = Column(float)\n",
    "    GrossProfitMargin_ActualSurprise = Column(float)\n",
    "    \n",
    "    # Analyst recommendations\n",
    "    Recommendation_NumberOfTotal = Column(float)\n",
    "    Recommendation_Median_1_5_ = Column(float)\n",
    "    Recommendation_NumberOfStrongBuy = Column(float)\n",
    "    Recommendation_NumberOfBuy = Column(float)\n",
    "    Recommendation_Mean_1_5_ = Column(float)\n",
    "    \n",
    "    # Cash\n",
    "    CashCashEquivalents_Total = Column(float)\n",
    "    \n",
    "    # Dividends\n",
    "    Dividend_Per_Share_SmartEstimate = Column(float)\n",
    "    \n",
    "    # VIX prediction signal\n",
    "    pred = Column(float)\n",
    "\n",
    "\n",
    "print(\"\u2713 CustomFundamentals Database class defined\")\n",
    "print(f\"  Database code: {CustomFundamentals.CODE}\")\n",
    "print(f\"  Lookback window: {CustomFundamentals.LOOKBACK_WINDOW} days\")\n",
    "# Count columns by checking for 'dataset' attribute (BoundColumn instances have this)\n",
    "print(f\"  Columns defined: {len([attr for attr in dir(CustomFundamentals) if hasattr(getattr(CustomFundamentals, attr, None), 'dataset')])}\")\n",
    "\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"  roe = CustomFundamentals.ReturnOnEquity_SmartEstimat.latest\")\n",
    "print(\"  pe_growth = CustomFundamentals.PriceEarningsToGrowthRatio_SmartEstimate_.latest\")\n",
    "print(\"  sector = CustomFundamentals.GICSSectorName.latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Verify Database\n",
    "\n",
    "Query the database to verify data was loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect and query\n",
    "conn = sqlite3.connect(str(DB_PATH))\n",
    "\n",
    "# Get row count\n",
    "row_count = pd.read_sql(\"SELECT COUNT(*) as count FROM Price\", conn).iloc[0, 0]\n",
    "print(f\"Total rows in database: {row_count:,}\")\n",
    "\n",
    "# Get date range\n",
    "date_range = pd.read_sql(\"SELECT MIN(Date) as min_date, MAX(Date) as max_date FROM Price\", conn)\n",
    "print(f\"Date range: {date_range.iloc[0, 0]} to {date_range.iloc[0, 1]}\")\n",
    "\n",
    "# Get symbol count\n",
    "symbol_count = pd.read_sql(\"SELECT COUNT(DISTINCT Symbol) as count FROM Price\", conn).iloc[0, 0]\n",
    "print(f\"Unique symbols: {symbol_count:,}\")\n",
    "\n",
    "# Show sample data for a specific symbol\n",
    "print(\"\\nSample data for AAPL:\")\n",
    "aapl_data = pd.read_sql(\"\"\"\n",
    "    SELECT Date, Symbol, RefPriceClose, CompanyMarketCap, \n",
    "           ReturnOnEquity_SmartEstimat, PriceTarget_Median\n",
    "    FROM Price \n",
    "    WHERE Symbol = 'AAPL' \n",
    "    ORDER BY Date DESC \n",
    "    LIMIT 5\n",
    "\"\"\", conn)\n",
    "print(aapl_data)\n",
    "\n",
    "print(\"\\nSample data for IBM:\")\n",
    "ibm_data = pd.read_sql(\"\"\"\n",
    "    SELECT Date, Symbol, RefPriceClose, CompanyMarketCap, \n",
    "           ReturnOnEquity_SmartEstimat, GICSSectorName\n",
    "    FROM Price \n",
    "    WHERE Symbol = 'IBM' \n",
    "    ORDER BY Date DESC \n",
    "    LIMIT 5\n",
    "\"\"\", conn)\n",
    "print(ibm_data)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"\\n\u2713 Database verification complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Usage Example\n",
    "\n",
    "Example of how to use this database in a backtest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"To use this database in your backtests:\")\n",
    "print(\"\\n1. Import the Database class:\")\n",
    "print(\"   from zipline.pipeline.data.db import Database, Column\")\n",
    "print(\"\\n2. Define the CustomFundamentals class (from cell 10 above)\")\n",
    "print(\"\\n3. Use in your pipeline:\")\n",
    "print(\"   \")\n",
    "print(\"   def make_pipeline():\")\n",
    "print(\"       roe = CustomFundamentals.ReturnOnEquity_SmartEstimat.latest\")\n",
    "print(\"       growth = CustomFundamentals.LongTermGrowth_Mean.latest\")\n",
    "print(\"       sector = CustomFundamentals.GICSSectorName.latest\")\n",
    "print(\"       \")\n",
    "print(\"       # Screen for quality companies\")\n",
    "print(\"       quality = (roe > 15) & (growth > 10)\")\n",
    "print(\"       \")\n",
    "print(\"       return Pipeline(\")\n",
    "print(\"           columns={\")\n",
    "print(\"               'ROE': roe,\")\n",
    "print(\"               'Growth': growth,\")\n",
    "print(\"               'Sector': sector,\")\n",
    "print(\"           },\")\n",
    "print(\"           screen=quality\")\n",
    "print(\"       )\")\n",
    "print(\"\\n4. The CustomSQLiteLoader will automatically load data based on CustomFundamentals.CODE\")\n",
    "\n",
    "print(\"\\n\u2713 Setup complete! Your custom fundamentals database is ready to use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "1. \u2705 Loaded CSV files with fundamental data\n",
    "2. \u2705 Mapped symbols to Zipline SIDs using the asset finder\n",
    "3. \u2705 Cleaned and prepared the data\n",
    "4. \u2705 Created a custom SQLite database in ~/.zipline/data/custom/\n",
    "5. \u2705 Defined a Database class for use in Pipeline\n",
    "6. \u2705 Verified the database contents\n",
    "\n",
    "The database is now ready to use in your Zipline backtests with the CustomSQLiteLoader.\n",
    "\n",
    "**Next steps:**\n",
    "- See the examples below for using the data with Pipeline\n",
    "- Copy the CustomFundamentals class definition to your backtest algorithm\n",
    "- Use CustomFundamentals.ColumnName.latest in your pipeline\n",
    "- The backtest_helpers.py will automatically detect and load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Pipeline Examples\n",
    "\n",
    "Now let's demonstrate how to query and analyze the fundamentals data using Zipline Pipeline.\n",
    "\n",
    "These examples show:\n",
    "- Creating a pipeline with custom fundamentals\n",
    "- Running the pipeline over date ranges\n",
    "- Filtering stocks by fundamental criteria\n",
    "- Extracting time series data for specific symbols\n",
    "- Combining multiple fundamental factors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Setup Pipeline Engine\n",
    "\n",
    "First, we need to set up the Pipeline engine to load our custom data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.pipeline import Pipeline\n",
    "from zipline.pipeline.engine import SimplePipelineEngine\n",
    "from zipline.pipeline.loaders import USEquityPricingLoader\n",
    "from zipline.pipeline.data import USEquityPricing\n",
    "from zipline.pipeline.domain import US_EQUITIES\n",
    "from zipline.utils.calendar_utils import get_calendar\n",
    "from pathlib import Path\n",
    "\n",
    "# Import the custom loader from zipline\n",
    "from zipline.data.custom import CustomSQLiteLoader\n",
    "\n",
    "# Get the trading calendar\n",
    "trading_calendar = get_calendar('NYSE')\n",
    "\n",
    "# Cache loader instances so we return the same object for all columns from a dataset\n",
    "_loader_cache = {}\n",
    "\n",
    "# Set up the pipeline engine with our custom loaders\n",
    "def get_pipeline_loader(column):\n",
    "    \"\"\"\n",
    "    Pipeline loader factory that routes columns to appropriate loaders.\n",
    "    Returns the same loader instance for all columns from the same dataset.\n",
    "    \"\"\"\n",
    "    # Route custom fundamentals to CustomSQLiteLoader\n",
    "    # Domain-bound datasets don't have CODE attribute, so check the dataset's __name__\n",
    "    dataset = column.dataset\n",
    "    \n",
    "    # Check if this is our CustomFundamentals dataset\n",
    "    # Domain-bound datasets have a __name__ attribute with the dataset class name\n",
    "    dataset_name = getattr(dataset, '__name__', '')\n",
    "    \n",
    "    if 'CustomFundamentals' in dataset_name or 'CustomFundamentals' in str(dataset):\n",
    "        # Return cached loader instance for this database\n",
    "        cache_key = CustomFundamentals.CODE\n",
    "        if cache_key not in _loader_cache:\n",
    "            # Specify the correct database directory where we created the database\n",
    "            db_dir = Path('/data/custom_databases')\n",
    "            _loader_cache[cache_key] = CustomSQLiteLoader(\n",
    "                db_code=CustomFundamentals.CODE,\n",
    "                db_dir=db_dir\n",
    "            )\n",
    "        return _loader_cache[cache_key]\n",
    "    \n",
    "    # Route pricing data to bundle\n",
    "    if column in USEquityPricing.columns:\n",
    "        # Use cached pricing loader\n",
    "        if 'pricing' not in _loader_cache:\n",
    "            _loader_cache['pricing'] = USEquityPricingLoader(\n",
    "                bundle_data.equity_daily_bar_reader, \n",
    "                bundle_data.adjustment_reader\n",
    "            )\n",
    "        return _loader_cache['pricing']\n",
    "    \n",
    "    raise ValueError(f\"No loader for {column}\")\n",
    "\n",
    "# Create the pipeline engine\n",
    "engine = SimplePipelineEngine(\n",
    "    get_loader=get_pipeline_loader,\n",
    "    asset_finder=asset_finder,\n",
    "    default_domain=US_EQUITIES,\n",
    ")\n",
    "\n",
    "print(\"\u2713 Pipeline engine configured with custom fundamentals loader\")\n",
    "print(f\"  Trading calendar: {trading_calendar.name}\")\n",
    "print(f\"  Asset finder: {len(asset_finder.sids):,} securities\")\n",
    "print(f\"  Database directory: {Path('/data/custom_databases/')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Basic Pipeline - Get Latest Fundamentals\n",
    "\n",
    "Create a simple pipeline to get the latest fundamentals for all stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pipeline with market cap filter for top 100 stocks\n",
    "def make_basic_pipeline():\n",
    "    \"\"\"\n",
    "    Get latest fundamentals for top 100 stocks by market cap.\n",
    "    This reduces sparse data issues.\n",
    "    \"\"\"\n",
    "    # Get fundamentals\n",
    "    roe = CustomFundamentals.ReturnOnEquity_SmartEstimat.latest\n",
    "    roa = CustomFundamentals.ReturnOnAssets_SmartEstimate.latest\n",
    "    market_cap = CustomFundamentals.CompanyMarketCap.latest\n",
    "    price = CustomFundamentals.RefPriceClose.latest\n",
    "    sector = CustomFundamentals.GICSSectorName.latest\n",
    "    ev_to_ebitda = CustomFundamentals.EnterpriseValueToEBITDA_DailyTimeSeriesRatio_.latest\n",
    "    \n",
    "    # Screen for top 100 stocks by market cap\n",
    "    # This eliminates sparse data issues with small/inactive stocks\n",
    "    top_100_by_mcap = market_cap.top(100)\n",
    "    \n",
    "    return Pipeline(\n",
    "        columns={\n",
    "            'ROE': roe,\n",
    "            'ROA': roa,\n",
    "            'Market_Cap': market_cap,\n",
    "            'Price': price,\n",
    "            'Sector': sector,\n",
    "            'EV_to_EBITDA': ev_to_ebitda,\n",
    "        },\n",
    "        screen=top_100_by_mcap,\n",
    "    )\n",
    "\n",
    "# Run the pipeline for a single date\n",
    "# Get a recent valid trading session from the bundle (last 3 months)\n",
    "pipeline = make_basic_pipeline()\n",
    "\n",
    "# Use recent trading sessions (last 3 months of data)\n",
    "# Note: sessions_in_range expects timezone-naive dates at midnight\n",
    "end_search = pd.Timestamp.now().normalize()\n",
    "start_search = (end_search - pd.DateOffset(months=3)).normalize()\n",
    "\n",
    "sessions = trading_calendar.sessions_in_range(start_search, end_search)\n",
    "start_date = sessions[-5]  # Use 5 days back from the end\n",
    "end_date = start_date\n",
    "\n",
    "print(f\"Using date: {start_date.date()}\")\n",
    "print(f\"Running pipeline with top 100 stocks by market cap filter...\")\n",
    "\n",
    "result = engine.run_pipeline(pipeline, start_date, end_date)\n",
    "\n",
    "# Normalize dataframe to fix pandas 2.x dtype issues\n",
    "result = result.copy()\n",
    "for col in result.columns:\n",
    "    if result[col].dtype == object:\n",
    "        try:\n",
    "            result[col] = result[col].astype(float)\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "\n",
    "print(f\"\\n\u2713 Pipeline run complete\")\n",
    "print(f\"  Date: {start_date.date()}\")\n",
    "print(f\"  Stocks in universe: {len(result):,}\")\n",
    "print(f\"  Sector breakdown:\")\n",
    "print(result['Sector'].value_counts())\n",
    "\n",
    "if len(result) == 0:\n",
    "    print(\"\\n\u26a0\ufe0f  No stocks in universe - no data available for this date\")\n",
    "else:\n",
    "\n",
    "    print(f\"\\nTop 10 stocks by ROE:\")\n",
    "    print(result.sort_values('ROE', ascending=False).head(10)[['ROE', 'ROA', 'Market_Cap', 'Sector']])\n",
    "\n",
    "    print(f\"\\nTop 10 stocks by Market Cap:\")\n",
    "    top_mcap = result.sort_values('Market_Cap', ascending=False).head(10)[['Market_Cap', 'ROE', 'Price', 'Sector']]\n",
    "    top_mcap['Market_Cap_B'] = top_mcap['Market_Cap'] / 1e9  # Convert to billions\n",
    "    print(top_mcap[['Market_Cap_B', 'ROE', 'Price', 'Sector']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Filtered Pipeline - Quality Stocks\n",
    "\n",
    "Filter stocks based on fundamental criteria (e.g., high ROE, profitable, large cap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_quality_pipeline():\n",
    "    \"\"\"\n",
    "    Screen for quality stocks with strong fundamentals.\n",
    "    Limited to top 500 by market cap to avoid sparse data.\n",
    "    \"\"\"\n",
    "    # Get fundamentals\n",
    "    roe = CustomFundamentals.ReturnOnEquity_SmartEstimat.latest\n",
    "    roa = CustomFundamentals.ReturnOnAssets_SmartEstimate.latest\n",
    "    market_cap = CustomFundamentals.CompanyMarketCap.latest\n",
    "    growth = CustomFundamentals.LongTermGrowth_Mean.latest\n",
    "    price_target = CustomFundamentals.PriceTarget_Median.latest\n",
    "    current_price = CustomFundamentals.RefPriceClose.latest\n",
    "    sector = CustomFundamentals.GICSSectorName.latest\n",
    "    \n",
    "    # Calculate upside potential\n",
    "    upside = ((price_target - current_price) / current_price) * 100\n",
    "    \n",
    "    # First filter: top 500 stocks by market cap (reduces sparse data)\n",
    "    top_500_by_mcap = market_cap.top(500)\n",
    "    \n",
    "    # Quality criteria (applied to top 500)\n",
    "    quality_screen = (\n",
    "        top_500_by_mcap &\n",
    "        (roe > 15) &  # Strong return on equity\n",
    "        (roa > 5) &   # Profitable\n",
    "        (market_cap > 1_000_000_000) &  # Large cap ($1B+)\n",
    "        (growth > 10) &  # Double-digit growth\n",
    "        (upside > 10)  # At least 10% upside\n",
    "    )\n",
    "    \n",
    "    return Pipeline(\n",
    "        columns={\n",
    "            'ROE': roe,\n",
    "            'ROA': roa,\n",
    "            'Market_Cap': market_cap,\n",
    "            'Growth': growth,\n",
    "            'Price': current_price,\n",
    "            'Target': price_target,\n",
    "            'Upside_%': upside,\n",
    "            'Sector': sector,\n",
    "        },\n",
    "        screen=quality_screen,\n",
    "    )\n",
    "\n",
    "# Run the filtered pipeline\n",
    "# Use a recent valid trading session (last 3 months)\n",
    "# Note: sessions_in_range expects timezone-naive dates at midnight\n",
    "end_search = pd.Timestamp.now().normalize()\n",
    "start_search = (end_search - pd.DateOffset(months=3)).normalize()\n",
    "\n",
    "sessions = trading_calendar.sessions_in_range(start_search, end_search)\n",
    "start_date = sessions[-5]  # Use 5 days back from the end\n",
    "\n",
    "print(f\"Running quality screen on top 500 stocks by market cap...\")\n",
    "pipeline = make_quality_pipeline()\n",
    "result = engine.run_pipeline(pipeline, start_date, start_date)\n",
    "\n",
    "# Normalize dataframe to fix pandas 2.x dtype issues\n",
    "result = result.copy()\n",
    "for col in result.columns:\n",
    "    if result[col].dtype == object:\n",
    "        try:\n",
    "            result[col] = result[col].astype(float)\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "\n",
    "print(f\"\\n\u2713 Quality screen results:\")\n",
    "print(f\"  Date: {start_date.date()}\")\n",
    "print(f\"  Stocks passing screen: {len(result)}\")\n",
    "\n",
    "if len(result) > 0:\n",
    "    print(f\"  Sector breakdown:\")\n",
    "    print(result['Sector'].value_counts())\n",
    "    \n",
    "    print(f\"\\nTop 10 by upside potential:\")\n",
    "    top_upside = result.sort_values('Upside_%', ascending=False).head(10)[['ROE', 'Growth', 'Price', 'Target', 'Upside_%', 'Sector']]\n",
    "    print(top_upside)\n",
    "else:\n",
    "    print(\"\\n  No stocks passed the quality screen criteria.\")\n",
    "    print(\"  Try relaxing the filters (e.g., ROE > 10, Growth > 5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Time Series Data - Track Fundamentals Over Time\n",
    "\n",
    "Get historical fundamental data for specific symbols to analyze trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define symbols to track\n",
    "symbols = ['AAPL', 'MSFT', 'GOOGL']\n",
    "\n",
    "# Get the assets\n",
    "assets = [asset_finder.lookup_symbol(sym, as_of_date=None) for sym in symbols]\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(\n",
    "    columns={\n",
    "        'ROE': CustomFundamentals.ReturnOnEquity_SmartEstimat.latest,\n",
    "        'Market_Cap': CustomFundamentals.CompanyMarketCap.latest,\n",
    "        'Price': CustomFundamentals.RefPriceClose.latest,\n",
    "        'Growth': CustomFundamentals.LongTermGrowth_Mean.latest,\n",
    "        'EV_EBITDA': CustomFundamentals.EnterpriseValueToEBITDA_DailyTimeSeriesRatio_.latest,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Run over a date range (last 2 months of available data)\n",
    "# Get valid trading sessions from the calendar\n",
    "# Note: sessions_in_range expects timezone-naive dates at midnight\n",
    "end_search = pd.Timestamp.now().normalize()\n",
    "start_search = (end_search - pd.DateOffset(months=3)).normalize()\n",
    "\n",
    "sessions = trading_calendar.sessions_in_range(start_search, end_search)\n",
    "end_date = sessions[-5]  # Use 5 days back from the end\n",
    "start_date = (end_date - pd.DateOffset(months=2)).normalize()\n",
    "\n",
    "# Ensure start_date is a valid trading session\n",
    "start_date = trading_calendar.sessions_in_range(start_date, end_date)[0]\n",
    "\n",
    "print(f\"Date range: {start_date.date()} to {end_date.date()}\")\n",
    "\n",
    "result = engine.run_pipeline(pipeline, start_date, end_date)\n",
    "\n",
    "# Normalize dataframe to fix pandas 2.x dtype issues\n",
    "result = result.copy()\n",
    "for col in result.columns:\n",
    "    if result[col].dtype == object:\n",
    "        try:\n",
    "            result[col] = result[col].astype(float)\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "\n",
    "print(f\"\u2713 Time series data extracted\")\n",
    "print(f\"  Period: {start_date.date()} to {end_date.date()}\")\n",
    "print(f\"  Total observations: {len(result):,}\")\n",
    "\n",
    "# Filter to our symbols of interest\n",
    "symbol_data = result[result.index.get_level_values(1).isin(assets)]\n",
    "\n",
    "print(f\"  Observations for {symbols}: {len(symbol_data):,}\")\n",
    "\n",
    "# Show AAPL time series\n",
    "aapl_asset = assets[0]\n",
    "aapl_data = symbol_data.loc[pd.IndexSlice[:, aapl_asset], :]\n",
    "\n",
    "print(f\"\\nAAPL Fundamental Trends (last 10 observations):\")\n",
    "print(aapl_data.tail(10)[['ROE', 'Market_Cap', 'Price', 'Growth']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: Visualize Time Series - Plot Fundamental Trends\n",
    "\n",
    "Create charts to visualize how fundamentals change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Define symbols to track (re-define in case previous cell wasn't run)\n",
    "symbols = ['AAPL', 'MSFT', 'GOOGL']\n",
    "assets = [asset_finder.lookup_symbol(sym, as_of_date=None) for sym in symbols]\n",
    "\n",
    "# Create pipeline if not already created in previous cell\n",
    "if 'symbol_data' not in locals():\n",
    "    print(\"Fetching time series data...\")\n",
    "    pipeline = Pipeline(\n",
    "        columns={\n",
    "            'ROE': CustomFundamentals.ReturnOnEquity_SmartEstimat.latest,\n",
    "            'Market_Cap': CustomFundamentals.CompanyMarketCap.latest,\n",
    "            'Price': CustomFundamentals.RefPriceClose.latest,\n",
    "            'Growth': CustomFundamentals.LongTermGrowth_Mean.latest,\n",
    "            'EV_EBITDA': CustomFundamentals.EnterpriseValueToEBITDA_DailyTimeSeriesRatio_.latest,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Get valid trading sessions (last 3 months, timezone-naive at midnight)\n",
    "    end_search = pd.Timestamp.now().normalize()\n",
    "    start_search = (end_search - pd.DateOffset(months=3)).normalize()\n",
    "    \n",
    "    sessions = trading_calendar.sessions_in_range(start_search, end_search)\n",
    "    end_date = sessions[-5]\n",
    "    start_date = (end_date - pd.DateOffset(months=2)).normalize()\n",
    "    start_date = trading_calendar.sessions_in_range(start_date, end_date)[0]\n",
    "    \n",
    "    result = engine.run_pipeline(pipeline, start_date, end_date)\n",
    "\n",
    "# Normalize dataframe to fix pandas 2.x dtype issues\n",
    "result = result.copy()\n",
    "for col in result.columns:\n",
    "    if result[col].dtype == object:\n",
    "        try:\n",
    "            result[col] = result[col].astype(float)\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "    symbol_data = result[result.index.get_level_values(1).isin(assets)]\n",
    "    print(f\"\u2713 Fetched {len(symbol_data):,} observations\")\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Fundamental Trends: AAPL, MSFT, GOOGL', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Prepare data for each symbol\n",
    "symbol_colors = {'AAPL': 'blue', 'MSFT': 'green', 'GOOGL': 'red'}\n",
    "\n",
    "for idx, (symbol, asset) in enumerate(zip(symbols, assets)):\n",
    "    sym_data = symbol_data.loc[pd.IndexSlice[:, asset], :]\n",
    "    sym_data = sym_data.reset_index(names=['date', 'asset'])\n",
    "    \n",
    "    color = symbol_colors[symbol]\n",
    "    \n",
    "    # Plot 1: ROE over time\n",
    "    axes[0, 0].plot(sym_data['date'], sym_data['ROE'], \n",
    "                    label=symbol, marker='o', color=color, alpha=0.7)\n",
    "    \n",
    "    # Plot 2: Market Cap over time\n",
    "    axes[0, 1].plot(sym_data['date'], sym_data['Market_Cap'] / 1e9, \n",
    "                    label=symbol, marker='s', color=color, alpha=0.7)\n",
    "    \n",
    "    # Plot 3: Growth Rate over time\n",
    "    axes[1, 0].plot(sym_data['date'], sym_data['Growth'], \n",
    "                    label=symbol, marker='^', color=color, alpha=0.7)\n",
    "    \n",
    "    # Plot 4: EV/EBITDA over time\n",
    "    axes[1, 1].plot(sym_data['date'], sym_data['EV_EBITDA'], \n",
    "                    label=symbol, marker='D', color=color, alpha=0.7)\n",
    "\n",
    "# Customize subplots\n",
    "axes[0, 0].set_title('Return on Equity (%)', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('ROE (%)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].set_title('Market Capitalization', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Market Cap ($B)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].set_title('Long-term Growth Rate', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Growth (%)')\n",
    "axes[1, 0].set_xlabel('Date')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].set_title('Enterprise Value / EBITDA', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('EV/EBITDA Ratio')\n",
    "axes[1, 1].set_xlabel('Date')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Format x-axis dates\n",
    "for ax in axes.flat:\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2713 Fundamental trends visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Examples Summary\n",
    "\n",
    "You now know how to:\n",
    "- \u2705 Set up a Pipeline engine with custom fundamentals\n",
    "- \u2705 Query latest fundamentals for all stocks\n",
    "- \u2705 Filter stocks using fundamental criteria\n",
    "- \u2705 Extract time series data for specific symbols\n",
    "- \u2705 Visualize fundamental trends over time\n",
    "\n",
    "**Key takeaways:**\n",
    "- Use `CustomFundamentals.ColumnName.latest` to access any fundamental metric\n",
    "- Combine multiple metrics with boolean operators (`&`, `|`) for screening\n",
    "- Run pipelines over date ranges to analyze trends\n",
    "- Filter results by asset to focus on specific symbols\n",
    "- Integrate with matplotlib for visualization\n",
    "\n",
    "**Next steps:**\n",
    "- Use these patterns in your backtesting algorithms\n",
    "- Create custom factors combining multiple fundamentals\n",
    "- Integrate with price data from USEquityPricing\n",
    "- Build sophisticated stock selection strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Running Backtests with Custom Fundamentals\n",
    "\n",
    "### Using the Strategy File\n",
    "\n",
    "I've created a working strategy file: **`strategy_top5_roe.py`**\n",
    "\n",
    "This strategy:\n",
    "- \u2705 Uses your custom Custom fundamentals\n",
    "- \u2705 Filters to top 100 stocks by market cap\n",
    "- \u2705 Selects top 5 stocks by ROE\n",
    "- \u2705 Rebalances weekly (every Monday)\n",
    "- \u2705 Equal weights (20% each)\n",
    "\n",
    "### How to Run\n",
    "\n",
    "**From terminal/command line:**\n",
    "```bash\n",
    "cd /data/backtest_results\n",
    "python strategy_top5_roe.py\n",
    "```\n",
    "\n",
    "**From Jupyter:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the strategy\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "result = subprocess.run(\n",
    "    [sys.executable, 'strategy_top5_roe.py'],\n",
    "    cwd='/app/examples/strategies',\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load saved results\n",
    "if not os.path.exists(\"/data/backtest_results/backtest_results.pkl\"):\n",
    "    print(\"\u26a0\ufe0f  Backtest results not found at /data/backtest_results/backtest_results.pkl\")\n",
    "    print(\"Please run the backtest in the previous cell first.\")\n",
    "else:\n",
    "    results = pd.read_pickle('/data/backtest_results/backtest_results.pkl')\n",
    "\n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "    # Portfolio value\n",
    "    axes[0].plot(results.index, results['portfolio_value'], linewidth=2)\n",
    "    axes[0].set_ylabel('Portfolio Value ($)', fontsize=12)\n",
    "    axes[0].set_title('Top 5 ROE Strategy Performance', fontsize=14, fontweight='bold')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Number of positions\n",
    "    axes[1].plot(results.index, results['num_positions'], linewidth=2, color='orange')\n",
    "    axes[1].set_ylabel('Number of Positions', fontsize=12)\n",
    "    axes[1].set_xlabel('Date', fontsize=12)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nPerformance Summary:\")\n",
    "    print(f\"  Final Value: ${results.portfolio_value.iloc[-1]:,.2f}\")\n",
    "    print(f\"  Total Return: {(results.portfolio_value.iloc[-1]/100000-1)*100:.2f}%\")\n",
    "    print(f\"  Avg Positions: {results.num_positions.mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze with pyfolio\n",
    "import pyfolio as pf\n",
    "\n",
    "returns = results.returns\n",
    "pf.create_simple_tear_sheet(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing the Strategy\n",
    "\n",
    "Edit `strategy_top5_roe.py` to:\n",
    "\n",
    "**Change selection criteria:**\n",
    "```python\n",
    "# Select top 10 instead of top 5\n",
    "top_10_roe = roe.top(10, mask=top_100_by_mcap)\n",
    "\n",
    "# Use different fundamentals\n",
    "roa = CustomFundamentals.ReturnOnAssets_SmartEstimate.latest\n",
    "growth = CustomFundamentals.LongTermGrowth_Mean.latest\n",
    "\n",
    "# Combine multiple factors\n",
    "roe_z = roe.zscore(mask=top_100_by_mcap)\n",
    "growth_z = growth.zscore(mask=top_100_by_mcap)\n",
    "combined = (roe_z + growth_z) / 2\n",
    "top_stocks = combined.top(5, mask=top_100_by_mcap)\n",
    "```\n",
    "\n",
    "**Change rebalancing frequency:**\n",
    "```python\n",
    "# Monthly rebalancing\n",
    "schedule_function(\n",
    "    rebalance,\n",
    "    date_rules.month_start(),\n",
    "    time_rules.market_open(hours=1),\n",
    ")\n",
    "\n",
    "# Daily rebalancing\n",
    "schedule_function(\n",
    "    rebalance,\n",
    "    date_rules.every_day(),\n",
    "    time_rules.market_open(hours=1),\n",
    ")\n",
    "```\n",
    "\n",
    "**Change date range:**\n",
    "```python\n",
    "start = pd.Timestamp('2023-01-01', tz='UTC')\n",
    "end = pd.Timestamp('2024-12-31', tz='UTC')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}