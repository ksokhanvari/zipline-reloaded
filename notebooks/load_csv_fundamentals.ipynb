{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CSV Fundamental Data into Zipline Custom Database\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load fundamental data from CSV files\n",
    "2. Map symbols to Zipline SIDs\n",
    "3. Create a custom SQLite database\n",
    "4. Use the data in Zipline Pipeline\n",
    "\n",
    "This is a zipline-reloaded native approach (no QuantRocket dependencies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport glob\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\n# Zipline imports\nfrom zipline.data.bundles import load as load_bundle, register\nfrom zipline.data.bundles.sharadar_bundle import sharadar_bundle\nfrom zipline.pipeline import Pipeline\nfrom zipline.pipeline.data.db import Database, Column\n\n# Register Sharadar bundle (in case extension.py didn't load)\ntry:\n    # Try to register the bundle\n    register(\n        'sharadar',\n        sharadar_bundle(\n            tickers=None,\n            incremental=True,\n            include_funds=True,\n        ),\n    )\n    print(\"âœ“ Registered Sharadar bundle\")\nexcept Exception as e:\n    # Bundle may already be registered\n    print(f\"âœ“ Sharadar bundle already registered (or error: {e})\")\n\n# Display settings\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 50)\npd.set_option('display.width', 120)\n\nprint(\"âœ“ Imports complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set your database name and data directory paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nDATABASE_NAME = \"refe-fundamentals\"  # Name for your custom database\nDATA_DIR = \"/data/csv/\"  # Directory with CSV files (persistent across Docker restarts)\nVIX_SIGNAL_PATH = \"/data/csv/vix_flag.csv\"  # Optional VIX signal data\n\n# Database will be created in ~/.zipline/data/custom/\nDB_DIR = Path.home() / '.zipline' / 'data' / 'custom'\nDB_DIR.mkdir(parents=True, exist_ok=True)\nDB_PATH = DB_DIR / f\"{DATABASE_NAME}.sqlite\"\n\n# Database update mode:\n# 'fresh' - Drop and recreate database (default)\n# 'replace' - Insert or replace existing records (updates duplicates)\n# 'ignore' - Insert or ignore (skips duplicates, keeps existing data)\nUPDATE_MODE = 'replace'  # Change to 'fresh', 'replace', or 'ignore'\n\nprint(f\"Database will be created at: {DB_PATH}\")\nprint(f\"Update mode: {UPDATE_MODE}\")\nprint(f\"  - 'fresh': Drop and recreate database\")\nprint(f\"  - 'replace': Update existing records with new data\")\nprint(f\"  - 'ignore': Skip records that already exist\")\nprint(f\"\\nLooking for CSV files in: {DATA_DIR}\")\nprint(f\"\\nðŸ’¡ Tip: Place your CSV files in /data/csv/ (inside container)\")\nprint(f\"   or ./data/csv/ (on host machine) for persistent storage\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Database Schema\n",
    "\n",
    "Define the columns that will be in your custom database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your database schema\n",
    "# This matches the columns from the QuantRocket example\n",
    "SCHEMA = {\n",
    "    'Symbol': 'TEXT',\n",
    "    'Sid': 'INTEGER',\n",
    "    'Date': 'TEXT',\n",
    "    'RefPriceClose': 'REAL',\n",
    "    'RefVolume': 'REAL',\n",
    "    'CompanyCommonName': 'TEXT',\n",
    "    'EnterpriseValue_DailyTimeSeries_': 'REAL',\n",
    "    'CompanyMarketCap': 'REAL',\n",
    "    'GICSSectorName': 'TEXT',\n",
    "    'FOCFExDividends_Discrete': 'REAL',\n",
    "    'InterestExpense_NetofCapitalizedInterest': 'REAL',\n",
    "    'Debt_Total': 'REAL',\n",
    "    'EarningsPerShare_Actual': 'REAL',\n",
    "    'EarningsPerShare_SmartEstimate_prev_Q': 'REAL',\n",
    "    'EarningsPerShare_ActualSurprise': 'REAL',\n",
    "    'EarningsPerShare_SmartEstimate_current_Q': 'REAL',\n",
    "    'LongTermGrowth_Mean': 'REAL',\n",
    "    'PriceTarget_Median': 'REAL',\n",
    "    'CombinedAlphaModelSectorRank': 'REAL',\n",
    "    'CombinedAlphaModelSectorRankChange': 'REAL',\n",
    "    'CombinedAlphaModelRegionRank': 'REAL',\n",
    "    'TradeDate': 'TEXT',\n",
    "    'EPS_SurpirsePrct_prev_Q': 'REAL',\n",
    "    'Estpricegrowth_percent': 'REAL',\n",
    "    'CashFlowComponent_Current': 'REAL',\n",
    "    'EarningsQualityRegionRank_Current': 'REAL',\n",
    "    'EnterpriseValueToEBIT_DailyTimeSeriesRatio_': 'REAL',\n",
    "    'EnterpriseValueToEBITDA_DailyTimeSeriesRatio_': 'REAL',\n",
    "    'EnterpriseValueToSales_DailyTimeSeriesRatio_': 'REAL',\n",
    "    'Dividend_Per_Share_SmartEstimate': 'REAL',\n",
    "    'CashFlowPerShare_BrokerEstimate': 'REAL',\n",
    "    'FreeCashFlowPerShare_BrokerEstimate': 'REAL',\n",
    "    'ForwardPEG_DailyTimeSeriesRatio_': 'REAL',\n",
    "    'PriceEarningsToGrowthRatio_SmartEstimate_': 'REAL',\n",
    "    'ReturnOnInvestedCapital_BrokerEstimate': 'REAL',\n",
    "    'Recommendation_NumberOfTotal': 'REAL',\n",
    "    'Recommendation_Median_1_5_': 'REAL',\n",
    "    'Recommendation_NumberOfStrongBuy': 'REAL',\n",
    "    'Recommendation_NumberOfBuy': 'REAL',\n",
    "    'Recommendation_Mean_1_5_': 'REAL',\n",
    "    'ReturnOnCapitalEmployed_Actual': 'REAL',\n",
    "    'GrossProfitMargin_': 'REAL',\n",
    "    'ReturnOnEquity_SmartEstimat': 'REAL',\n",
    "    'ReturnOnAssets_SmartEstimate': 'REAL',\n",
    "    'CashCashEquivalents_Total': 'REAL',\n",
    "    'ForwardPriceToCashFlowPerShare_DailyTimeSeriesRatio_': 'REAL',\n",
    "    'ForwardPriceToSalesPerShare_DailyTimeSeriesRatio_': 'REAL',\n",
    "    'ForwardEnterpriseValueToOperatingCashFlow_DailyTimeSeriesRatio_': 'REAL',\n",
    "    'GrossProfitMargin_ActualSurprise': 'REAL',\n",
    "    'pred': 'REAL',  # VIX signal\n",
    "}\n",
    "\n",
    "print(f\"âœ“ Schema defined with {len(SCHEMA)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load CSV Files\n",
    "\n",
    "Load all CSV files from the data directory and concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all CSV files\n",
    "os.chdir(DATA_DIR)\n",
    "csv_files = sorted(glob.glob('*.csv'))\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV files:\")\n",
    "for f in csv_files[:5]:  # Show first 5\n",
    "    print(f\"  - {f}\")\n",
    "if len(csv_files) > 5:\n",
    "    print(f\"  ... and {len(csv_files) - 5} more\")\n",
    "\n",
    "# Load and concatenate all CSV files\n",
    "print(\"\\nLoading CSV files...\")\n",
    "custom_data = pd.DataFrame()\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    print(f\"  Loading {csv_file}...\")\n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, csv_file))\n",
    "    custom_data = pd.concat([custom_data, df], ignore_index=True)\n",
    "\n",
    "print(f\"\\nâœ“ Loaded {len(custom_data):,} total rows\")\n",
    "print(f\"Date range: {custom_data['Date'].min()} to {custom_data['Date'].max()}\")\n",
    "print(f\"Unique symbols: {custom_data['Symbol'].nunique()}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample data:\")\n",
    "custom_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optional: Load Recent Data Only\n",
    "\n",
    "To reduce memory usage, you can filter to recent data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Keep only recent data (e.g., last 600,000 rows)\n",
    "# Comment out if you want all historical data\n",
    "RECENT_ROWS = 600000\n",
    "\n",
    "if len(custom_data) > RECENT_ROWS:\n",
    "    print(f\"Filtering to most recent {RECENT_ROWS:,} rows...\")\n",
    "    custom_data = custom_data.tail(RECENT_ROWS).copy()\n",
    "    print(f\"âœ“ Filtered. New date range: {custom_data['Date'].min()} to {custom_data['Date'].max()}\")\n",
    "else:\n",
    "    print(f\"Dataset has {len(custom_data):,} rows - no filtering needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Map Symbols to Zipline SIDs\n",
    "\n",
    "Map your symbols to Zipline Security IDs (SIDs) using the asset finder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the Sharadar bundle to get the asset finder\nprint(\"Loading Sharadar bundle...\")\n\n# Load bundle with current timestamp\n# This gives us access to the asset database\nbundle_timestamp = pd.Timestamp.now(tz='UTC')\nbundle_data = load_bundle('sharadar', timestamp=bundle_timestamp)\nasset_finder = bundle_data.asset_finder\n\n# Get all equities\nprint(\"Mapping symbols to SIDs...\")\nall_assets = asset_finder.retrieve_all(asset_finder.sids)\n\n# Create symbol -> sid mapping\nsymbol_to_sid = {}\nfor asset in all_assets:\n    if hasattr(asset, 'symbol'):\n        symbol_to_sid[asset.symbol] = asset.sid\n\nprint(f\"âœ“ Found {len(symbol_to_sid):,} symbols in bundle\")\n\n# Map SIDs to your data\ncustom_data['Sid'] = custom_data['Symbol'].map(symbol_to_sid)\n\n# Check mapping success\nmapped = custom_data['Sid'].notna().sum()\nunmapped = custom_data['Sid'].isna().sum()\n\nprint(f\"\\nMapping results:\")\nprint(f\"  Mapped: {mapped:,} rows ({mapped/len(custom_data)*100:.1f}%)\")\nprint(f\"  Unmapped: {unmapped:,} rows ({unmapped/len(custom_data)*100:.1f}%)\")\n\nif unmapped > 0:\n    unmapped_symbols = custom_data[custom_data['Sid'].isna()]['Symbol'].unique()\n    print(f\"\\n  Unmapped symbols (first 10): {list(unmapped_symbols[:10])}\")\n    print(f\"  Tip: These symbols may not be in the Sharadar bundle\")\n\n# Remove unmapped rows\ncustom_data = custom_data[custom_data['Sid'].notna()].copy()\nprint(f\"\\nâœ“ Kept {len(custom_data):,} mapped rows\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Merge VIX Signal Data (Optional)\n",
    "\n",
    "If you have additional data like VIX signals, merge it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VIX signal data if available\n",
    "if os.path.exists(VIX_SIGNAL_PATH):\n",
    "    print(f\"Loading VIX signal from {VIX_SIGNAL_PATH}...\")\n",
    "    vix_signal = pd.read_csv(VIX_SIGNAL_PATH)\n",
    "    \n",
    "    # Standardize column names\n",
    "    vix_signal.rename(columns={'symbol': 'Symbol', 'date': 'Date'}, inplace=True)\n",
    "    vix_signal['Date'] = pd.to_datetime(vix_signal['Date'])\n",
    "    \n",
    "    # Merge with custom data\n",
    "    custom_data['Date'] = pd.to_datetime(custom_data['Date'])\n",
    "    custom_data = pd.merge(custom_data, vix_signal[['Symbol', 'Date', 'pred']], \n",
    "                          on=['Symbol', 'Date'], how='left')\n",
    "    \n",
    "    print(f\"âœ“ Merged VIX signal data\")\n",
    "else:\n",
    "    print(f\"VIX signal file not found at {VIX_SIGNAL_PATH}\")\n",
    "    print(\"Skipping VIX merge (this is optional)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Cleaning\n",
    "\n",
    "Clean and prepare data for database insertion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleaning data...\")\n",
    "\n",
    "# Ensure Date is datetime\n",
    "custom_data['Date'] = pd.to_datetime(custom_data['Date'])\n",
    "\n",
    "# Forward fill missing values by symbol\n",
    "print(\"  Forward filling missing values by symbol...\")\n",
    "for col in custom_data.columns:\n",
    "    if col not in ['Symbol', 'Sid', 'Date']:\n",
    "        custom_data[col] = custom_data.groupby('Symbol')[col].transform(lambda x: x.ffill())\n",
    "\n",
    "# Handle sector - fill empty strings instead of NaN\n",
    "custom_data['GICSSectorName'] = custom_data['GICSSectorName'].fillna('')\n",
    "\n",
    "# Fill remaining NaNs with 0 (for numeric columns)\n",
    "print(\"  Filling remaining NaN values...\")\n",
    "custom_data = custom_data.fillna(0)\n",
    "\n",
    "# Convert Sid to integer\n",
    "custom_data['Sid'] = custom_data['Sid'].astype(int)\n",
    "\n",
    "# Sort by date and symbol\n",
    "custom_data = custom_data.sort_values(['Date', 'Symbol'])\n",
    "\n",
    "print(f\"âœ“ Data cleaned\")\n",
    "print(f\"\\nFinal dataset:\")\n",
    "print(f\"  Rows: {len(custom_data):,}\")\n",
    "print(f\"  Columns: {len(custom_data.columns)}\")\n",
    "print(f\"  Date range: {custom_data['Date'].min()} to {custom_data['Date'].max()}\")\n",
    "print(f\"  Symbols: {custom_data['Symbol'].nunique()}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample cleaned data:\")\n",
    "custom_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Create SQLite Database\n\nCreate the custom SQLite database in Zipline format.\n\nThe notebook supports three update modes (configured in Cell 2):\n- **`fresh`**: Drop and recreate the database (default for initial load)\n- **`replace`**: INSERT OR REPLACE - Updates existing records based on (Sid, Date) key\n- **`ignore`**: INSERT OR IGNORE - Skips records that already exist, keeps existing data\n\nUse `replace` mode to update data with newer values, or `ignore` mode to only add new data without overwriting existing records."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"Creating database at {DB_PATH}...\")\n\n# Handle database based on update mode\ndb_exists = DB_PATH.exists()\nif UPDATE_MODE == 'fresh' and db_exists:\n    print(f\"  Removing existing database (mode='fresh')...\")\n    DB_PATH.unlink()\n    db_exists = False\nelif db_exists:\n    print(f\"  Database exists - will {UPDATE_MODE} existing records...\")\n\n# Create database connection\nconn = sqlite3.connect(str(DB_PATH))\ncursor = conn.cursor()\n\n# Create table if it doesn't exist (with UNIQUE constraint for upserts)\nif not db_exists or UPDATE_MODE == 'fresh':\n    columns_def = ', '.join([f'\"{col}\" {dtype}' for col, dtype in SCHEMA.items()])\n    create_table_sql = f'''\n    CREATE TABLE IF NOT EXISTS fundamentals (\n        {columns_def},\n        UNIQUE(Sid, Date)\n    );\n    '''\n    \n    print(\"  Creating table...\")\n    cursor.execute(create_table_sql)\n    \n    # Create indices for fast lookups\n    print(\"  Creating indices...\")\n    cursor.execute('CREATE INDEX IF NOT EXISTS idx_sid ON fundamentals(Sid);')\n    cursor.execute('CREATE INDEX IF NOT EXISTS idx_date ON fundamentals(Date);')\n    cursor.execute('CREATE INDEX IF NOT EXISTS idx_symbol ON fundamentals(Symbol);')\n\n# Insert data\nprint(f\"  Inserting {len(custom_data):,} rows with mode='{UPDATE_MODE}'...\")\n\n# Prepare data for insertion - only use columns that exist in custom_data\n# Add missing columns with default values (0 for numeric, empty string for text)\ninsert_data = custom_data.copy()\n\n# Add any missing schema columns with appropriate defaults\nmissing_cols = []\nfor col, dtype in SCHEMA.items():\n    if col not in insert_data.columns:\n        if dtype == 'TEXT':\n            insert_data[col] = ''\n        else:  # REAL or INTEGER\n            insert_data[col] = 0\n        missing_cols.append(col)\n\nif missing_cols:\n    print(f\"  Added {len(missing_cols)} missing columns with default values\")\n\n# Select only the columns in the schema (in the correct order)\ninsert_data = insert_data[list(SCHEMA.keys())].copy()\n\n# Convert Date to string format for SQLite\ninsert_data['Date'] = insert_data['Date'].dt.strftime('%Y-%m-%d')\n\n# Choose SQL command based on update mode\nif UPDATE_MODE == 'replace':\n    sql_command = 'INSERT OR REPLACE'\nelif UPDATE_MODE == 'ignore':\n    sql_command = 'INSERT OR IGNORE'\nelse:  # 'fresh' or default\n    sql_command = 'INSERT'\n\n# Create parameterized INSERT statement\ncolumns = list(SCHEMA.keys())\nplaceholders = ', '.join(['?' for _ in columns])\ncolumn_names = ', '.join([f'\"{col}\"' for col in columns])\ninsert_sql = f'{sql_command} INTO fundamentals ({column_names}) VALUES ({placeholders})'\n\n# Insert in chunks for better performance\nchunk_size = 10000\ntotal_chunks = (len(insert_data) + chunk_size - 1) // chunk_size\ntotal_inserted = 0\ntotal_skipped = 0\n\nfor i in range(0, len(insert_data), chunk_size):\n    chunk = insert_data.iloc[i:i+chunk_size]\n    \n    # Execute batch insert\n    cursor.executemany(insert_sql, chunk.values.tolist())\n    \n    rows_affected = cursor.rowcount\n    if UPDATE_MODE == 'ignore':\n        # With INSERT OR IGNORE, rowcount shows actual inserts (not skipped)\n        total_inserted += rows_affected\n        total_skipped += len(chunk) - rows_affected\n    else:\n        total_inserted += rows_affected\n    \n    chunk_num = i // chunk_size + 1\n    if chunk_num % 10 == 0 or chunk_num == total_chunks:\n        print(f\"    Processed chunk {chunk_num}/{total_chunks} ({i+len(chunk):,} rows)...\")\n\nconn.commit()\n\n# Report results\nprint(f\"\\nâœ“ Database operation completed!\")\nprint(f\"  Path: {DB_PATH}\")\nprint(f\"  Mode: {UPDATE_MODE}\")\nprint(f\"  Rows processed: {len(insert_data):,}\")\nif UPDATE_MODE == 'ignore' and total_skipped > 0:\n    print(f\"  Rows inserted: {total_inserted:,}\")\n    print(f\"  Rows skipped (already existed): {total_skipped:,}\")\n\n# Get final row count\ncursor.execute(\"SELECT COUNT(*) FROM fundamentals\")\ntotal_rows = cursor.fetchone()[0]\nprint(f\"  Total rows in database: {total_rows:,}\")\n\nconn.close()\n\nprint(f\"  Size: {DB_PATH.stat().st_size / 1024 / 1024:.1f} MB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Define Database Class\n",
    "\n",
    "Create a Database class to use this data in Zipline Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Database class\n",
    "class REFEFundamentals(Database):\n",
    "    \"\"\"\n",
    "    Custom REFE Fundamentals database.\n",
    "    \n",
    "    Usage in Pipeline:\n",
    "        roe = REFEFundamentals.ReturnOnEquity_SmartEstimat.latest\n",
    "        sector = REFEFundamentals.GICSSectorName.latest\n",
    "    \"\"\"\n",
    "    \n",
    "    CODE = DATABASE_NAME\n",
    "    LOOKBACK_WINDOW = 252  # Days to look back\n",
    "    \n",
    "    # Price and volume\n",
    "    RefPriceClose = Column(float)\n",
    "    RefVolume = Column(float)\n",
    "    \n",
    "    # Company info\n",
    "    CompanyCommonName = Column(str)\n",
    "    GICSSectorName = Column(str)\n",
    "    \n",
    "    # Valuation metrics\n",
    "    EnterpriseValue_DailyTimeSeries_ = Column(float)\n",
    "    CompanyMarketCap = Column(float)\n",
    "    \n",
    "    # Cash flow\n",
    "    FOCFExDividends_Discrete = Column(float)\n",
    "    CashFlowComponent_Current = Column(float)\n",
    "    CashFlowPerShare_BrokerEstimate = Column(float)\n",
    "    FreeCashFlowPerShare_BrokerEstimate = Column(float)\n",
    "    \n",
    "    # Debt and interest\n",
    "    InterestExpense_NetofCapitalizedInterest = Column(float)\n",
    "    Debt_Total = Column(float)\n",
    "    \n",
    "    # Earnings\n",
    "    EarningsPerShare_Actual = Column(float)\n",
    "    EarningsPerShare_SmartEstimate_prev_Q = Column(float)\n",
    "    EarningsPerShare_ActualSurprise = Column(float)\n",
    "    EarningsPerShare_SmartEstimate_current_Q = Column(float)\n",
    "    EPS_SurpirsePrct_prev_Q = Column(float)\n",
    "    \n",
    "    # Growth and targets\n",
    "    LongTermGrowth_Mean = Column(float)\n",
    "    PriceTarget_Median = Column(float)\n",
    "    Estpricegrowth_percent = Column(float)\n",
    "    \n",
    "    # Rankings\n",
    "    CombinedAlphaModelSectorRank = Column(float)\n",
    "    CombinedAlphaModelSectorRankChange = Column(float)\n",
    "    CombinedAlphaModelRegionRank = Column(float)\n",
    "    EarningsQualityRegionRank_Current = Column(float)\n",
    "    \n",
    "    # Ratios\n",
    "    EnterpriseValueToEBIT_DailyTimeSeriesRatio_ = Column(float)\n",
    "    EnterpriseValueToEBITDA_DailyTimeSeriesRatio_ = Column(float)\n",
    "    EnterpriseValueToSales_DailyTimeSeriesRatio_ = Column(float)\n",
    "    ForwardPEG_DailyTimeSeriesRatio_ = Column(float)\n",
    "    PriceEarningsToGrowthRatio_SmartEstimate_ = Column(float)\n",
    "    ForwardPriceToCashFlowPerShare_DailyTimeSeriesRatio_ = Column(float)\n",
    "    ForwardPriceToSalesPerShare_DailyTimeSeriesRatio_ = Column(float)\n",
    "    ForwardEnterpriseValueToOperatingCashFlow_DailyTimeSeriesRatio_ = Column(float)\n",
    "    \n",
    "    # Returns\n",
    "    ReturnOnInvestedCapital_BrokerEstimate = Column(float)\n",
    "    ReturnOnCapitalEmployed_Actual = Column(float)\n",
    "    ReturnOnEquity_SmartEstimat = Column(float)\n",
    "    ReturnOnAssets_SmartEstimate = Column(float)\n",
    "    \n",
    "    # Margins\n",
    "    GrossProfitMargin_ = Column(float)\n",
    "    GrossProfitMargin_ActualSurprise = Column(float)\n",
    "    \n",
    "    # Analyst recommendations\n",
    "    Recommendation_NumberOfTotal = Column(float)\n",
    "    Recommendation_Median_1_5_ = Column(float)\n",
    "    Recommendation_NumberOfStrongBuy = Column(float)\n",
    "    Recommendation_NumberOfBuy = Column(float)\n",
    "    Recommendation_Mean_1_5_ = Column(float)\n",
    "    \n",
    "    # Cash\n",
    "    CashCashEquivalents_Total = Column(float)\n",
    "    \n",
    "    # Dividends\n",
    "    Dividend_Per_Share_SmartEstimate = Column(float)\n",
    "    \n",
    "    # VIX prediction signal\n",
    "    pred = Column(float)\n",
    "\n",
    "\n",
    "print(\"âœ“ REFEFundamentals Database class defined\")\n",
    "print(f\"  Database code: {REFEFundamentals.CODE}\")\n",
    "print(f\"  Lookback window: {REFEFundamentals.LOOKBACK_WINDOW} days\")\n",
    "print(f\"  Columns defined: {len([attr for attr in dir(REFEFundamentals) if isinstance(getattr(REFEFundamentals, attr), Column)])}\")\n",
    "\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"  roe = REFEFundamentals.ReturnOnEquity_SmartEstimat.latest\")\n",
    "print(\"  pe_growth = REFEFundamentals.PriceEarningsToGrowthRatio_SmartEstimate_.latest\")\n",
    "print(\"  sector = REFEFundamentals.GICSSectorName.latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Verify Database\n",
    "\n",
    "Query the database to verify data was loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect and query\n",
    "conn = sqlite3.connect(str(DB_PATH))\n",
    "\n",
    "# Get row count\n",
    "row_count = pd.read_sql(\"SELECT COUNT(*) as count FROM fundamentals\", conn).iloc[0, 0]\n",
    "print(f\"Total rows in database: {row_count:,}\")\n",
    "\n",
    "# Get date range\n",
    "date_range = pd.read_sql(\"SELECT MIN(Date) as min_date, MAX(Date) as max_date FROM fundamentals\", conn)\n",
    "print(f\"Date range: {date_range.iloc[0, 0]} to {date_range.iloc[0, 1]}\")\n",
    "\n",
    "# Get symbol count\n",
    "symbol_count = pd.read_sql(\"SELECT COUNT(DISTINCT Symbol) as count FROM fundamentals\", conn).iloc[0, 0]\n",
    "print(f\"Unique symbols: {symbol_count:,}\")\n",
    "\n",
    "# Show sample data for a specific symbol\n",
    "print(\"\\nSample data for AAPL:\")\n",
    "aapl_data = pd.read_sql(\"\"\"\n",
    "    SELECT Date, Symbol, RefPriceClose, CompanyMarketCap, \n",
    "           ReturnOnEquity_SmartEstimat, PriceTarget_Median\n",
    "    FROM fundamentals \n",
    "    WHERE Symbol = 'AAPL' \n",
    "    ORDER BY Date DESC \n",
    "    LIMIT 5\n",
    "\"\"\", conn)\n",
    "print(aapl_data)\n",
    "\n",
    "print(\"\\nSample data for IBM:\")\n",
    "ibm_data = pd.read_sql(\"\"\"\n",
    "    SELECT Date, Symbol, RefPriceClose, CompanyMarketCap, \n",
    "           ReturnOnEquity_SmartEstimat, GICSSectorName\n",
    "    FROM fundamentals \n",
    "    WHERE Symbol = 'IBM' \n",
    "    ORDER BY Date DESC \n",
    "    LIMIT 5\n",
    "\"\"\", conn)\n",
    "print(ibm_data)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"\\nâœ“ Database verification complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Usage Example\n",
    "\n",
    "Example of how to use this database in a backtest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"To use this database in your backtests:\")\n",
    "print(\"\\n1. Import the Database class:\")\n",
    "print(\"   from zipline.pipeline.data.db import Database, Column\")\n",
    "print(\"\\n2. Define the REFEFundamentals class (from cell 10 above)\")\n",
    "print(\"\\n3. Use in your pipeline:\")\n",
    "print(\"   \")\n",
    "print(\"   def make_pipeline():\")\n",
    "print(\"       roe = REFEFundamentals.ReturnOnEquity_SmartEstimat.latest\")\n",
    "print(\"       growth = REFEFundamentals.LongTermGrowth_Mean.latest\")\n",
    "print(\"       sector = REFEFundamentals.GICSSectorName.latest\")\n",
    "print(\"       \")\n",
    "print(\"       # Screen for quality companies\")\n",
    "print(\"       quality = (roe > 15) & (growth > 10)\")\n",
    "print(\"       \")\n",
    "print(\"       return Pipeline(\")\n",
    "print(\"           columns={\")\n",
    "print(\"               'ROE': roe,\")\n",
    "print(\"               'Growth': growth,\")\n",
    "print(\"               'Sector': sector,\")\n",
    "print(\"           },\")\n",
    "print(\"           screen=quality\")\n",
    "print(\"       )\")\n",
    "print(\"\\n4. The CustomSQLiteLoader will automatically load data based on REFEFundamentals.CODE\")\n",
    "\n",
    "print(\"\\nâœ“ Setup complete! Your custom fundamentals database is ready to use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nThis notebook:\n1. âœ… Loaded CSV files with fundamental data\n2. âœ… Mapped symbols to Zipline SIDs using the asset finder\n3. âœ… Cleaned and prepared the data\n4. âœ… Created a custom SQLite database in ~/.zipline/data/custom/\n5. âœ… Defined a Database class for use in Pipeline\n6. âœ… Verified the database contents\n\nThe database is now ready to use in your Zipline backtests with the CustomSQLiteLoader.\n\n**Next steps:**\n- See the examples below for using the data with Pipeline\n- Copy the REFEFundamentals class definition to your backtest algorithm\n- Use REFEFundamentals.ColumnName.latest in your pipeline\n- The backtest_helpers.py will automatically detect and load the data"
  },
  {
   "cell_type": "markdown",
   "source": "## 13. Pipeline Examples\n\nNow let's demonstrate how to query and analyze the fundamentals data using Zipline Pipeline.\n\nThese examples show:\n- Creating a pipeline with custom fundamentals\n- Running the pipeline over date ranges\n- Filtering stocks by fundamental criteria\n- Extracting time series data for specific symbols\n- Combining multiple fundamental factors",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Pipeline Examples\n",
    "\n",
    "Now let's demonstrate how to query and analyze the fundamentals data using Zipline Pipeline.\n",
    "\n",
    "These examples show:\n",
    "- Creating a pipeline with custom fundamentals\n",
    "- Running the pipeline over date ranges\n",
    "- Filtering stocks by fundamental criteria\n",
    "- Extracting time series data for specific symbols\n",
    "- Combining multiple fundamental factors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Example 1: Setup Pipeline Engine\n\nFirst, we need to set up the Pipeline engine to load our custom data.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from zipline.pipeline import Pipeline\nfrom zipline.pipeline.engine import SimplePipelineEngine\nfrom zipline.pipeline.loaders import USEquityPricingLoader\nfrom zipline.pipeline.data import USEquityPricing\nfrom zipline.pipeline.domain import US_EQUITIES\nfrom zipline.utils.calendar_utils import get_calendar\n\n# Import the custom loader from zipline\nfrom zipline.data.custom import CustomSQLiteLoader\n\n# Get the trading calendar\ntrading_calendar = get_calendar('NYSE')\n\n# Set up the pipeline engine with our custom loaders\ndef get_pipeline_loader(column):\n    \"\"\"\n    Pipeline loader factory that routes columns to appropriate loaders.\n    \"\"\"\n    # Route custom fundamentals to CustomSQLiteLoader\n    # Check by database CODE since datasets get bound to domains\n    if hasattr(column.dataset, 'CODE') and column.dataset.CODE == REFEFundamentals.CODE:\n        # Pass the database CODE to the loader\n        return CustomSQLiteLoader(REFEFundamentals.CODE)\n    \n    # Route pricing data to bundle\n    if column in USEquityPricing.columns:\n        return USEquityPricingLoader(bundle_data.equity_daily_bar_reader, bundle_data.adjustment_reader)\n    \n    raise ValueError(f\"No loader for {column}\")\n\n# Create the pipeline engine\nengine = SimplePipelineEngine(\n    get_loader=get_pipeline_loader,\n    asset_finder=asset_finder,\n    default_domain=US_EQUITIES,\n)\n\nprint(\"âœ“ Pipeline engine configured with custom fundamentals loader\")\nprint(f\"  Trading calendar: {trading_calendar.name}\")\nprint(f\"  Asset finder: {len(asset_finder.sids):,} securities\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Example 2: Basic Pipeline - Get Latest Fundamentals\n\nCreate a simple pipeline to get the latest fundamentals for all stocks.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Define a simple pipeline\ndef make_basic_pipeline():\n    \"\"\"\n    Get latest fundamentals for all stocks.\n    \"\"\"\n    return Pipeline(\n        columns={\n            'ROE': REFEFundamentals.ReturnOnEquity_SmartEstimat.latest,\n            'ROA': REFEFundamentals.ReturnOnAssets_SmartEstimate.latest,\n            'Market_Cap': REFEFundamentals.CompanyMarketCap.latest,\n            'Price': REFEFundamentals.RefPriceClose.latest,\n            'Sector': REFEFundamentals.GICSSectorName.latest,\n            'EV_to_EBITDA': REFEFundamentals.EnterpriseValueToEBITDA_DailyTimeSeriesRatio_.latest,\n        },\n    )\n\n# Run the pipeline for a single date\n# Get a recent valid trading session from the bundle (last 3 months)\npipeline = make_basic_pipeline()\n\n# Use recent trading sessions (last 3 months of data)\n# Note: sessions_in_range expects timezone-naive dates\nend_search = pd.Timestamp.now()\nstart_search = end_search - pd.DateOffset(months=3)\n\nsessions = trading_calendar.sessions_in_range(\n    start_search.tz_localize(None) if start_search.tz else start_search,\n    end_search.tz_localize(None) if end_search.tz else end_search\n)\nstart_date = sessions[-5]  # Use 5 days back from the end\nend_date = start_date\n\nprint(f\"Using date: {start_date.date()}\")\n\nresult = engine.run_pipeline(pipeline, start_date, end_date)\n\nprint(f\"âœ“ Pipeline run complete\")\nprint(f\"  Date: {start_date.date()}\")\nprint(f\"  Stocks: {len(result):,}\")\nprint(f\"\\nTop 10 stocks by ROE:\")\nprint(result.nlargest(10, 'ROE')[['ROE', 'ROA', 'Market_Cap', 'Sector']])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Example 3: Filtered Pipeline - Quality Stocks\n\nFilter stocks based on fundamental criteria (e.g., high ROE, profitable, large cap).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def make_quality_pipeline():\n    \"\"\"\n    Screen for quality stocks with strong fundamentals.\n    \"\"\"\n    # Get fundamentals\n    roe = REFEFundamentals.ReturnOnEquity_SmartEstimat.latest\n    roa = REFEFundamentals.ReturnOnAssets_SmartEstimate.latest\n    market_cap = REFEFundamentals.CompanyMarketCap.latest\n    growth = REFEFundamentals.LongTermGrowth_Mean.latest\n    price_target = REFEFundamentals.PriceTarget_Median.latest\n    current_price = REFEFundamentals.RefPriceClose.latest\n    sector = REFEFundamentals.GICSSectorName.latest\n    \n    # Calculate upside potential\n    upside = ((price_target - current_price) / current_price) * 100\n    \n    # Define quality screen\n    quality_screen = (\n        (roe > 15) &  # Strong return on equity\n        (roa > 5) &   # Profitable\n        (market_cap > 1_000_000_000) &  # Large cap ($1B+)\n        (growth > 10) &  # Double-digit growth\n        (upside > 10)  # At least 10% upside\n    )\n    \n    return Pipeline(\n        columns={\n            'ROE': roe,\n            'ROA': roa,\n            'Market_Cap': market_cap,\n            'Growth': growth,\n            'Price': current_price,\n            'Target': price_target,\n            'Upside_%': upside,\n            'Sector': sector,\n        },\n        screen=quality_screen,\n    )\n\n# Run the filtered pipeline\n# Use a recent valid trading session (last 3 months)\n# Note: sessions_in_range expects timezone-naive dates\nend_search = pd.Timestamp.now()\nstart_search = end_search - pd.DateOffset(months=3)\n\nsessions = trading_calendar.sessions_in_range(\n    start_search.tz_localize(None) if start_search.tz else start_search,\n    end_search.tz_localize(None) if end_search.tz else end_search\n)\nstart_date = sessions[-5]  # Use 5 days back from the end\n\npipeline = make_quality_pipeline()\nresult = engine.run_pipeline(pipeline, start_date, start_date)\n\nprint(f\"âœ“ Quality screen results:\")\nprint(f\"  Date: {start_date.date()}\")\nprint(f\"  Stocks passing screen: {len(result)}\")\nprint(f\"\\nTop 10 by upside potential:\")\nprint(result.nlargest(10, 'Upside_%')[['ROE', 'Growth', 'Price', 'Target', 'Upside_%', 'Sector']])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Example 4: Time Series Data - Track Fundamentals Over Time\n\nGet historical fundamental data for specific symbols to analyze trends.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Define symbols to track\nsymbols = ['AAPL', 'MSFT', 'GOOGL']\n\n# Get the assets\nassets = [asset_finder.lookup_symbol(sym, as_of_date=None) for sym in symbols]\n\n# Create pipeline\npipeline = Pipeline(\n    columns={\n        'ROE': REFEFundamentals.ReturnOnEquity_SmartEstimat.latest,\n        'Market_Cap': REFEFundamentals.CompanyMarketCap.latest,\n        'Price': REFEFundamentals.RefPriceClose.latest,\n        'Growth': REFEFundamentals.LongTermGrowth_Mean.latest,\n        'EV_EBITDA': REFEFundamentals.EnterpriseValueToEBITDA_DailyTimeSeriesRatio_.latest,\n    }\n)\n\n# Run over a date range (last 2 months of available data)\n# Get valid trading sessions from the calendar\n# Note: sessions_in_range expects timezone-naive dates\nend_search = pd.Timestamp.now()\nstart_search = end_search - pd.DateOffset(months=3)\n\nsessions = trading_calendar.sessions_in_range(\n    start_search.tz_localize(None) if start_search.tz else start_search,\n    end_search.tz_localize(None) if end_search.tz else end_search\n)\nend_date = sessions[-5]  # Use 5 days back from the end\nstart_date = end_date - pd.DateOffset(months=2)\n\n# Ensure start_date is a valid trading session\nstart_date = trading_calendar.sessions_in_range(start_date.tz_localize(None), end_date.tz_localize(None))[0]\n\nprint(f\"Date range: {start_date.date()} to {end_date.date()}\")\n\nresult = engine.run_pipeline(pipeline, start_date, end_date)\n\nprint(f\"âœ“ Time series data extracted\")\nprint(f\"  Period: {start_date.date()} to {end_date.date()}\")\nprint(f\"  Total observations: {len(result):,}\")\n\n# Filter to our symbols of interest\nsymbol_data = result[result.index.get_level_values('asset').isin(assets)]\n\nprint(f\"  Observations for {symbols}: {len(symbol_data):,}\")\n\n# Show AAPL time series\naapl_asset = assets[0]\naapl_data = symbol_data.loc[pd.IndexSlice[:, aapl_asset], :]\n\nprint(f\"\\nAAPL Fundamental Trends (last 10 observations):\")\nprint(aapl_data.tail(10)[['ROE', 'Market_Cap', 'Price', 'Growth']])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Example 5: Visualize Time Series - Plot Fundamental Trends\n\nCreate charts to visualize how fundamentals change over time.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\n# Define symbols to track (re-define in case previous cell wasn't run)\nsymbols = ['AAPL', 'MSFT', 'GOOGL']\nassets = [asset_finder.lookup_symbol(sym, as_of_date=None) for sym in symbols]\n\n# Create pipeline if not already created in previous cell\nif 'symbol_data' not in locals():\n    print(\"Fetching time series data...\")\n    pipeline = Pipeline(\n        columns={\n            'ROE': REFEFundamentals.ReturnOnEquity_SmartEstimat.latest,\n            'Market_Cap': REFEFundamentals.CompanyMarketCap.latest,\n            'Price': REFEFundamentals.RefPriceClose.latest,\n            'Growth': REFEFundamentals.LongTermGrowth_Mean.latest,\n            'EV_EBITDA': REFEFundamentals.EnterpriseValueToEBITDA_DailyTimeSeriesRatio_.latest,\n        }\n    )\n    \n    # Get valid trading sessions (last 3 months, timezone-naive for sessions_in_range)\n    end_search = pd.Timestamp.now()\n    start_search = end_search - pd.DateOffset(months=3)\n    \n    sessions = trading_calendar.sessions_in_range(\n        start_search.tz_localize(None) if start_search.tz else start_search,\n        end_search.tz_localize(None) if end_search.tz else end_search\n    )\n    end_date = sessions[-5]\n    start_date = end_date - pd.DateOffset(months=2)\n    start_date = trading_calendar.sessions_in_range(start_date.tz_localize(None), end_date.tz_localize(None))[0]\n    \n    result = engine.run_pipeline(pipeline, start_date, end_date)\n    symbol_data = result[result.index.get_level_values('asset').isin(assets)]\n    print(f\"âœ“ Fetched {len(symbol_data):,} observations\")\n\n# Create figure with subplots\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\nfig.suptitle('Fundamental Trends: AAPL, MSFT, GOOGL', fontsize=16, fontweight='bold')\n\n# Prepare data for each symbol\nsymbol_colors = {'AAPL': 'blue', 'MSFT': 'green', 'GOOGL': 'red'}\n\nfor idx, (symbol, asset) in enumerate(zip(symbols, assets)):\n    sym_data = symbol_data.loc[pd.IndexSlice[:, asset], :]\n    sym_data = sym_data.reset_index()\n    \n    color = symbol_colors[symbol]\n    \n    # Plot 1: ROE over time\n    axes[0, 0].plot(sym_data['date'], sym_data['ROE'], \n                    label=symbol, marker='o', color=color, alpha=0.7)\n    \n    # Plot 2: Market Cap over time\n    axes[0, 1].plot(sym_data['date'], sym_data['Market_Cap'] / 1e9, \n                    label=symbol, marker='s', color=color, alpha=0.7)\n    \n    # Plot 3: Growth Rate over time\n    axes[1, 0].plot(sym_data['date'], sym_data['Growth'], \n                    label=symbol, marker='^', color=color, alpha=0.7)\n    \n    # Plot 4: EV/EBITDA over time\n    axes[1, 1].plot(sym_data['date'], sym_data['EV_EBITDA'], \n                    label=symbol, marker='D', color=color, alpha=0.7)\n\n# Customize subplots\naxes[0, 0].set_title('Return on Equity (%)', fontweight='bold')\naxes[0, 0].set_ylabel('ROE (%)')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\naxes[0, 1].set_title('Market Capitalization', fontweight='bold')\naxes[0, 1].set_ylabel('Market Cap ($B)')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\naxes[1, 0].set_title('Long-term Growth Rate', fontweight='bold')\naxes[1, 0].set_ylabel('Growth (%)')\naxes[1, 0].set_xlabel('Date')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\naxes[1, 1].set_title('Enterprise Value / EBITDA', fontweight='bold')\naxes[1, 1].set_ylabel('EV/EBITDA Ratio')\naxes[1, 1].set_xlabel('Date')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\n# Format x-axis dates\nfor ax in axes.flat:\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"âœ“ Fundamental trends visualization complete\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Pipeline Examples Summary\n\nYou now know how to:\n- âœ… Set up a Pipeline engine with custom fundamentals\n- âœ… Query latest fundamentals for all stocks\n- âœ… Filter stocks using fundamental criteria\n- âœ… Extract time series data for specific symbols\n- âœ… Visualize fundamental trends over time\n\n**Key takeaways:**\n- Use `REFEFundamentals.ColumnName.latest` to access any fundamental metric\n- Combine multiple metrics with boolean operators (`&`, `|`) for screening\n- Run pipelines over date ranges to analyze trends\n- Filter results by asset to focus on specific symbols\n- Integrate with matplotlib for visualization\n\n**Next steps:**\n- Use these patterns in your backtesting algorithms\n- Create custom factors combining multiple fundamentals\n- Integrate with price data from USEquityPricing\n- Build sophisticated stock selection strategies",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}