{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CSV Fundamental Data into Zipline Custom Database\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load fundamental data from CSV files\n",
    "2. Map symbols to Zipline SIDs\n",
    "3. Create a custom SQLite database\n",
    "4. Use the data in Zipline Pipeline\n",
    "\n",
    "This is a zipline-reloaded native approach (no QuantRocket dependencies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport glob\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\n# Zipline imports\nfrom zipline.data.bundles import load as load_bundle, register\nfrom zipline.data.bundles.sharadar_bundle import sharadar_bundle\nfrom zipline.pipeline import Pipeline\nfrom zipline.pipeline.data.db import Database, Column\n\n# Register Sharadar bundle (in case extension.py didn't load)\ntry:\n    # Try to register the bundle\n    register(\n        'sharadar',\n        sharadar_bundle(\n            tickers=None,\n            incremental=True,\n            include_funds=True,\n        ),\n    )\n    print(\"âœ“ Registered Sharadar bundle\")\nexcept Exception as e:\n    # Bundle may already be registered\n    print(f\"âœ“ Sharadar bundle already registered (or error: {e})\")\n\n# Display settings\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 50)\npd.set_option('display.width', 120)\n\nprint(\"âœ“ Imports complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set your database name and data directory paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nDATABASE_NAME = \"refe-fundamentals\"  # Name for your custom database\nDATA_DIR = \"/data/csv/\"  # Directory with CSV files (persistent across Docker restarts)\nVIX_SIGNAL_PATH = \"/data/csv/vix_flag.csv\"  # Optional VIX signal data\n\n# Database will be created in ~/.zipline/data/custom/\nDB_DIR = Path.home() / '.zipline' / 'data' / 'custom'\nDB_DIR.mkdir(parents=True, exist_ok=True)\nDB_PATH = DB_DIR / f\"{DATABASE_NAME}.sqlite\"\n\nprint(f\"Database will be created at: {DB_PATH}\")\nprint(f\"Looking for CSV files in: {DATA_DIR}\")\nprint(f\"\\nðŸ’¡ Tip: Place your CSV files in /data/csv/ (inside container)\")\nprint(f\"   or ./data/csv/ (on host machine) for persistent storage\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Database Schema\n",
    "\n",
    "Define the columns that will be in your custom database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your database schema\n",
    "# This matches the columns from the QuantRocket example\n",
    "SCHEMA = {\n",
    "    'Symbol': 'TEXT',\n",
    "    'Sid': 'INTEGER',\n",
    "    'Date': 'TEXT',\n",
    "    'RefPriceClose': 'REAL',\n",
    "    'RefVolume': 'REAL',\n",
    "    'CompanyCommonName': 'TEXT',\n",
    "    'EnterpriseValue_DailyTimeSeries_': 'REAL',\n",
    "    'CompanyMarketCap': 'REAL',\n",
    "    'GICSSectorName': 'TEXT',\n",
    "    'FOCFExDividends_Discrete': 'REAL',\n",
    "    'InterestExpense_NetofCapitalizedInterest': 'REAL',\n",
    "    'Debt_Total': 'REAL',\n",
    "    'EarningsPerShare_Actual': 'REAL',\n",
    "    'EarningsPerShare_SmartEstimate_prev_Q': 'REAL',\n",
    "    'EarningsPerShare_ActualSurprise': 'REAL',\n",
    "    'EarningsPerShare_SmartEstimate_current_Q': 'REAL',\n",
    "    'LongTermGrowth_Mean': 'REAL',\n",
    "    'PriceTarget_Median': 'REAL',\n",
    "    'CombinedAlphaModelSectorRank': 'REAL',\n",
    "    'CombinedAlphaModelSectorRankChange': 'REAL',\n",
    "    'CombinedAlphaModelRegionRank': 'REAL',\n",
    "    'TradeDate': 'TEXT',\n",
    "    'EPS_SurpirsePrct_prev_Q': 'REAL',\n",
    "    'Estpricegrowth_percent': 'REAL',\n",
    "    'CashFlowComponent_Current': 'REAL',\n",
    "    'EarningsQualityRegionRank_Current': 'REAL',\n",
    "    'EnterpriseValueToEBIT_DailyTimeSeriesRatio_': 'REAL',\n",
    "    'EnterpriseValueToEBITDA_DailyTimeSeriesRatio_': 'REAL',\n",
    "    'EnterpriseValueToSales_DailyTimeSeriesRatio_': 'REAL',\n",
    "    'Dividend_Per_Share_SmartEstimate': 'REAL',\n",
    "    'CashFlowPerShare_BrokerEstimate': 'REAL',\n",
    "    'FreeCashFlowPerShare_BrokerEstimate': 'REAL',\n",
    "    'ForwardPEG_DailyTimeSeriesRatio_': 'REAL',\n",
    "    'PriceEarningsToGrowthRatio_SmartEstimate_': 'REAL',\n",
    "    'ReturnOnInvestedCapital_BrokerEstimate': 'REAL',\n",
    "    'Recommendation_NumberOfTotal': 'REAL',\n",
    "    'Recommendation_Median_1_5_': 'REAL',\n",
    "    'Recommendation_NumberOfStrongBuy': 'REAL',\n",
    "    'Recommendation_NumberOfBuy': 'REAL',\n",
    "    'Recommendation_Mean_1_5_': 'REAL',\n",
    "    'ReturnOnCapitalEmployed_Actual': 'REAL',\n",
    "    'GrossProfitMargin_': 'REAL',\n",
    "    'ReturnOnEquity_SmartEstimat': 'REAL',\n",
    "    'ReturnOnAssets_SmartEstimate': 'REAL',\n",
    "    'CashCashEquivalents_Total': 'REAL',\n",
    "    'ForwardPriceToCashFlowPerShare_DailyTimeSeriesRatio_': 'REAL',\n",
    "    'ForwardPriceToSalesPerShare_DailyTimeSeriesRatio_': 'REAL',\n",
    "    'ForwardEnterpriseValueToOperatingCashFlow_DailyTimeSeriesRatio_': 'REAL',\n",
    "    'GrossProfitMargin_ActualSurprise': 'REAL',\n",
    "    'pred': 'REAL',  # VIX signal\n",
    "}\n",
    "\n",
    "print(f\"âœ“ Schema defined with {len(SCHEMA)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load CSV Files\n",
    "\n",
    "Load all CSV files from the data directory and concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all CSV files\n",
    "os.chdir(DATA_DIR)\n",
    "csv_files = sorted(glob.glob('*.csv'))\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV files:\")\n",
    "for f in csv_files[:5]:  # Show first 5\n",
    "    print(f\"  - {f}\")\n",
    "if len(csv_files) > 5:\n",
    "    print(f\"  ... and {len(csv_files) - 5} more\")\n",
    "\n",
    "# Load and concatenate all CSV files\n",
    "print(\"\\nLoading CSV files...\")\n",
    "custom_data = pd.DataFrame()\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    print(f\"  Loading {csv_file}...\")\n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, csv_file))\n",
    "    custom_data = pd.concat([custom_data, df], ignore_index=True)\n",
    "\n",
    "print(f\"\\nâœ“ Loaded {len(custom_data):,} total rows\")\n",
    "print(f\"Date range: {custom_data['Date'].min()} to {custom_data['Date'].max()}\")\n",
    "print(f\"Unique symbols: {custom_data['Symbol'].nunique()}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample data:\")\n",
    "custom_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optional: Load Recent Data Only\n",
    "\n",
    "To reduce memory usage, you can filter to recent data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Keep only recent data (e.g., last 600,000 rows)\n",
    "# Comment out if you want all historical data\n",
    "RECENT_ROWS = 600000\n",
    "\n",
    "if len(custom_data) > RECENT_ROWS:\n",
    "    print(f\"Filtering to most recent {RECENT_ROWS:,} rows...\")\n",
    "    custom_data = custom_data.tail(RECENT_ROWS).copy()\n",
    "    print(f\"âœ“ Filtered. New date range: {custom_data['Date'].min()} to {custom_data['Date'].max()}\")\n",
    "else:\n",
    "    print(f\"Dataset has {len(custom_data):,} rows - no filtering needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Map Symbols to Zipline SIDs\n",
    "\n",
    "Map your symbols to Zipline Security IDs (SIDs) using the asset finder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the Sharadar bundle to get the asset finder\nprint(\"Loading Sharadar bundle...\")\n\n# Load bundle with current timestamp\n# This gives us access to the asset database\nbundle_timestamp = pd.Timestamp.now(tz='UTC')\nbundle_data = load_bundle('sharadar', timestamp=bundle_timestamp)\nasset_finder = bundle_data.asset_finder\n\n# Get all equities\nprint(\"Mapping symbols to SIDs...\")\nall_assets = asset_finder.retrieve_all(asset_finder.sids)\n\n# Create symbol -> sid mapping\nsymbol_to_sid = {}\nfor asset in all_assets:\n    if hasattr(asset, 'symbol'):\n        symbol_to_sid[asset.symbol] = asset.sid\n\nprint(f\"âœ“ Found {len(symbol_to_sid):,} symbols in bundle\")\n\n# Map SIDs to your data\ncustom_data['Sid'] = custom_data['Symbol'].map(symbol_to_sid)\n\n# Check mapping success\nmapped = custom_data['Sid'].notna().sum()\nunmapped = custom_data['Sid'].isna().sum()\n\nprint(f\"\\nMapping results:\")\nprint(f\"  Mapped: {mapped:,} rows ({mapped/len(custom_data)*100:.1f}%)\")\nprint(f\"  Unmapped: {unmapped:,} rows ({unmapped/len(custom_data)*100:.1f}%)\")\n\nif unmapped > 0:\n    unmapped_symbols = custom_data[custom_data['Sid'].isna()]['Symbol'].unique()\n    print(f\"\\n  Unmapped symbols (first 10): {list(unmapped_symbols[:10])}\")\n    print(f\"  Tip: These symbols may not be in the Sharadar bundle\")\n\n# Remove unmapped rows\ncustom_data = custom_data[custom_data['Sid'].notna()].copy()\nprint(f\"\\nâœ“ Kept {len(custom_data):,} mapped rows\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Merge VIX Signal Data (Optional)\n",
    "\n",
    "If you have additional data like VIX signals, merge it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VIX signal data if available\n",
    "if os.path.exists(VIX_SIGNAL_PATH):\n",
    "    print(f\"Loading VIX signal from {VIX_SIGNAL_PATH}...\")\n",
    "    vix_signal = pd.read_csv(VIX_SIGNAL_PATH)\n",
    "    \n",
    "    # Standardize column names\n",
    "    vix_signal.rename(columns={'symbol': 'Symbol', 'date': 'Date'}, inplace=True)\n",
    "    vix_signal['Date'] = pd.to_datetime(vix_signal['Date'])\n",
    "    \n",
    "    # Merge with custom data\n",
    "    custom_data['Date'] = pd.to_datetime(custom_data['Date'])\n",
    "    custom_data = pd.merge(custom_data, vix_signal[['Symbol', 'Date', 'pred']], \n",
    "                          on=['Symbol', 'Date'], how='left')\n",
    "    \n",
    "    print(f\"âœ“ Merged VIX signal data\")\n",
    "else:\n",
    "    print(f\"VIX signal file not found at {VIX_SIGNAL_PATH}\")\n",
    "    print(\"Skipping VIX merge (this is optional)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Cleaning\n",
    "\n",
    "Clean and prepare data for database insertion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleaning data...\")\n",
    "\n",
    "# Ensure Date is datetime\n",
    "custom_data['Date'] = pd.to_datetime(custom_data['Date'])\n",
    "\n",
    "# Forward fill missing values by symbol\n",
    "print(\"  Forward filling missing values by symbol...\")\n",
    "for col in custom_data.columns:\n",
    "    if col not in ['Symbol', 'Sid', 'Date']:\n",
    "        custom_data[col] = custom_data.groupby('Symbol')[col].transform(lambda x: x.ffill())\n",
    "\n",
    "# Handle sector - fill empty strings instead of NaN\n",
    "custom_data['GICSSectorName'] = custom_data['GICSSectorName'].fillna('')\n",
    "\n",
    "# Fill remaining NaNs with 0 (for numeric columns)\n",
    "print(\"  Filling remaining NaN values...\")\n",
    "custom_data = custom_data.fillna(0)\n",
    "\n",
    "# Convert Sid to integer\n",
    "custom_data['Sid'] = custom_data['Sid'].astype(int)\n",
    "\n",
    "# Sort by date and symbol\n",
    "custom_data = custom_data.sort_values(['Date', 'Symbol'])\n",
    "\n",
    "print(f\"âœ“ Data cleaned\")\n",
    "print(f\"\\nFinal dataset:\")\n",
    "print(f\"  Rows: {len(custom_data):,}\")\n",
    "print(f\"  Columns: {len(custom_data.columns)}\")\n",
    "print(f\"  Date range: {custom_data['Date'].min()} to {custom_data['Date'].max()}\")\n",
    "print(f\"  Symbols: {custom_data['Symbol'].nunique()}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample cleaned data:\")\n",
    "custom_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create SQLite Database\n",
    "\n",
    "Create the custom SQLite database in Zipline format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"Creating database at {DB_PATH}...\")\n\n# Remove existing database if it exists\nif DB_PATH.exists():\n    print(f\"  Removing existing database...\")\n    DB_PATH.unlink()\n\n# Create database connection\nconn = sqlite3.connect(str(DB_PATH))\ncursor = conn.cursor()\n\n# Create table with schema\ncolumns_def = ', '.join([f'\"{col}\" {dtype}' for col, dtype in SCHEMA.items()])\ncreate_table_sql = f'''\nCREATE TABLE fundamentals (\n    {columns_def}\n);\n'''\n\nprint(\"  Creating table...\")\ncursor.execute(create_table_sql)\n\n# Create indices for fast lookups\nprint(\"  Creating indices...\")\ncursor.execute('CREATE INDEX idx_sid ON fundamentals(Sid);')\ncursor.execute('CREATE INDEX idx_date ON fundamentals(Date);')\ncursor.execute('CREATE INDEX idx_sid_date ON fundamentals(Sid, Date);')\n\n# Insert data\nprint(f\"  Inserting {len(custom_data):,} rows...\")\n\n# Prepare data for insertion - only use columns that exist in custom_data\n# Add missing columns with default values (0 for numeric, empty string for text)\ninsert_data = custom_data.copy()\n\n# Add any missing schema columns with appropriate defaults\nfor col, dtype in SCHEMA.items():\n    if col not in insert_data.columns:\n        if dtype == 'TEXT':\n            insert_data[col] = ''\n        else:  # REAL or INTEGER\n            insert_data[col] = 0\n        print(f\"  Added missing column '{col}' with default values\")\n\n# Select only the columns in the schema (in the correct order)\ninsert_data = insert_data[list(SCHEMA.keys())].copy()\n\n# Convert Date to string format for SQLite\ninsert_data['Date'] = insert_data['Date'].dt.strftime('%Y-%m-%d')\n\n# Insert in chunks for better performance\nchunk_size = 10000\ntotal_chunks = (len(insert_data) + chunk_size - 1) // chunk_size\n\nfor i in range(0, len(insert_data), chunk_size):\n    chunk = insert_data.iloc[i:i+chunk_size]\n    chunk.to_sql('fundamentals', conn, if_exists='append', index=False)\n    \n    chunk_num = i // chunk_size + 1\n    if chunk_num % 10 == 0 or chunk_num == total_chunks:\n        print(f\"    Inserted chunk {chunk_num}/{total_chunks} ({i+len(chunk):,} rows)...\")\n\nconn.commit()\nconn.close()\n\nprint(f\"\\nâœ“ Database created successfully!\")\nprint(f\"  Path: {DB_PATH}\")\nprint(f\"  Size: {DB_PATH.stat().st_size / 1024 / 1024:.1f} MB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Define Database Class\n",
    "\n",
    "Create a Database class to use this data in Zipline Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Database class\n",
    "class REFEFundamentals(Database):\n",
    "    \"\"\"\n",
    "    Custom REFE Fundamentals database.\n",
    "    \n",
    "    Usage in Pipeline:\n",
    "        roe = REFEFundamentals.ReturnOnEquity_SmartEstimat.latest\n",
    "        sector = REFEFundamentals.GICSSectorName.latest\n",
    "    \"\"\"\n",
    "    \n",
    "    CODE = DATABASE_NAME\n",
    "    LOOKBACK_WINDOW = 252  # Days to look back\n",
    "    \n",
    "    # Price and volume\n",
    "    RefPriceClose = Column(float)\n",
    "    RefVolume = Column(float)\n",
    "    \n",
    "    # Company info\n",
    "    CompanyCommonName = Column(str)\n",
    "    GICSSectorName = Column(str)\n",
    "    \n",
    "    # Valuation metrics\n",
    "    EnterpriseValue_DailyTimeSeries_ = Column(float)\n",
    "    CompanyMarketCap = Column(float)\n",
    "    \n",
    "    # Cash flow\n",
    "    FOCFExDividends_Discrete = Column(float)\n",
    "    CashFlowComponent_Current = Column(float)\n",
    "    CashFlowPerShare_BrokerEstimate = Column(float)\n",
    "    FreeCashFlowPerShare_BrokerEstimate = Column(float)\n",
    "    \n",
    "    # Debt and interest\n",
    "    InterestExpense_NetofCapitalizedInterest = Column(float)\n",
    "    Debt_Total = Column(float)\n",
    "    \n",
    "    # Earnings\n",
    "    EarningsPerShare_Actual = Column(float)\n",
    "    EarningsPerShare_SmartEstimate_prev_Q = Column(float)\n",
    "    EarningsPerShare_ActualSurprise = Column(float)\n",
    "    EarningsPerShare_SmartEstimate_current_Q = Column(float)\n",
    "    EPS_SurpirsePrct_prev_Q = Column(float)\n",
    "    \n",
    "    # Growth and targets\n",
    "    LongTermGrowth_Mean = Column(float)\n",
    "    PriceTarget_Median = Column(float)\n",
    "    Estpricegrowth_percent = Column(float)\n",
    "    \n",
    "    # Rankings\n",
    "    CombinedAlphaModelSectorRank = Column(float)\n",
    "    CombinedAlphaModelSectorRankChange = Column(float)\n",
    "    CombinedAlphaModelRegionRank = Column(float)\n",
    "    EarningsQualityRegionRank_Current = Column(float)\n",
    "    \n",
    "    # Ratios\n",
    "    EnterpriseValueToEBIT_DailyTimeSeriesRatio_ = Column(float)\n",
    "    EnterpriseValueToEBITDA_DailyTimeSeriesRatio_ = Column(float)\n",
    "    EnterpriseValueToSales_DailyTimeSeriesRatio_ = Column(float)\n",
    "    ForwardPEG_DailyTimeSeriesRatio_ = Column(float)\n",
    "    PriceEarningsToGrowthRatio_SmartEstimate_ = Column(float)\n",
    "    ForwardPriceToCashFlowPerShare_DailyTimeSeriesRatio_ = Column(float)\n",
    "    ForwardPriceToSalesPerShare_DailyTimeSeriesRatio_ = Column(float)\n",
    "    ForwardEnterpriseValueToOperatingCashFlow_DailyTimeSeriesRatio_ = Column(float)\n",
    "    \n",
    "    # Returns\n",
    "    ReturnOnInvestedCapital_BrokerEstimate = Column(float)\n",
    "    ReturnOnCapitalEmployed_Actual = Column(float)\n",
    "    ReturnOnEquity_SmartEstimat = Column(float)\n",
    "    ReturnOnAssets_SmartEstimate = Column(float)\n",
    "    \n",
    "    # Margins\n",
    "    GrossProfitMargin_ = Column(float)\n",
    "    GrossProfitMargin_ActualSurprise = Column(float)\n",
    "    \n",
    "    # Analyst recommendations\n",
    "    Recommendation_NumberOfTotal = Column(float)\n",
    "    Recommendation_Median_1_5_ = Column(float)\n",
    "    Recommendation_NumberOfStrongBuy = Column(float)\n",
    "    Recommendation_NumberOfBuy = Column(float)\n",
    "    Recommendation_Mean_1_5_ = Column(float)\n",
    "    \n",
    "    # Cash\n",
    "    CashCashEquivalents_Total = Column(float)\n",
    "    \n",
    "    # Dividends\n",
    "    Dividend_Per_Share_SmartEstimate = Column(float)\n",
    "    \n",
    "    # VIX prediction signal\n",
    "    pred = Column(float)\n",
    "\n",
    "\n",
    "print(\"âœ“ REFEFundamentals Database class defined\")\n",
    "print(f\"  Database code: {REFEFundamentals.CODE}\")\n",
    "print(f\"  Lookback window: {REFEFundamentals.LOOKBACK_WINDOW} days\")\n",
    "print(f\"  Columns defined: {len([attr for attr in dir(REFEFundamentals) if isinstance(getattr(REFEFundamentals, attr), Column)])}\")\n",
    "\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"  roe = REFEFundamentals.ReturnOnEquity_SmartEstimat.latest\")\n",
    "print(\"  pe_growth = REFEFundamentals.PriceEarningsToGrowthRatio_SmartEstimate_.latest\")\n",
    "print(\"  sector = REFEFundamentals.GICSSectorName.latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Verify Database\n",
    "\n",
    "Query the database to verify data was loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect and query\n",
    "conn = sqlite3.connect(str(DB_PATH))\n",
    "\n",
    "# Get row count\n",
    "row_count = pd.read_sql(\"SELECT COUNT(*) as count FROM fundamentals\", conn).iloc[0, 0]\n",
    "print(f\"Total rows in database: {row_count:,}\")\n",
    "\n",
    "# Get date range\n",
    "date_range = pd.read_sql(\"SELECT MIN(Date) as min_date, MAX(Date) as max_date FROM fundamentals\", conn)\n",
    "print(f\"Date range: {date_range.iloc[0, 0]} to {date_range.iloc[0, 1]}\")\n",
    "\n",
    "# Get symbol count\n",
    "symbol_count = pd.read_sql(\"SELECT COUNT(DISTINCT Symbol) as count FROM fundamentals\", conn).iloc[0, 0]\n",
    "print(f\"Unique symbols: {symbol_count:,}\")\n",
    "\n",
    "# Show sample data for a specific symbol\n",
    "print(\"\\nSample data for AAPL:\")\n",
    "aapl_data = pd.read_sql(\"\"\"\n",
    "    SELECT Date, Symbol, RefPriceClose, CompanyMarketCap, \n",
    "           ReturnOnEquity_SmartEstimat, PriceTarget_Median\n",
    "    FROM fundamentals \n",
    "    WHERE Symbol = 'AAPL' \n",
    "    ORDER BY Date DESC \n",
    "    LIMIT 5\n",
    "\"\"\", conn)\n",
    "print(aapl_data)\n",
    "\n",
    "print(\"\\nSample data for IBM:\")\n",
    "ibm_data = pd.read_sql(\"\"\"\n",
    "    SELECT Date, Symbol, RefPriceClose, CompanyMarketCap, \n",
    "           ReturnOnEquity_SmartEstimat, GICSSectorName\n",
    "    FROM fundamentals \n",
    "    WHERE Symbol = 'IBM' \n",
    "    ORDER BY Date DESC \n",
    "    LIMIT 5\n",
    "\"\"\", conn)\n",
    "print(ibm_data)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"\\nâœ“ Database verification complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Usage Example\n",
    "\n",
    "Example of how to use this database in a backtest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"To use this database in your backtests:\")\n",
    "print(\"\\n1. Import the Database class:\")\n",
    "print(\"   from zipline.pipeline.data.db import Database, Column\")\n",
    "print(\"\\n2. Define the REFEFundamentals class (from cell 10 above)\")\n",
    "print(\"\\n3. Use in your pipeline:\")\n",
    "print(\"   \")\n",
    "print(\"   def make_pipeline():\")\n",
    "print(\"       roe = REFEFundamentals.ReturnOnEquity_SmartEstimat.latest\")\n",
    "print(\"       growth = REFEFundamentals.LongTermGrowth_Mean.latest\")\n",
    "print(\"       sector = REFEFundamentals.GICSSectorName.latest\")\n",
    "print(\"       \")\n",
    "print(\"       # Screen for quality companies\")\n",
    "print(\"       quality = (roe > 15) & (growth > 10)\")\n",
    "print(\"       \")\n",
    "print(\"       return Pipeline(\")\n",
    "print(\"           columns={\")\n",
    "print(\"               'ROE': roe,\")\n",
    "print(\"               'Growth': growth,\")\n",
    "print(\"               'Sector': sector,\")\n",
    "print(\"           },\")\n",
    "print(\"           screen=quality\")\n",
    "print(\"       )\")\n",
    "print(\"\\n4. The CustomSQLiteLoader will automatically load data based on REFEFundamentals.CODE\")\n",
    "\n",
    "print(\"\\nâœ“ Setup complete! Your custom fundamentals database is ready to use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "1. âœ… Loaded CSV files with fundamental data\n",
    "2. âœ… Mapped symbols to Zipline SIDs using the asset finder\n",
    "3. âœ… Cleaned and prepared the data\n",
    "4. âœ… Created a custom SQLite database in ~/.zipline/data/custom/\n",
    "5. âœ… Defined a Database class for use in Pipeline\n",
    "6. âœ… Verified the database contents\n",
    "\n",
    "The database is now ready to use in your Zipline backtests with the CustomSQLiteLoader.\n",
    "\n",
    "**Next steps:**\n",
    "- Copy the REFEFundamentals class definition to your backtest algorithm\n",
    "- Use REFEFundamentals.ColumnName.latest in your pipeline\n",
    "- The backtest_helpers.py will automatically detect and load the data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}